{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229a7bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:06.098557Z",
     "start_time": "2022-10-13T04:13:06.083110Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452a405d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:06.620132Z",
     "start_time": "2022-10-13T04:13:06.593097Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2410c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:07.056083Z",
     "start_time": "2022-10-13T04:13:07.037637Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list = df.columns.to_list()\n",
    "features_list.pop(0)\n",
    "y = df['gesture_id']\n",
    "X = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1614094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:07.605601Z",
     "start_time": "2022-10-13T04:13:07.593554Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417c9b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:11.985184Z",
     "start_time": "2022-10-13T04:13:08.309555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80572809\n",
      "Iteration 2, loss = 1.80506821\n",
      "Iteration 3, loss = 1.80398263\n",
      "Iteration 4, loss = 1.80264054\n",
      "Iteration 5, loss = 1.80121945\n",
      "Iteration 6, loss = 1.79977581\n",
      "Iteration 7, loss = 1.79830789\n",
      "Iteration 8, loss = 1.79680014\n",
      "Iteration 9, loss = 1.79524543\n",
      "Iteration 10, loss = 1.79380611\n",
      "Iteration 11, loss = 1.79239347\n",
      "Iteration 12, loss = 1.79117044\n",
      "Iteration 13, loss = 1.78982870\n",
      "Iteration 14, loss = 1.78849283\n",
      "Iteration 15, loss = 1.78741341\n",
      "Iteration 16, loss = 1.78613726\n",
      "Iteration 17, loss = 1.78503886\n",
      "Iteration 18, loss = 1.78397389\n",
      "Iteration 19, loss = 1.78291095\n",
      "Iteration 20, loss = 1.78198316\n",
      "Iteration 21, loss = 1.78104319\n",
      "Iteration 22, loss = 1.78018420\n",
      "Iteration 23, loss = 1.77927429\n",
      "Iteration 24, loss = 1.77859122\n",
      "Iteration 25, loss = 1.77769645\n",
      "Iteration 26, loss = 1.77693556\n",
      "Iteration 27, loss = 1.77625215\n",
      "Iteration 28, loss = 1.77549162\n",
      "Iteration 29, loss = 1.77482672\n",
      "Iteration 30, loss = 1.77418686\n",
      "Iteration 31, loss = 1.77361055\n",
      "Iteration 32, loss = 1.77294144\n",
      "Iteration 33, loss = 1.77226387\n",
      "Iteration 34, loss = 1.77163855\n",
      "Iteration 35, loss = 1.77104857\n",
      "Iteration 36, loss = 1.77040919\n",
      "Iteration 37, loss = 1.76974548\n",
      "Iteration 38, loss = 1.76910360\n",
      "Iteration 39, loss = 1.76851586\n",
      "Iteration 40, loss = 1.76791587\n",
      "Iteration 41, loss = 1.76732217\n",
      "Iteration 42, loss = 1.76684374\n",
      "Iteration 43, loss = 1.76628069\n",
      "Iteration 44, loss = 1.76573366\n",
      "Iteration 45, loss = 1.76531992\n",
      "Iteration 46, loss = 1.76480442\n",
      "Iteration 47, loss = 1.76435745\n",
      "Iteration 48, loss = 1.76391029\n",
      "Iteration 49, loss = 1.76354217\n",
      "Iteration 50, loss = 1.76307459\n",
      "Iteration 51, loss = 1.76269089\n",
      "Iteration 52, loss = 1.76229012\n",
      "Iteration 53, loss = 1.76192643\n",
      "Iteration 54, loss = 1.76148092\n",
      "Iteration 55, loss = 1.76106121\n",
      "Iteration 56, loss = 1.76066441\n",
      "Iteration 57, loss = 1.76025165\n",
      "Iteration 58, loss = 1.75986368\n",
      "Iteration 59, loss = 1.75942514\n",
      "Iteration 60, loss = 1.75901454\n",
      "Iteration 61, loss = 1.75863025\n",
      "Iteration 62, loss = 1.75820562\n",
      "Iteration 63, loss = 1.75781183\n",
      "Iteration 64, loss = 1.75741509\n",
      "Iteration 65, loss = 1.75699445\n",
      "Iteration 66, loss = 1.75665104\n",
      "Iteration 67, loss = 1.75627511\n",
      "Iteration 68, loss = 1.75594590\n",
      "Iteration 69, loss = 1.75555558\n",
      "Iteration 70, loss = 1.75517880\n",
      "Iteration 71, loss = 1.75488352\n",
      "Iteration 72, loss = 1.75446999\n",
      "Iteration 73, loss = 1.75416031\n",
      "Iteration 74, loss = 1.75380162\n",
      "Iteration 75, loss = 1.75349945\n",
      "Iteration 76, loss = 1.75315180\n",
      "Iteration 77, loss = 1.75283888\n",
      "Iteration 78, loss = 1.75251514\n",
      "Iteration 79, loss = 1.75220764\n",
      "Iteration 80, loss = 1.75185195\n",
      "Iteration 81, loss = 1.75154536\n",
      "Iteration 82, loss = 1.75124192\n",
      "Iteration 83, loss = 1.75088470\n",
      "Iteration 84, loss = 1.75054148\n",
      "Iteration 85, loss = 1.75018376\n",
      "Iteration 86, loss = 1.74988827\n",
      "Iteration 87, loss = 1.74949712\n",
      "Iteration 88, loss = 1.74914057\n",
      "Iteration 89, loss = 1.74879097\n",
      "Iteration 90, loss = 1.74841975\n",
      "Iteration 91, loss = 1.74807375\n",
      "Iteration 92, loss = 1.74769907\n",
      "Iteration 93, loss = 1.74734639\n",
      "Iteration 94, loss = 1.74696404\n",
      "Iteration 95, loss = 1.74656085\n",
      "Iteration 96, loss = 1.74620037\n",
      "Iteration 97, loss = 1.74580278\n",
      "Iteration 98, loss = 1.74540461\n",
      "Iteration 99, loss = 1.74504297\n",
      "Iteration 100, loss = 1.74459732\n",
      "Iteration 101, loss = 1.74422616\n",
      "Iteration 102, loss = 1.74380033\n",
      "Iteration 103, loss = 1.74338407\n",
      "Iteration 104, loss = 1.74296735\n",
      "Iteration 105, loss = 1.74255160\n",
      "Iteration 106, loss = 1.74217761\n",
      "Iteration 107, loss = 1.74173110\n",
      "Iteration 108, loss = 1.74133010\n",
      "Iteration 109, loss = 1.74096435\n",
      "Iteration 110, loss = 1.74053957\n",
      "Iteration 111, loss = 1.74015552\n",
      "Iteration 112, loss = 1.73978666\n",
      "Iteration 113, loss = 1.73940054\n",
      "Iteration 114, loss = 1.73903189\n",
      "Iteration 115, loss = 1.73866972\n",
      "Iteration 116, loss = 1.73832565\n",
      "Iteration 117, loss = 1.73796127\n",
      "Iteration 118, loss = 1.73758511\n",
      "Iteration 119, loss = 1.73722584\n",
      "Iteration 120, loss = 1.73692015\n",
      "Iteration 121, loss = 1.73653651\n",
      "Iteration 122, loss = 1.73620376\n",
      "Iteration 123, loss = 1.73582766\n",
      "Iteration 124, loss = 1.73548653\n",
      "Iteration 125, loss = 1.73513168\n",
      "Iteration 126, loss = 1.73478988\n",
      "Iteration 127, loss = 1.73441775\n",
      "Iteration 128, loss = 1.73408144\n",
      "Iteration 129, loss = 1.73371928\n",
      "Iteration 130, loss = 1.73335944\n",
      "Iteration 131, loss = 1.73300381\n",
      "Iteration 132, loss = 1.73263626\n",
      "Iteration 133, loss = 1.73228093\n",
      "Iteration 134, loss = 1.73193095\n",
      "Iteration 135, loss = 1.73156586\n",
      "Iteration 136, loss = 1.73120378\n",
      "Iteration 137, loss = 1.73080808\n",
      "Iteration 138, loss = 1.73047496\n",
      "Iteration 139, loss = 1.73006301\n",
      "Iteration 140, loss = 1.72970572\n",
      "Iteration 141, loss = 1.72931292\n",
      "Iteration 142, loss = 1.72891866\n",
      "Iteration 143, loss = 1.72853545\n",
      "Iteration 144, loss = 1.72815298\n",
      "Iteration 145, loss = 1.72776057\n",
      "Iteration 146, loss = 1.72733650\n",
      "Iteration 147, loss = 1.72696223\n",
      "Iteration 148, loss = 1.72649472\n",
      "Iteration 149, loss = 1.72611435\n",
      "Iteration 150, loss = 1.72568315\n",
      "Iteration 151, loss = 1.72524132\n",
      "Iteration 152, loss = 1.72482320\n",
      "Iteration 153, loss = 1.72440166\n",
      "Iteration 154, loss = 1.72396151\n",
      "Iteration 155, loss = 1.72356194\n",
      "Iteration 156, loss = 1.72310915\n",
      "Iteration 157, loss = 1.72267934\n",
      "Iteration 158, loss = 1.72224459\n",
      "Iteration 159, loss = 1.72176966\n",
      "Iteration 160, loss = 1.72132487\n",
      "Iteration 161, loss = 1.72085479\n",
      "Iteration 162, loss = 1.72039551\n",
      "Iteration 163, loss = 1.71998198\n",
      "Iteration 164, loss = 1.71952186\n",
      "Iteration 165, loss = 1.71903786\n",
      "Iteration 166, loss = 1.71857819\n",
      "Iteration 167, loss = 1.71810269\n",
      "Iteration 168, loss = 1.71764522\n",
      "Iteration 169, loss = 1.71716921\n",
      "Iteration 170, loss = 1.71670112\n",
      "Iteration 171, loss = 1.71623644\n",
      "Iteration 172, loss = 1.71579482\n",
      "Iteration 173, loss = 1.71530928\n",
      "Iteration 174, loss = 1.71485488\n",
      "Iteration 175, loss = 1.71440186\n",
      "Iteration 176, loss = 1.71392980\n",
      "Iteration 177, loss = 1.71346626\n",
      "Iteration 178, loss = 1.71294188\n",
      "Iteration 179, loss = 1.71251439\n",
      "Iteration 180, loss = 1.71198604\n",
      "Iteration 181, loss = 1.71150255\n",
      "Iteration 182, loss = 1.71108556\n",
      "Iteration 183, loss = 1.71058503\n",
      "Iteration 184, loss = 1.71009499\n",
      "Iteration 185, loss = 1.70956915\n",
      "Iteration 186, loss = 1.70909822\n",
      "Iteration 187, loss = 1.70859189\n",
      "Iteration 188, loss = 1.70807096\n",
      "Iteration 189, loss = 1.70758988\n",
      "Iteration 190, loss = 1.70707830\n",
      "Iteration 191, loss = 1.70657770\n",
      "Iteration 192, loss = 1.70606656\n",
      "Iteration 193, loss = 1.70552327\n",
      "Iteration 194, loss = 1.70502158\n",
      "Iteration 195, loss = 1.70451039\n",
      "Iteration 196, loss = 1.70398110\n",
      "Iteration 197, loss = 1.70342731\n",
      "Iteration 198, loss = 1.70289799\n",
      "Iteration 199, loss = 1.70235812\n",
      "Iteration 200, loss = 1.70181768\n",
      "Iteration 201, loss = 1.70128133\n",
      "Iteration 202, loss = 1.70074932\n",
      "Iteration 203, loss = 1.70020569\n",
      "Iteration 204, loss = 1.69967159\n",
      "Iteration 205, loss = 1.69909339\n",
      "Iteration 206, loss = 1.69856522\n",
      "Iteration 207, loss = 1.69800232\n",
      "Iteration 208, loss = 1.69746490\n",
      "Iteration 209, loss = 1.69683987\n",
      "Iteration 210, loss = 1.69626862\n",
      "Iteration 211, loss = 1.69571395\n",
      "Iteration 212, loss = 1.69512244\n",
      "Iteration 213, loss = 1.69451314\n",
      "Iteration 214, loss = 1.69392773\n",
      "Iteration 215, loss = 1.69334546\n",
      "Iteration 216, loss = 1.69274909\n",
      "Iteration 217, loss = 1.69214749\n",
      "Iteration 218, loss = 1.69154166\n",
      "Iteration 219, loss = 1.69093374\n",
      "Iteration 220, loss = 1.69033859\n",
      "Iteration 221, loss = 1.68968914\n",
      "Iteration 222, loss = 1.68908000\n",
      "Iteration 223, loss = 1.68844130\n",
      "Iteration 224, loss = 1.68780327\n",
      "Iteration 225, loss = 1.68718863\n",
      "Iteration 226, loss = 1.68654136\n",
      "Iteration 227, loss = 1.68589034\n",
      "Iteration 228, loss = 1.68520231\n",
      "Iteration 229, loss = 1.68456275\n",
      "Iteration 230, loss = 1.68385974\n",
      "Iteration 231, loss = 1.68320308\n",
      "Iteration 232, loss = 1.68249692\n",
      "Iteration 233, loss = 1.68184312\n",
      "Iteration 234, loss = 1.68110358\n",
      "Iteration 235, loss = 1.68044602\n",
      "Iteration 236, loss = 1.67972817\n",
      "Iteration 237, loss = 1.67902614\n",
      "Iteration 238, loss = 1.67826492\n",
      "Iteration 239, loss = 1.67754110\n",
      "Iteration 240, loss = 1.67680694\n",
      "Iteration 241, loss = 1.67607804\n",
      "Iteration 242, loss = 1.67536666\n",
      "Iteration 243, loss = 1.67460904\n",
      "Iteration 244, loss = 1.67385164\n",
      "Iteration 245, loss = 1.67307978\n",
      "Iteration 246, loss = 1.67231122\n",
      "Iteration 247, loss = 1.67152486\n",
      "Iteration 248, loss = 1.67076304\n",
      "Iteration 249, loss = 1.66997501\n",
      "Iteration 250, loss = 1.66913477\n",
      "Iteration 251, loss = 1.66828667\n",
      "Iteration 252, loss = 1.66751727\n",
      "Iteration 253, loss = 1.66668860\n",
      "Iteration 254, loss = 1.66584774\n",
      "Iteration 255, loss = 1.66501405\n",
      "Iteration 256, loss = 1.66418990\n",
      "Iteration 257, loss = 1.66330771\n",
      "Iteration 258, loss = 1.66242404\n",
      "Iteration 259, loss = 1.66153975\n",
      "Iteration 260, loss = 1.66067183\n",
      "Iteration 261, loss = 1.65976452\n",
      "Iteration 262, loss = 1.65888184\n",
      "Iteration 263, loss = 1.65795974\n",
      "Iteration 264, loss = 1.65700309\n",
      "Iteration 265, loss = 1.65607330\n",
      "Iteration 266, loss = 1.65511995\n",
      "Iteration 267, loss = 1.65416052\n",
      "Iteration 268, loss = 1.65326294\n",
      "Iteration 269, loss = 1.65224456\n",
      "Iteration 270, loss = 1.65127150\n",
      "Iteration 271, loss = 1.65031344\n",
      "Iteration 272, loss = 1.64934803\n",
      "Iteration 273, loss = 1.64831422\n",
      "Iteration 274, loss = 1.64728147\n",
      "Iteration 275, loss = 1.64623905\n",
      "Iteration 276, loss = 1.64519967\n",
      "Iteration 277, loss = 1.64415781\n",
      "Iteration 278, loss = 1.64307816\n",
      "Iteration 279, loss = 1.64200126\n",
      "Iteration 280, loss = 1.64088977\n",
      "Iteration 281, loss = 1.63987432\n",
      "Iteration 282, loss = 1.63869461\n",
      "Iteration 283, loss = 1.63759525\n",
      "Iteration 284, loss = 1.63650371\n",
      "Iteration 285, loss = 1.63529524\n",
      "Iteration 286, loss = 1.63412145\n",
      "Iteration 287, loss = 1.63304149\n",
      "Iteration 288, loss = 1.63182153\n",
      "Iteration 289, loss = 1.63057599\n",
      "Iteration 290, loss = 1.62936121\n",
      "Iteration 291, loss = 1.62822529\n",
      "Iteration 292, loss = 1.62701300\n",
      "Iteration 293, loss = 1.62570080\n",
      "Iteration 294, loss = 1.62443767\n",
      "Iteration 295, loss = 1.62316104\n",
      "Iteration 296, loss = 1.62185835\n",
      "Iteration 297, loss = 1.62059119\n",
      "Iteration 298, loss = 1.61924846\n",
      "Iteration 299, loss = 1.61788108\n",
      "Iteration 300, loss = 1.61653448\n",
      "Iteration 301, loss = 1.61518855\n",
      "Iteration 302, loss = 1.61378990\n",
      "Iteration 303, loss = 1.61244288\n",
      "Iteration 304, loss = 1.61099138\n",
      "Iteration 305, loss = 1.60963587\n",
      "Iteration 306, loss = 1.60824916\n",
      "Iteration 307, loss = 1.60674055\n",
      "Iteration 308, loss = 1.60529899\n",
      "Iteration 309, loss = 1.60389539\n",
      "Iteration 310, loss = 1.60237440\n",
      "Iteration 311, loss = 1.60092570\n",
      "Iteration 312, loss = 1.59926838\n",
      "Iteration 313, loss = 1.59778001\n",
      "Iteration 314, loss = 1.59620569\n",
      "Iteration 315, loss = 1.59461580\n",
      "Iteration 316, loss = 1.59300633\n",
      "Iteration 317, loss = 1.59141010\n",
      "Iteration 318, loss = 1.58980249\n",
      "Iteration 319, loss = 1.58811207\n",
      "Iteration 320, loss = 1.58651384\n",
      "Iteration 321, loss = 1.58476302\n",
      "Iteration 322, loss = 1.58305345\n",
      "Iteration 323, loss = 1.58132595\n",
      "Iteration 324, loss = 1.57957719\n",
      "Iteration 325, loss = 1.57785081\n",
      "Iteration 326, loss = 1.57615838\n",
      "Iteration 327, loss = 1.57433877\n",
      "Iteration 328, loss = 1.57250122\n",
      "Iteration 329, loss = 1.57085375\n",
      "Iteration 330, loss = 1.56884227\n",
      "Iteration 331, loss = 1.56698603\n",
      "Iteration 332, loss = 1.56510839\n",
      "Iteration 333, loss = 1.56326384\n",
      "Iteration 334, loss = 1.56138547\n",
      "Iteration 335, loss = 1.55939429\n",
      "Iteration 336, loss = 1.55746489\n",
      "Iteration 337, loss = 1.55547973\n",
      "Iteration 338, loss = 1.55351351\n",
      "Iteration 339, loss = 1.55164870\n",
      "Iteration 340, loss = 1.54964276\n",
      "Iteration 341, loss = 1.54757797\n",
      "Iteration 342, loss = 1.54542463\n",
      "Iteration 343, loss = 1.54340817\n",
      "Iteration 344, loss = 1.54125437\n",
      "Iteration 345, loss = 1.53914219\n",
      "Iteration 346, loss = 1.53696022\n",
      "Iteration 347, loss = 1.53491673\n",
      "Iteration 348, loss = 1.53266744\n",
      "Iteration 349, loss = 1.53043838\n",
      "Iteration 350, loss = 1.52827654\n",
      "Iteration 351, loss = 1.52607013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 352, loss = 1.52368695\n",
      "Iteration 353, loss = 1.52150736\n",
      "Iteration 354, loss = 1.51912969\n",
      "Iteration 355, loss = 1.51681701\n",
      "Iteration 356, loss = 1.51451181\n",
      "Iteration 357, loss = 1.51219260\n",
      "Iteration 358, loss = 1.50975229\n",
      "Iteration 359, loss = 1.50740181\n",
      "Iteration 360, loss = 1.50506724\n",
      "Iteration 361, loss = 1.50258142\n",
      "Iteration 362, loss = 1.50020604\n",
      "Iteration 363, loss = 1.49772363\n",
      "Iteration 364, loss = 1.49519961\n",
      "Iteration 365, loss = 1.49274264\n",
      "Iteration 366, loss = 1.49016003\n",
      "Iteration 367, loss = 1.48762558\n",
      "Iteration 368, loss = 1.48504448\n",
      "Iteration 369, loss = 1.48240202\n",
      "Iteration 370, loss = 1.47983329\n",
      "Iteration 371, loss = 1.47712109\n",
      "Iteration 372, loss = 1.47451754\n",
      "Iteration 373, loss = 1.47180835\n",
      "Iteration 374, loss = 1.46916894\n",
      "Iteration 375, loss = 1.46637127\n",
      "Iteration 376, loss = 1.46371189\n",
      "Iteration 377, loss = 1.46091350\n",
      "Iteration 378, loss = 1.45814082\n",
      "Iteration 379, loss = 1.45539714\n",
      "Iteration 380, loss = 1.45266392\n",
      "Iteration 381, loss = 1.44998524\n",
      "Iteration 382, loss = 1.44707415\n",
      "Iteration 383, loss = 1.44421973\n",
      "Iteration 384, loss = 1.44132846\n",
      "Iteration 385, loss = 1.43850514\n",
      "Iteration 386, loss = 1.43564210\n",
      "Iteration 387, loss = 1.43272624\n",
      "Iteration 388, loss = 1.42992058\n",
      "Iteration 389, loss = 1.42709771\n",
      "Iteration 390, loss = 1.42423802\n",
      "Iteration 391, loss = 1.42123161\n",
      "Iteration 392, loss = 1.41844465\n",
      "Iteration 393, loss = 1.41544538\n",
      "Iteration 394, loss = 1.41258164\n",
      "Iteration 395, loss = 1.40959196\n",
      "Iteration 396, loss = 1.40661482\n",
      "Iteration 397, loss = 1.40361561\n",
      "Iteration 398, loss = 1.40055887\n",
      "Iteration 399, loss = 1.39768397\n",
      "Iteration 400, loss = 1.39455913\n",
      "Iteration 401, loss = 1.39178776\n",
      "Iteration 402, loss = 1.38849852\n",
      "Iteration 403, loss = 1.38565063\n",
      "Iteration 404, loss = 1.38243203\n",
      "Iteration 405, loss = 1.37930931\n",
      "Iteration 406, loss = 1.37624453\n",
      "Iteration 407, loss = 1.37309392\n",
      "Iteration 408, loss = 1.36992928\n",
      "Iteration 409, loss = 1.36686588\n",
      "Iteration 410, loss = 1.36375494\n",
      "Iteration 411, loss = 1.36050266\n",
      "Iteration 412, loss = 1.35725911\n",
      "Iteration 413, loss = 1.35405088\n",
      "Iteration 414, loss = 1.35086091\n",
      "Iteration 415, loss = 1.34760725\n",
      "Iteration 416, loss = 1.34434917\n",
      "Iteration 417, loss = 1.34084188\n",
      "Iteration 418, loss = 1.33740752\n",
      "Iteration 419, loss = 1.33417196\n",
      "Iteration 420, loss = 1.33083497\n",
      "Iteration 421, loss = 1.32742977\n",
      "Iteration 422, loss = 1.32406689\n",
      "Iteration 423, loss = 1.32073440\n",
      "Iteration 424, loss = 1.31748875\n",
      "Iteration 425, loss = 1.31393831\n",
      "Iteration 426, loss = 1.31050098\n",
      "Iteration 427, loss = 1.30711550\n",
      "Iteration 428, loss = 1.30376304\n",
      "Iteration 429, loss = 1.30023913\n",
      "Iteration 430, loss = 1.29686651\n",
      "Iteration 431, loss = 1.29324138\n",
      "Iteration 432, loss = 1.28982588\n",
      "Iteration 433, loss = 1.28621282\n",
      "Iteration 434, loss = 1.28271150\n",
      "Iteration 435, loss = 1.27899631\n",
      "Iteration 436, loss = 1.27551011\n",
      "Iteration 437, loss = 1.27188505\n",
      "Iteration 438, loss = 1.26835926\n",
      "Iteration 439, loss = 1.26492646\n",
      "Iteration 440, loss = 1.26123558\n",
      "Iteration 441, loss = 1.25774762\n",
      "Iteration 442, loss = 1.25418529\n",
      "Iteration 443, loss = 1.25053476\n",
      "Iteration 444, loss = 1.24695867\n",
      "Iteration 445, loss = 1.24352265\n",
      "Iteration 446, loss = 1.23965809\n",
      "Iteration 447, loss = 1.23648049\n",
      "Iteration 448, loss = 1.23290384\n",
      "Iteration 449, loss = 1.22907637\n",
      "Iteration 450, loss = 1.22544576\n",
      "Iteration 451, loss = 1.22184129\n",
      "Iteration 452, loss = 1.21825704\n",
      "Iteration 453, loss = 1.21475329\n",
      "Iteration 454, loss = 1.21109014\n",
      "Iteration 455, loss = 1.20757357\n",
      "Iteration 456, loss = 1.20398688\n",
      "Iteration 457, loss = 1.20037152\n",
      "Iteration 458, loss = 1.19679994\n",
      "Iteration 459, loss = 1.19343398\n",
      "Iteration 460, loss = 1.18956630\n",
      "Iteration 461, loss = 1.18598514\n",
      "Iteration 462, loss = 1.18256641\n",
      "Iteration 463, loss = 1.17894804\n",
      "Iteration 464, loss = 1.17526062\n",
      "Iteration 465, loss = 1.17160991\n",
      "Iteration 466, loss = 1.16811577\n",
      "Iteration 467, loss = 1.16466812\n",
      "Iteration 468, loss = 1.16082610\n",
      "Iteration 469, loss = 1.15711900\n",
      "Iteration 470, loss = 1.15365708\n",
      "Iteration 471, loss = 1.14996759\n",
      "Iteration 472, loss = 1.14633185\n",
      "Iteration 473, loss = 1.14279484\n",
      "Iteration 474, loss = 1.13920664\n",
      "Iteration 475, loss = 1.13594693\n",
      "Iteration 476, loss = 1.13217476\n",
      "Iteration 477, loss = 1.12832982\n",
      "Iteration 478, loss = 1.12467448\n",
      "Iteration 479, loss = 1.12097663\n",
      "Iteration 480, loss = 1.11748258\n",
      "Iteration 481, loss = 1.11374154\n",
      "Iteration 482, loss = 1.11010443\n",
      "Iteration 483, loss = 1.10629711\n",
      "Iteration 484, loss = 1.10244119\n",
      "Iteration 485, loss = 1.09905176\n",
      "Iteration 486, loss = 1.09535981\n",
      "Iteration 487, loss = 1.09150401\n",
      "Iteration 488, loss = 1.08764637\n",
      "Iteration 489, loss = 1.08382286\n",
      "Iteration 490, loss = 1.08017667\n",
      "Iteration 491, loss = 1.07623148\n",
      "Iteration 492, loss = 1.07262469\n",
      "Iteration 493, loss = 1.06871864\n",
      "Iteration 494, loss = 1.06527361\n",
      "Iteration 495, loss = 1.06110353\n",
      "Iteration 496, loss = 1.05763895\n",
      "Iteration 497, loss = 1.05353922\n",
      "Iteration 498, loss = 1.05004986\n",
      "Iteration 499, loss = 1.04551297\n",
      "Iteration 500, loss = 1.04181544\n",
      "Iteration 501, loss = 1.03738877\n",
      "Iteration 502, loss = 1.03361612\n",
      "Iteration 503, loss = 1.02994102\n",
      "Iteration 504, loss = 1.02554246\n",
      "Iteration 505, loss = 1.02207828\n",
      "Iteration 506, loss = 1.01773678\n",
      "Iteration 507, loss = 1.01368380\n",
      "Iteration 508, loss = 1.00942181\n",
      "Iteration 509, loss = 1.00529082\n",
      "Iteration 510, loss = 1.00124608\n",
      "Iteration 511, loss = 0.99729655\n",
      "Iteration 512, loss = 0.99307285\n",
      "Iteration 513, loss = 0.98899956\n",
      "Iteration 514, loss = 0.98526642\n",
      "Iteration 515, loss = 0.98105881\n",
      "Iteration 516, loss = 0.97721309\n",
      "Iteration 517, loss = 0.97310935\n",
      "Iteration 518, loss = 0.96906109\n",
      "Iteration 519, loss = 0.96500758\n",
      "Iteration 520, loss = 0.96113007\n",
      "Iteration 521, loss = 0.95688364\n",
      "Iteration 522, loss = 0.95295494\n",
      "Iteration 523, loss = 0.94869526\n",
      "Iteration 524, loss = 0.94455843\n",
      "Iteration 525, loss = 0.94071303\n",
      "Iteration 526, loss = 0.93608846\n",
      "Iteration 527, loss = 0.93150577\n",
      "Iteration 528, loss = 0.92678755\n",
      "Iteration 529, loss = 0.92215501\n",
      "Iteration 530, loss = 0.91744355\n",
      "Iteration 531, loss = 0.91277375\n",
      "Iteration 532, loss = 0.90851324\n",
      "Iteration 533, loss = 0.90392517\n",
      "Iteration 534, loss = 0.89972462\n",
      "Iteration 535, loss = 0.89547854\n",
      "Iteration 536, loss = 0.89074365\n",
      "Iteration 537, loss = 0.88685643\n",
      "Iteration 538, loss = 0.88242210\n",
      "Iteration 539, loss = 0.87793406\n",
      "Iteration 540, loss = 0.87427346\n",
      "Iteration 541, loss = 0.87031720\n",
      "Iteration 542, loss = 0.86585701\n",
      "Iteration 543, loss = 0.86172896\n",
      "Iteration 544, loss = 0.85734093\n",
      "Iteration 545, loss = 0.85396352\n",
      "Iteration 546, loss = 0.84902817\n",
      "Iteration 547, loss = 0.84512597\n",
      "Iteration 548, loss = 0.84088893\n",
      "Iteration 549, loss = 0.83719961\n",
      "Iteration 550, loss = 0.83343949\n",
      "Iteration 551, loss = 0.82883704\n",
      "Iteration 552, loss = 0.82527330\n",
      "Iteration 553, loss = 0.82100953\n",
      "Iteration 554, loss = 0.81745804\n",
      "Iteration 555, loss = 0.81311375\n",
      "Iteration 556, loss = 0.80915143\n",
      "Iteration 557, loss = 0.80524794\n",
      "Iteration 558, loss = 0.80173234\n",
      "Iteration 559, loss = 0.79720718\n",
      "Iteration 560, loss = 0.79348427\n",
      "Iteration 561, loss = 0.78973645\n",
      "Iteration 562, loss = 0.78553919\n",
      "Iteration 563, loss = 0.78169813\n",
      "Iteration 564, loss = 0.77782641\n",
      "Iteration 565, loss = 0.77429800\n",
      "Iteration 566, loss = 0.77060000\n",
      "Iteration 567, loss = 0.76696349\n",
      "Iteration 568, loss = 0.76328072\n",
      "Iteration 569, loss = 0.75958473\n",
      "Iteration 570, loss = 0.75626570\n",
      "Iteration 571, loss = 0.75284624\n",
      "Iteration 572, loss = 0.74901892\n",
      "Iteration 573, loss = 0.74598758\n",
      "Iteration 574, loss = 0.74182378\n",
      "Iteration 575, loss = 0.73852050\n",
      "Iteration 576, loss = 0.73471049\n",
      "Iteration 577, loss = 0.73137088\n",
      "Iteration 578, loss = 0.72776602\n",
      "Iteration 579, loss = 0.72434466\n",
      "Iteration 580, loss = 0.72074695\n",
      "Iteration 581, loss = 0.71709457\n",
      "Iteration 582, loss = 0.71350797\n",
      "Iteration 583, loss = 0.70949186\n",
      "Iteration 584, loss = 0.70571123\n",
      "Iteration 585, loss = 0.70224249\n",
      "Iteration 586, loss = 0.69782811\n",
      "Iteration 587, loss = 0.69386228\n",
      "Iteration 588, loss = 0.69010228\n",
      "Iteration 589, loss = 0.68635865\n",
      "Iteration 590, loss = 0.68204272\n",
      "Iteration 591, loss = 0.67686110\n",
      "Iteration 592, loss = 0.67166506\n",
      "Iteration 593, loss = 0.66653685\n",
      "Iteration 594, loss = 0.66200686\n",
      "Iteration 595, loss = 0.65732682\n",
      "Iteration 596, loss = 0.65247345\n",
      "Iteration 597, loss = 0.64730883\n",
      "Iteration 598, loss = 0.64231783\n",
      "Iteration 599, loss = 0.63723075\n",
      "Iteration 600, loss = 0.63230722\n",
      "Iteration 601, loss = 0.62828619\n",
      "Iteration 602, loss = 0.62338694\n",
      "Iteration 603, loss = 0.61874046\n",
      "Iteration 604, loss = 0.61509290\n",
      "Iteration 605, loss = 0.61110793\n",
      "Iteration 606, loss = 0.60645845\n",
      "Iteration 607, loss = 0.60232581\n",
      "Iteration 608, loss = 0.59921434\n",
      "Iteration 609, loss = 0.59485808\n",
      "Iteration 610, loss = 0.59098736\n",
      "Iteration 611, loss = 0.58727547\n",
      "Iteration 612, loss = 0.58453172\n",
      "Iteration 613, loss = 0.58063989\n",
      "Iteration 614, loss = 0.57764207\n",
      "Iteration 615, loss = 0.57387365\n",
      "Iteration 616, loss = 0.57021819\n",
      "Iteration 617, loss = 0.56725802\n",
      "Iteration 618, loss = 0.56471135\n",
      "Iteration 619, loss = 0.56055731\n",
      "Iteration 620, loss = 0.55799870\n",
      "Iteration 621, loss = 0.55415372\n",
      "Iteration 622, loss = 0.55143495\n",
      "Iteration 623, loss = 0.54836201\n",
      "Iteration 624, loss = 0.54514492\n",
      "Iteration 625, loss = 0.54274634\n",
      "Iteration 626, loss = 0.53936949\n",
      "Iteration 627, loss = 0.53759108\n",
      "Iteration 628, loss = 0.53383743\n",
      "Iteration 629, loss = 0.53093711\n",
      "Iteration 630, loss = 0.52844227\n",
      "Iteration 631, loss = 0.52551547\n",
      "Iteration 632, loss = 0.52257791\n",
      "Iteration 633, loss = 0.52016108\n",
      "Iteration 634, loss = 0.51776941\n",
      "Iteration 635, loss = 0.51531482\n",
      "Iteration 636, loss = 0.51295009\n",
      "Iteration 637, loss = 0.51032332\n",
      "Iteration 638, loss = 0.50762714\n",
      "Iteration 639, loss = 0.50535939\n",
      "Iteration 640, loss = 0.50239683\n",
      "Iteration 641, loss = 0.49991858\n",
      "Iteration 642, loss = 0.49915245\n",
      "Iteration 643, loss = 0.49559858\n",
      "Iteration 644, loss = 0.49303151\n",
      "Iteration 645, loss = 0.49081467\n",
      "Iteration 646, loss = 0.48845519\n",
      "Iteration 647, loss = 0.48612897\n",
      "Iteration 648, loss = 0.48397089\n",
      "Iteration 649, loss = 0.48215783\n",
      "Iteration 650, loss = 0.47938469\n",
      "Iteration 651, loss = 0.47754039\n",
      "Iteration 652, loss = 0.47489234\n",
      "Iteration 653, loss = 0.47281774\n",
      "Iteration 654, loss = 0.47070107\n",
      "Iteration 655, loss = 0.46945972\n",
      "Iteration 656, loss = 0.46645627\n",
      "Iteration 657, loss = 0.46459470\n",
      "Iteration 658, loss = 0.46199862\n",
      "Iteration 659, loss = 0.46039313\n",
      "Iteration 660, loss = 0.45890319\n",
      "Iteration 661, loss = 0.45658760\n",
      "Iteration 662, loss = 0.45373966\n",
      "Iteration 663, loss = 0.45192841\n",
      "Iteration 664, loss = 0.45026068\n",
      "Iteration 665, loss = 0.44831561\n",
      "Iteration 666, loss = 0.44608572\n",
      "Iteration 667, loss = 0.44424026\n",
      "Iteration 668, loss = 0.44215545\n",
      "Iteration 669, loss = 0.44066315\n",
      "Iteration 670, loss = 0.43834077\n",
      "Iteration 671, loss = 0.43635522\n",
      "Iteration 672, loss = 0.43474387\n",
      "Iteration 673, loss = 0.43309520\n",
      "Iteration 674, loss = 0.43049915\n",
      "Iteration 675, loss = 0.42869514\n",
      "Iteration 676, loss = 0.42731221\n",
      "Iteration 677, loss = 0.42522142\n",
      "Iteration 678, loss = 0.42376752\n",
      "Iteration 679, loss = 0.42147686\n",
      "Iteration 680, loss = 0.41996726\n",
      "Iteration 681, loss = 0.41790988\n",
      "Iteration 682, loss = 0.41666420\n",
      "Iteration 683, loss = 0.41502459\n",
      "Iteration 684, loss = 0.41333358\n",
      "Iteration 685, loss = 0.41117835\n",
      "Iteration 686, loss = 0.40971183\n",
      "Iteration 687, loss = 0.40781087\n",
      "Iteration 688, loss = 0.40590237\n",
      "Iteration 689, loss = 0.40431710\n",
      "Iteration 690, loss = 0.40330609\n",
      "Iteration 691, loss = 0.40163653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 692, loss = 0.40023683\n",
      "Iteration 693, loss = 0.39819621\n",
      "Iteration 694, loss = 0.39660245\n",
      "Iteration 695, loss = 0.39545706\n",
      "Iteration 696, loss = 0.39296015\n",
      "Iteration 697, loss = 0.39224899\n",
      "Iteration 698, loss = 0.39033913\n",
      "Iteration 699, loss = 0.38900563\n",
      "Iteration 700, loss = 0.38707638\n",
      "Iteration 701, loss = 0.38516366\n",
      "Iteration 702, loss = 0.38436911\n",
      "Iteration 703, loss = 0.38283302\n",
      "Iteration 704, loss = 0.38124374\n",
      "Iteration 705, loss = 0.38092415\n",
      "Iteration 706, loss = 0.37877664\n",
      "Iteration 707, loss = 0.37748879\n",
      "Iteration 708, loss = 0.37529151\n",
      "Iteration 709, loss = 0.37313752\n",
      "Iteration 710, loss = 0.37186313\n",
      "Iteration 711, loss = 0.37058789\n",
      "Iteration 712, loss = 0.36870390\n",
      "Iteration 713, loss = 0.36741592\n",
      "Iteration 714, loss = 0.36567350\n",
      "Iteration 715, loss = 0.36437353\n",
      "Iteration 716, loss = 0.36269460\n",
      "Iteration 717, loss = 0.36120533\n",
      "Iteration 718, loss = 0.36036134\n",
      "Iteration 719, loss = 0.35938475\n",
      "Iteration 720, loss = 0.35735241\n",
      "Iteration 721, loss = 0.35599631\n",
      "Iteration 722, loss = 0.35422633\n",
      "Iteration 723, loss = 0.35265273\n",
      "Iteration 724, loss = 0.35119549\n",
      "Iteration 725, loss = 0.34993739\n",
      "Iteration 726, loss = 0.34828532\n",
      "Iteration 727, loss = 0.34702828\n",
      "Iteration 728, loss = 0.34564163\n",
      "Iteration 729, loss = 0.34531641\n",
      "Iteration 730, loss = 0.34607670\n",
      "Iteration 731, loss = 0.34205644\n",
      "Iteration 732, loss = 0.34145340\n",
      "Iteration 733, loss = 0.33935401\n",
      "Iteration 734, loss = 0.33832351\n",
      "Iteration 735, loss = 0.33652861\n",
      "Iteration 736, loss = 0.33576314\n",
      "Iteration 737, loss = 0.33374528\n",
      "Iteration 738, loss = 0.33276110\n",
      "Iteration 739, loss = 0.33360528\n",
      "Iteration 740, loss = 0.32967063\n",
      "Iteration 741, loss = 0.32904742\n",
      "Iteration 742, loss = 0.32734107\n",
      "Iteration 743, loss = 0.32697529\n",
      "Iteration 744, loss = 0.32572772\n",
      "Iteration 745, loss = 0.32518996\n",
      "Iteration 746, loss = 0.32273061\n",
      "Iteration 747, loss = 0.32082208\n",
      "Iteration 748, loss = 0.32044096\n",
      "Iteration 749, loss = 0.31892401\n",
      "Iteration 750, loss = 0.31829615\n",
      "Iteration 751, loss = 0.31735026\n",
      "Iteration 752, loss = 0.31557091\n",
      "Iteration 753, loss = 0.31412569\n",
      "Iteration 754, loss = 0.31241401\n",
      "Iteration 755, loss = 0.31124590\n",
      "Iteration 756, loss = 0.31037734\n",
      "Iteration 757, loss = 0.30896088\n",
      "Iteration 758, loss = 0.30735017\n",
      "Iteration 759, loss = 0.30610895\n",
      "Iteration 760, loss = 0.30454273\n",
      "Iteration 761, loss = 0.30324702\n",
      "Iteration 762, loss = 0.30217145\n",
      "Iteration 763, loss = 0.30084590\n",
      "Iteration 764, loss = 0.30051285\n",
      "Iteration 765, loss = 0.29828750\n",
      "Iteration 766, loss = 0.29783384\n",
      "Iteration 767, loss = 0.29651875\n",
      "Iteration 768, loss = 0.29555531\n",
      "Iteration 769, loss = 0.29510500\n",
      "Iteration 770, loss = 0.29309618\n",
      "Iteration 771, loss = 0.29222914\n",
      "Iteration 772, loss = 0.29028788\n",
      "Iteration 773, loss = 0.28867874\n",
      "Iteration 774, loss = 0.28672572\n",
      "Iteration 775, loss = 0.28690789\n",
      "Iteration 776, loss = 0.28483453\n",
      "Iteration 777, loss = 0.28404202\n",
      "Iteration 778, loss = 0.28203753\n",
      "Iteration 779, loss = 0.28123052\n",
      "Iteration 780, loss = 0.28017496\n",
      "Iteration 781, loss = 0.27952446\n",
      "Iteration 782, loss = 0.27927309\n",
      "Iteration 783, loss = 0.27831198\n",
      "Iteration 784, loss = 0.27484606\n",
      "Iteration 785, loss = 0.27381933\n",
      "Iteration 786, loss = 0.27253743\n",
      "Iteration 787, loss = 0.27080152\n",
      "Iteration 788, loss = 0.26934662\n",
      "Iteration 789, loss = 0.26815975\n",
      "Iteration 790, loss = 0.26831965\n",
      "Iteration 791, loss = 0.26642768\n",
      "Iteration 792, loss = 0.26416480\n",
      "Iteration 793, loss = 0.26360449\n",
      "Iteration 794, loss = 0.26250966\n",
      "Iteration 795, loss = 0.26080184\n",
      "Iteration 796, loss = 0.25960232\n",
      "Iteration 797, loss = 0.25779256\n",
      "Iteration 798, loss = 0.25631448\n",
      "Iteration 799, loss = 0.25544629\n",
      "Iteration 800, loss = 0.25515712\n",
      "Iteration 801, loss = 0.25326784\n",
      "Iteration 802, loss = 0.25147191\n",
      "Iteration 803, loss = 0.25047650\n",
      "Iteration 804, loss = 0.24863054\n",
      "Iteration 805, loss = 0.24756423\n",
      "Iteration 806, loss = 0.24690690\n",
      "Iteration 807, loss = 0.24461190\n",
      "Iteration 808, loss = 0.24384829\n",
      "Iteration 809, loss = 0.24180122\n",
      "Iteration 810, loss = 0.24204180\n",
      "Iteration 811, loss = 0.23851355\n",
      "Iteration 812, loss = 0.23727955\n",
      "Iteration 813, loss = 0.23565099\n",
      "Iteration 814, loss = 0.23417756\n",
      "Iteration 815, loss = 0.23434364\n",
      "Iteration 816, loss = 0.23102211\n",
      "Iteration 817, loss = 0.22876446\n",
      "Iteration 818, loss = 0.22732031\n",
      "Iteration 819, loss = 0.22748302\n",
      "Iteration 820, loss = 0.22422489\n",
      "Iteration 821, loss = 0.22266376\n",
      "Iteration 822, loss = 0.22097026\n",
      "Iteration 823, loss = 0.21949200\n",
      "Iteration 824, loss = 0.21827955\n",
      "Iteration 825, loss = 0.21659442\n",
      "Iteration 826, loss = 0.21811572\n",
      "Iteration 827, loss = 0.21374513\n",
      "Iteration 828, loss = 0.21610037\n",
      "Iteration 829, loss = 0.21223579\n",
      "Iteration 830, loss = 0.20918751\n",
      "Iteration 831, loss = 0.20868743\n",
      "Iteration 832, loss = 0.20734699\n",
      "Iteration 833, loss = 0.20522881\n",
      "Iteration 834, loss = 0.20415931\n",
      "Iteration 835, loss = 0.20439127\n",
      "Iteration 836, loss = 0.20177105\n",
      "Iteration 837, loss = 0.20150489\n",
      "Iteration 838, loss = 0.19974447\n",
      "Iteration 839, loss = 0.19910175\n",
      "Iteration 840, loss = 0.19717838\n",
      "Iteration 841, loss = 0.19487677\n",
      "Iteration 842, loss = 0.19418538\n",
      "Iteration 843, loss = 0.19309561\n",
      "Iteration 844, loss = 0.19225396\n",
      "Iteration 845, loss = 0.19134713\n",
      "Iteration 846, loss = 0.18915186\n",
      "Iteration 847, loss = 0.18825036\n",
      "Iteration 848, loss = 0.18711134\n",
      "Iteration 849, loss = 0.18546278\n",
      "Iteration 850, loss = 0.18440770\n",
      "Iteration 851, loss = 0.18367719\n",
      "Iteration 852, loss = 0.18233471\n",
      "Iteration 853, loss = 0.18153335\n",
      "Iteration 854, loss = 0.18277595\n",
      "Iteration 855, loss = 0.17864047\n",
      "Iteration 856, loss = 0.17848745\n",
      "Iteration 857, loss = 0.17756711\n",
      "Iteration 858, loss = 0.17613236\n",
      "Iteration 859, loss = 0.17554987\n",
      "Iteration 860, loss = 0.17556704\n",
      "Iteration 861, loss = 0.17272212\n",
      "Iteration 862, loss = 0.17143158\n",
      "Iteration 863, loss = 0.17042097\n",
      "Iteration 864, loss = 0.16959076\n",
      "Iteration 865, loss = 0.16807768\n",
      "Iteration 866, loss = 0.16724016\n",
      "Iteration 867, loss = 0.16813562\n",
      "Iteration 868, loss = 0.16560148\n",
      "Iteration 869, loss = 0.16438187\n",
      "Iteration 870, loss = 0.16299374\n",
      "Iteration 871, loss = 0.16316117\n",
      "Iteration 872, loss = 0.16529431\n",
      "Iteration 873, loss = 0.16013798\n",
      "Iteration 874, loss = 0.16080450\n",
      "Iteration 875, loss = 0.15989848\n",
      "Iteration 876, loss = 0.15915619\n",
      "Iteration 877, loss = 0.15728506\n",
      "Iteration 878, loss = 0.15588404\n",
      "Iteration 879, loss = 0.15602289\n",
      "Iteration 880, loss = 0.15517266\n",
      "Iteration 881, loss = 0.15363878\n",
      "Iteration 882, loss = 0.15270472\n",
      "Iteration 883, loss = 0.15181458\n",
      "Iteration 884, loss = 0.15136262\n",
      "Iteration 885, loss = 0.15040818\n",
      "Iteration 886, loss = 0.14928210\n",
      "Iteration 887, loss = 0.15003247\n",
      "Iteration 888, loss = 0.14758983\n",
      "Iteration 889, loss = 0.14714847\n",
      "Iteration 890, loss = 0.14586690\n",
      "Iteration 891, loss = 0.14592939\n",
      "Iteration 892, loss = 0.14542051\n",
      "Iteration 893, loss = 0.14424565\n",
      "Iteration 894, loss = 0.14357229\n",
      "Iteration 895, loss = 0.14321238\n",
      "Iteration 896, loss = 0.14255758\n",
      "Iteration 897, loss = 0.14062947\n",
      "Iteration 898, loss = 0.13991571\n",
      "Iteration 899, loss = 0.13933181\n",
      "Iteration 900, loss = 0.13950490\n",
      "Iteration 901, loss = 0.13836504\n",
      "Iteration 902, loss = 0.13772601\n",
      "Iteration 903, loss = 0.13759082\n",
      "Iteration 904, loss = 0.13570942\n",
      "Iteration 905, loss = 0.13483024\n",
      "Iteration 906, loss = 0.13529413\n",
      "Iteration 907, loss = 0.13614171\n",
      "Iteration 908, loss = 0.13385540\n",
      "Iteration 909, loss = 0.13253373\n",
      "Iteration 910, loss = 0.13171680\n",
      "Iteration 911, loss = 0.13288626\n",
      "Iteration 912, loss = 0.13130034\n",
      "Iteration 913, loss = 0.13130225\n",
      "Iteration 914, loss = 0.12958024\n",
      "Iteration 915, loss = 0.12969678\n",
      "Iteration 916, loss = 0.12888382\n",
      "Iteration 917, loss = 0.12929192\n",
      "Iteration 918, loss = 0.12753672\n",
      "Iteration 919, loss = 0.12679318\n",
      "Iteration 920, loss = 0.12626727\n",
      "Iteration 921, loss = 0.12558826\n",
      "Iteration 922, loss = 0.12520418\n",
      "Iteration 923, loss = 0.12431185\n",
      "Iteration 924, loss = 0.12578200\n",
      "Iteration 925, loss = 0.12423220\n",
      "Iteration 926, loss = 0.12451940\n",
      "Iteration 927, loss = 0.12322537\n",
      "Iteration 928, loss = 0.12172995\n",
      "Iteration 929, loss = 0.12144669\n",
      "Iteration 930, loss = 0.12237191\n",
      "Iteration 931, loss = 0.11972852\n",
      "Iteration 932, loss = 0.11917143\n",
      "Iteration 933, loss = 0.11869488\n",
      "Iteration 934, loss = 0.11824471\n",
      "Iteration 935, loss = 0.11808405\n",
      "Iteration 936, loss = 0.11781720\n",
      "Iteration 937, loss = 0.11727612\n",
      "Iteration 938, loss = 0.11643096\n",
      "Iteration 939, loss = 0.11625017\n",
      "Iteration 940, loss = 0.11501528\n",
      "Iteration 941, loss = 0.11452682\n",
      "Iteration 942, loss = 0.11430247\n",
      "Iteration 943, loss = 0.11428747\n",
      "Iteration 944, loss = 0.11317245\n",
      "Iteration 945, loss = 0.11239167\n",
      "Iteration 946, loss = 0.11269130\n",
      "Iteration 947, loss = 0.11311864\n",
      "Iteration 948, loss = 0.11341229\n",
      "Iteration 949, loss = 0.11126209\n",
      "Iteration 950, loss = 0.11077614\n",
      "Iteration 951, loss = 0.10972837\n",
      "Iteration 952, loss = 0.10958942\n",
      "Iteration 953, loss = 0.11005241\n",
      "Iteration 954, loss = 0.11032862\n",
      "Iteration 955, loss = 0.10779897\n",
      "Iteration 956, loss = 0.10804759\n",
      "Iteration 957, loss = 0.10720325\n",
      "Iteration 958, loss = 0.10867107\n",
      "Iteration 959, loss = 0.10712044\n",
      "Iteration 960, loss = 0.10630103\n",
      "Iteration 961, loss = 0.10858219\n",
      "Iteration 962, loss = 0.10578971\n",
      "Iteration 963, loss = 0.10531955\n",
      "Iteration 964, loss = 0.10461875\n",
      "Iteration 965, loss = 0.10380492\n",
      "Iteration 966, loss = 0.10338651\n",
      "Iteration 967, loss = 0.10325530\n",
      "Iteration 968, loss = 0.10446268\n",
      "Iteration 969, loss = 0.10341540\n",
      "Iteration 970, loss = 0.10195034\n",
      "Iteration 971, loss = 0.10205143\n",
      "Iteration 972, loss = 0.10273647\n",
      "Iteration 973, loss = 0.10123668\n",
      "Iteration 974, loss = 0.10079715\n",
      "Iteration 975, loss = 0.10029569\n",
      "Iteration 976, loss = 0.10014818\n",
      "Iteration 977, loss = 0.09955444\n",
      "Iteration 978, loss = 0.09870740\n",
      "Iteration 979, loss = 0.09838413\n",
      "Iteration 980, loss = 0.09844987\n",
      "Iteration 981, loss = 0.09875324\n",
      "Iteration 982, loss = 0.09846899\n",
      "Iteration 983, loss = 0.09750284\n",
      "Iteration 984, loss = 0.09794952\n",
      "Iteration 985, loss = 0.09696890\n",
      "Iteration 986, loss = 0.09637737\n",
      "Iteration 987, loss = 0.09554921\n",
      "Iteration 988, loss = 0.09561242\n",
      "Iteration 989, loss = 0.09562481\n",
      "Iteration 990, loss = 0.09547032\n",
      "Iteration 991, loss = 0.09476849\n",
      "Iteration 992, loss = 0.09433098\n",
      "Iteration 993, loss = 0.09399269\n",
      "Iteration 994, loss = 0.09400906\n",
      "Iteration 995, loss = 0.09286791\n",
      "Iteration 996, loss = 0.09253898\n",
      "Iteration 997, loss = 0.09296754\n",
      "Iteration 998, loss = 0.09350844\n",
      "Iteration 999, loss = 0.09232866\n",
      "Iteration 1000, loss = 0.09144810\n",
      "Iteration 1001, loss = 0.09140913\n",
      "Iteration 1002, loss = 0.09176417\n",
      "Iteration 1003, loss = 0.09209882\n",
      "Iteration 1004, loss = 0.09192481\n",
      "Iteration 1005, loss = 0.08945038\n",
      "Iteration 1006, loss = 0.08986740\n",
      "Iteration 1007, loss = 0.08887255\n",
      "Iteration 1008, loss = 0.08941244\n",
      "Iteration 1009, loss = 0.08856682\n",
      "Iteration 1010, loss = 0.08946868\n",
      "Iteration 1011, loss = 0.08771569\n",
      "Iteration 1012, loss = 0.08768595\n",
      "Iteration 1013, loss = 0.08795983\n",
      "Iteration 1014, loss = 0.08729904\n",
      "Iteration 1015, loss = 0.08799234\n",
      "Iteration 1016, loss = 0.08633674\n",
      "Iteration 1017, loss = 0.08618735\n",
      "Iteration 1018, loss = 0.08687426\n",
      "Iteration 1019, loss = 0.08580929\n",
      "Iteration 1020, loss = 0.08563080\n",
      "Iteration 1021, loss = 0.08611513\n",
      "Iteration 1022, loss = 0.08423068\n",
      "Iteration 1023, loss = 0.08415656\n",
      "Iteration 1024, loss = 0.08456199\n",
      "Iteration 1025, loss = 0.08385325\n",
      "Iteration 1026, loss = 0.08320671\n",
      "Iteration 1027, loss = 0.08351833\n",
      "Iteration 1028, loss = 0.08389157\n",
      "Iteration 1029, loss = 0.08208319\n",
      "Iteration 1030, loss = 0.08203732\n",
      "Iteration 1031, loss = 0.08216222\n",
      "Iteration 1032, loss = 0.08212350\n",
      "Iteration 1033, loss = 0.08098597\n",
      "Iteration 1034, loss = 0.08078305\n",
      "Iteration 1035, loss = 0.08098307\n",
      "Iteration 1036, loss = 0.08116164\n",
      "Iteration 1037, loss = 0.08061828\n",
      "Iteration 1038, loss = 0.08039309\n",
      "Iteration 1039, loss = 0.07993826\n",
      "Iteration 1040, loss = 0.07948483\n",
      "Iteration 1041, loss = 0.07920272\n",
      "Iteration 1042, loss = 0.07872934\n",
      "Iteration 1043, loss = 0.07940650\n",
      "Iteration 1044, loss = 0.07889555\n",
      "Iteration 1045, loss = 0.07894393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1046, loss = 0.07828237\n",
      "Iteration 1047, loss = 0.07714392\n",
      "Iteration 1048, loss = 0.07738359\n",
      "Iteration 1049, loss = 0.07756509\n",
      "Iteration 1050, loss = 0.07801844\n",
      "Iteration 1051, loss = 0.07723767\n",
      "Iteration 1052, loss = 0.07705804\n",
      "Iteration 1053, loss = 0.07851131\n",
      "Iteration 1054, loss = 0.07571482\n",
      "Iteration 1055, loss = 0.07563744\n",
      "Iteration 1056, loss = 0.07452739\n",
      "Iteration 1057, loss = 0.07539234\n",
      "Iteration 1058, loss = 0.07510152\n",
      "Iteration 1059, loss = 0.07532284\n",
      "Iteration 1060, loss = 0.07478210\n",
      "Iteration 1061, loss = 0.07369527\n",
      "Iteration 1062, loss = 0.07397734\n",
      "Iteration 1063, loss = 0.07287230\n",
      "Iteration 1064, loss = 0.07304981\n",
      "Iteration 1065, loss = 0.07225488\n",
      "Iteration 1066, loss = 0.07449875\n",
      "Iteration 1067, loss = 0.07311436\n",
      "Iteration 1068, loss = 0.07170614\n",
      "Iteration 1069, loss = 0.07151070\n",
      "Iteration 1070, loss = 0.07168837\n",
      "Iteration 1071, loss = 0.07096782\n",
      "Iteration 1072, loss = 0.07048255\n",
      "Iteration 1073, loss = 0.07011695\n",
      "Iteration 1074, loss = 0.07056032\n",
      "Iteration 1075, loss = 0.06989073\n",
      "Iteration 1076, loss = 0.06933770\n",
      "Iteration 1077, loss = 0.06940872\n",
      "Iteration 1078, loss = 0.06994972\n",
      "Iteration 1079, loss = 0.06861011\n",
      "Iteration 1080, loss = 0.06850884\n",
      "Iteration 1081, loss = 0.06856307\n",
      "Iteration 1082, loss = 0.06830893\n",
      "Iteration 1083, loss = 0.06816103\n",
      "Iteration 1084, loss = 0.06801024\n",
      "Iteration 1085, loss = 0.06791376\n",
      "Iteration 1086, loss = 0.06869920\n",
      "Iteration 1087, loss = 0.06739381\n",
      "Iteration 1088, loss = 0.06740717\n",
      "Iteration 1089, loss = 0.06630011\n",
      "Iteration 1090, loss = 0.06621970\n",
      "Iteration 1091, loss = 0.06778298\n",
      "Iteration 1092, loss = 0.06682628\n",
      "Iteration 1093, loss = 0.06621586\n",
      "Iteration 1094, loss = 0.06617153\n",
      "Iteration 1095, loss = 0.06515622\n",
      "Iteration 1096, loss = 0.06626835\n",
      "Iteration 1097, loss = 0.06504289\n",
      "Iteration 1098, loss = 0.06441545\n",
      "Iteration 1099, loss = 0.06438299\n",
      "Iteration 1100, loss = 0.06377955\n",
      "Iteration 1101, loss = 0.06447484\n",
      "Iteration 1102, loss = 0.06455717\n",
      "Iteration 1103, loss = 0.06319722\n",
      "Iteration 1104, loss = 0.06325560\n",
      "Iteration 1105, loss = 0.06303582\n",
      "Iteration 1106, loss = 0.06255971\n",
      "Iteration 1107, loss = 0.06271346\n",
      "Iteration 1108, loss = 0.06175144\n",
      "Iteration 1109, loss = 0.06225835\n",
      "Iteration 1110, loss = 0.06167202\n",
      "Iteration 1111, loss = 0.06200990\n",
      "Iteration 1112, loss = 0.06155999\n",
      "Iteration 1113, loss = 0.06082776\n",
      "Iteration 1114, loss = 0.06092777\n",
      "Iteration 1115, loss = 0.06091621\n",
      "Iteration 1116, loss = 0.06053490\n",
      "Iteration 1117, loss = 0.06183423\n",
      "Iteration 1118, loss = 0.06004301\n",
      "Iteration 1119, loss = 0.06013320\n",
      "Iteration 1120, loss = 0.05963507\n",
      "Iteration 1121, loss = 0.05995469\n",
      "Iteration 1122, loss = 0.05906204\n",
      "Iteration 1123, loss = 0.05916297\n",
      "Iteration 1124, loss = 0.05934537\n",
      "Iteration 1125, loss = 0.05838072\n",
      "Iteration 1126, loss = 0.06026339\n",
      "Iteration 1127, loss = 0.05993930\n",
      "Iteration 1128, loss = 0.06046641\n",
      "Iteration 1129, loss = 0.05806614\n",
      "Iteration 1130, loss = 0.05764762\n",
      "Iteration 1131, loss = 0.05780839\n",
      "Iteration 1132, loss = 0.05771875\n",
      "Iteration 1133, loss = 0.05685287\n",
      "Iteration 1134, loss = 0.05729785\n",
      "Iteration 1135, loss = 0.05807659\n",
      "Iteration 1136, loss = 0.05803113\n",
      "Iteration 1137, loss = 0.05693873\n",
      "Iteration 1138, loss = 0.05614732\n",
      "Iteration 1139, loss = 0.05626302\n",
      "Iteration 1140, loss = 0.05578078\n",
      "Iteration 1141, loss = 0.05597327\n",
      "Iteration 1142, loss = 0.05594340\n",
      "Iteration 1143, loss = 0.05591070\n",
      "Iteration 1144, loss = 0.05551816\n",
      "Iteration 1145, loss = 0.05524287\n",
      "Iteration 1146, loss = 0.05492304\n",
      "Iteration 1147, loss = 0.05489221\n",
      "Iteration 1148, loss = 0.05436632\n",
      "Iteration 1149, loss = 0.05478903\n",
      "Iteration 1150, loss = 0.05643441\n",
      "Iteration 1151, loss = 0.05383033\n",
      "Iteration 1152, loss = 0.05444490\n",
      "Iteration 1153, loss = 0.05383511\n",
      "Iteration 1154, loss = 0.05332288\n",
      "Iteration 1155, loss = 0.05334119\n",
      "Iteration 1156, loss = 0.05321345\n",
      "Iteration 1157, loss = 0.05306842\n",
      "Iteration 1158, loss = 0.05276198\n",
      "Iteration 1159, loss = 0.05302987\n",
      "Iteration 1160, loss = 0.05259297\n",
      "Iteration 1161, loss = 0.05326904\n",
      "Iteration 1162, loss = 0.05221217\n",
      "Iteration 1163, loss = 0.05259764\n",
      "Iteration 1164, loss = 0.05303538\n",
      "Iteration 1165, loss = 0.05299858\n",
      "Iteration 1166, loss = 0.05193355\n",
      "Iteration 1167, loss = 0.05162512\n",
      "Iteration 1168, loss = 0.05165146\n",
      "Iteration 1169, loss = 0.05111403\n",
      "Iteration 1170, loss = 0.05273368\n",
      "Iteration 1171, loss = 0.05122696\n",
      "Iteration 1172, loss = 0.05068408\n",
      "Iteration 1173, loss = 0.05144070\n",
      "Iteration 1174, loss = 0.05022347\n",
      "Iteration 1175, loss = 0.05019797\n",
      "Iteration 1176, loss = 0.04988178\n",
      "Iteration 1177, loss = 0.05092862\n",
      "Iteration 1178, loss = 0.05027211\n",
      "Iteration 1179, loss = 0.04962696\n",
      "Iteration 1180, loss = 0.04959377\n",
      "Iteration 1181, loss = 0.04957014\n",
      "Iteration 1182, loss = 0.04935623\n",
      "Iteration 1183, loss = 0.04914868\n",
      "Iteration 1184, loss = 0.04906132\n",
      "Iteration 1185, loss = 0.04897341\n",
      "Iteration 1186, loss = 0.04895635\n",
      "Iteration 1187, loss = 0.04849512\n",
      "Iteration 1188, loss = 0.04862490\n",
      "Iteration 1189, loss = 0.04907127\n",
      "Iteration 1190, loss = 0.04797583\n",
      "Iteration 1191, loss = 0.04826282\n",
      "Iteration 1192, loss = 0.04867339\n",
      "Iteration 1193, loss = 0.04859969\n",
      "Iteration 1194, loss = 0.04733501\n",
      "Iteration 1195, loss = 0.04796178\n",
      "Iteration 1196, loss = 0.04803544\n",
      "Iteration 1197, loss = 0.04801777\n",
      "Iteration 1198, loss = 0.04828268\n",
      "Iteration 1199, loss = 0.04671802\n",
      "Iteration 1200, loss = 0.04705517\n",
      "Iteration 1201, loss = 0.04749380\n",
      "Iteration 1202, loss = 0.04836648\n",
      "Iteration 1203, loss = 0.04631820\n",
      "Iteration 1204, loss = 0.04632814\n",
      "Iteration 1205, loss = 0.04637085\n",
      "Iteration 1206, loss = 0.04564666\n",
      "Iteration 1207, loss = 0.04629329\n",
      "Iteration 1208, loss = 0.04552938\n",
      "Iteration 1209, loss = 0.04637764\n",
      "Iteration 1210, loss = 0.04498144\n",
      "Iteration 1211, loss = 0.04559553\n",
      "Iteration 1212, loss = 0.04510692\n",
      "Iteration 1213, loss = 0.04555495\n",
      "Iteration 1214, loss = 0.04523639\n",
      "Iteration 1215, loss = 0.04429697\n",
      "Iteration 1216, loss = 0.04408429\n",
      "Iteration 1217, loss = 0.04431103\n",
      "Iteration 1218, loss = 0.04546224\n",
      "Iteration 1219, loss = 0.04497215\n",
      "Iteration 1220, loss = 0.04424026\n",
      "Iteration 1221, loss = 0.04373004\n",
      "Iteration 1222, loss = 0.04382765\n",
      "Iteration 1223, loss = 0.04325680\n",
      "Iteration 1224, loss = 0.04359416\n",
      "Iteration 1225, loss = 0.04296487\n",
      "Iteration 1226, loss = 0.04369178\n",
      "Iteration 1227, loss = 0.04338066\n",
      "Iteration 1228, loss = 0.04364839\n",
      "Iteration 1229, loss = 0.04277040\n",
      "Iteration 1230, loss = 0.04325903\n",
      "Iteration 1231, loss = 0.04257718\n",
      "Iteration 1232, loss = 0.04244144\n",
      "Iteration 1233, loss = 0.04241611\n",
      "Iteration 1234, loss = 0.04330357\n",
      "Iteration 1235, loss = 0.04186193\n",
      "Iteration 1236, loss = 0.04178331\n",
      "Iteration 1237, loss = 0.04180928\n",
      "Iteration 1238, loss = 0.04148105\n",
      "Iteration 1239, loss = 0.04123504\n",
      "Iteration 1240, loss = 0.04161228\n",
      "Iteration 1241, loss = 0.04176027\n",
      "Iteration 1242, loss = 0.04139152\n",
      "Iteration 1243, loss = 0.04063773\n",
      "Iteration 1244, loss = 0.04186696\n",
      "Iteration 1245, loss = 0.04069235\n",
      "Iteration 1246, loss = 0.04043041\n",
      "Iteration 1247, loss = 0.04102533\n",
      "Iteration 1248, loss = 0.04105234\n",
      "Iteration 1249, loss = 0.04021935\n",
      "Iteration 1250, loss = 0.04063580\n",
      "Iteration 1251, loss = 0.04258069\n",
      "Iteration 1252, loss = 0.04075958\n",
      "Iteration 1253, loss = 0.04018516\n",
      "Iteration 1254, loss = 0.04044389\n",
      "Iteration 1255, loss = 0.03968797\n",
      "Iteration 1256, loss = 0.03919799\n",
      "Iteration 1257, loss = 0.03919528\n",
      "Iteration 1258, loss = 0.03928422\n",
      "Iteration 1259, loss = 0.03902984\n",
      "Iteration 1260, loss = 0.03891547\n",
      "Iteration 1261, loss = 0.03837038\n",
      "Iteration 1262, loss = 0.03862172\n",
      "Iteration 1263, loss = 0.03903775\n",
      "Iteration 1264, loss = 0.03885910\n",
      "Iteration 1265, loss = 0.03851627\n",
      "Iteration 1266, loss = 0.03868636\n",
      "Iteration 1267, loss = 0.03909382\n",
      "Iteration 1268, loss = 0.03984863\n",
      "Iteration 1269, loss = 0.03791342\n",
      "Iteration 1270, loss = 0.03849885\n",
      "Iteration 1271, loss = 0.03740486\n",
      "Iteration 1272, loss = 0.03780045\n",
      "Iteration 1273, loss = 0.03765106\n",
      "Iteration 1274, loss = 0.03750815\n",
      "Iteration 1275, loss = 0.03730022\n",
      "Iteration 1276, loss = 0.03740996\n",
      "Iteration 1277, loss = 0.03828722\n",
      "Iteration 1278, loss = 0.03685143\n",
      "Iteration 1279, loss = 0.03704752\n",
      "Iteration 1280, loss = 0.03681289\n",
      "Iteration 1281, loss = 0.03656391\n",
      "Iteration 1282, loss = 0.03692094\n",
      "Iteration 1283, loss = 0.03611218\n",
      "Iteration 1284, loss = 0.03626365\n",
      "Iteration 1285, loss = 0.03638710\n",
      "Iteration 1286, loss = 0.03629575\n",
      "Iteration 1287, loss = 0.03596065\n",
      "Iteration 1288, loss = 0.03582543\n",
      "Iteration 1289, loss = 0.03548550\n",
      "Iteration 1290, loss = 0.03618544\n",
      "Iteration 1291, loss = 0.03634906\n",
      "Iteration 1292, loss = 0.03535132\n",
      "Iteration 1293, loss = 0.03527899\n",
      "Iteration 1294, loss = 0.03502312\n",
      "Iteration 1295, loss = 0.03631953\n",
      "Iteration 1296, loss = 0.03481721\n",
      "Iteration 1297, loss = 0.03468602\n",
      "Iteration 1298, loss = 0.03461890\n",
      "Iteration 1299, loss = 0.03486434\n",
      "Iteration 1300, loss = 0.03447468\n",
      "Iteration 1301, loss = 0.03481282\n",
      "Iteration 1302, loss = 0.03464429\n",
      "Iteration 1303, loss = 0.03431838\n",
      "Iteration 1304, loss = 0.03438278\n",
      "Iteration 1305, loss = 0.03386408\n",
      "Iteration 1306, loss = 0.03374652\n",
      "Iteration 1307, loss = 0.03369289\n",
      "Iteration 1308, loss = 0.03451482\n",
      "Iteration 1309, loss = 0.03400016\n",
      "Iteration 1310, loss = 0.03340019\n",
      "Iteration 1311, loss = 0.03345271\n",
      "Iteration 1312, loss = 0.03350834\n",
      "Iteration 1313, loss = 0.03329215\n",
      "Iteration 1314, loss = 0.03323365\n",
      "Iteration 1315, loss = 0.03312864\n",
      "Iteration 1316, loss = 0.03294769\n",
      "Iteration 1317, loss = 0.03286018\n",
      "Iteration 1318, loss = 0.03266566\n",
      "Iteration 1319, loss = 0.03377813\n",
      "Iteration 1320, loss = 0.03238335\n",
      "Iteration 1321, loss = 0.03254827\n",
      "Iteration 1322, loss = 0.03215464\n",
      "Iteration 1323, loss = 0.03263528\n",
      "Iteration 1324, loss = 0.03213171\n",
      "Iteration 1325, loss = 0.03196962\n",
      "Iteration 1326, loss = 0.03188309\n",
      "Iteration 1327, loss = 0.03195306\n",
      "Iteration 1328, loss = 0.03185719\n",
      "Iteration 1329, loss = 0.03204710\n",
      "Iteration 1330, loss = 0.03147927\n",
      "Iteration 1331, loss = 0.03138778\n",
      "Iteration 1332, loss = 0.03153107\n",
      "Iteration 1333, loss = 0.03131909\n",
      "Iteration 1334, loss = 0.03210916\n",
      "Iteration 1335, loss = 0.03091399\n",
      "Iteration 1336, loss = 0.03081374\n",
      "Iteration 1337, loss = 0.03122570\n",
      "Iteration 1338, loss = 0.03053394\n",
      "Iteration 1339, loss = 0.03084944\n",
      "Iteration 1340, loss = 0.03076326\n",
      "Iteration 1341, loss = 0.03141267\n",
      "Iteration 1342, loss = 0.03023102\n",
      "Iteration 1343, loss = 0.03062198\n",
      "Iteration 1344, loss = 0.02985328\n",
      "Iteration 1345, loss = 0.03103714\n",
      "Iteration 1346, loss = 0.02993462\n",
      "Iteration 1347, loss = 0.03059350\n",
      "Iteration 1348, loss = 0.03003364\n",
      "Iteration 1349, loss = 0.02991846\n",
      "Iteration 1350, loss = 0.03039647\n",
      "Iteration 1351, loss = 0.03051683\n",
      "Iteration 1352, loss = 0.02899588\n",
      "Iteration 1353, loss = 0.03024852\n",
      "Iteration 1354, loss = 0.02879916\n",
      "Iteration 1355, loss = 0.02868499\n",
      "Iteration 1356, loss = 0.02850694\n",
      "Iteration 1357, loss = 0.02899300\n",
      "Iteration 1358, loss = 0.02886806\n",
      "Iteration 1359, loss = 0.02911932\n",
      "Iteration 1360, loss = 0.02805552\n",
      "Iteration 1361, loss = 0.02815763\n",
      "Iteration 1362, loss = 0.02807810\n",
      "Iteration 1363, loss = 0.02798420\n",
      "Iteration 1364, loss = 0.02785335\n",
      "Iteration 1365, loss = 0.02767551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1366, loss = 0.02794213\n",
      "Iteration 1367, loss = 0.02763106\n",
      "Iteration 1368, loss = 0.02736327\n",
      "Iteration 1369, loss = 0.02742413\n",
      "Iteration 1370, loss = 0.02711824\n",
      "Iteration 1371, loss = 0.02724026\n",
      "Iteration 1372, loss = 0.02744581\n",
      "Iteration 1373, loss = 0.02739014\n",
      "Iteration 1374, loss = 0.02705513\n",
      "Iteration 1375, loss = 0.02724354\n",
      "Iteration 1376, loss = 0.02680920\n",
      "Iteration 1377, loss = 0.02707324\n",
      "Iteration 1378, loss = 0.02711836\n",
      "Iteration 1379, loss = 0.02679971\n",
      "Iteration 1380, loss = 0.02635992\n",
      "Iteration 1381, loss = 0.02664996\n",
      "Iteration 1382, loss = 0.02647284\n",
      "Iteration 1383, loss = 0.02605713\n",
      "Iteration 1384, loss = 0.02668295\n",
      "Iteration 1385, loss = 0.02614684\n",
      "Iteration 1386, loss = 0.02631573\n",
      "Iteration 1387, loss = 0.02587091\n",
      "Iteration 1388, loss = 0.02688116\n",
      "Iteration 1389, loss = 0.02596038\n",
      "Iteration 1390, loss = 0.02613337\n",
      "Iteration 1391, loss = 0.02595578\n",
      "Iteration 1392, loss = 0.02548130\n",
      "Iteration 1393, loss = 0.02576704\n",
      "Iteration 1394, loss = 0.02541291\n",
      "Iteration 1395, loss = 0.02512527\n",
      "Iteration 1396, loss = 0.02523803\n",
      "Iteration 1397, loss = 0.02587012\n",
      "Iteration 1398, loss = 0.02562554\n",
      "Iteration 1399, loss = 0.02512464\n",
      "Iteration 1400, loss = 0.02507495\n",
      "Iteration 1401, loss = 0.02538749\n",
      "Iteration 1402, loss = 0.02459232\n",
      "Iteration 1403, loss = 0.02454806\n",
      "Iteration 1404, loss = 0.02478929\n",
      "Iteration 1405, loss = 0.02443253\n",
      "Iteration 1406, loss = 0.02441382\n",
      "Iteration 1407, loss = 0.02458318\n",
      "Iteration 1408, loss = 0.02442608\n",
      "Iteration 1409, loss = 0.02422009\n",
      "Iteration 1410, loss = 0.02418348\n",
      "Iteration 1411, loss = 0.02398946\n",
      "Iteration 1412, loss = 0.02411159\n",
      "Iteration 1413, loss = 0.02384532\n",
      "Iteration 1414, loss = 0.02387083\n",
      "Iteration 1415, loss = 0.02358530\n",
      "Iteration 1416, loss = 0.02362530\n",
      "Iteration 1417, loss = 0.02393910\n",
      "Iteration 1418, loss = 0.02379449\n",
      "Iteration 1419, loss = 0.02380144\n",
      "Iteration 1420, loss = 0.02344352\n",
      "Iteration 1421, loss = 0.02326078\n",
      "Iteration 1422, loss = 0.02321135\n",
      "Iteration 1423, loss = 0.02302500\n",
      "Iteration 1424, loss = 0.02311791\n",
      "Iteration 1425, loss = 0.02323068\n",
      "Iteration 1426, loss = 0.02288623\n",
      "Iteration 1427, loss = 0.02325008\n",
      "Iteration 1428, loss = 0.02253331\n",
      "Iteration 1429, loss = 0.02283216\n",
      "Iteration 1430, loss = 0.02262815\n",
      "Iteration 1431, loss = 0.02243820\n",
      "Iteration 1432, loss = 0.02280413\n",
      "Iteration 1433, loss = 0.02256860\n",
      "Iteration 1434, loss = 0.02242382\n",
      "Iteration 1435, loss = 0.02236937\n",
      "Iteration 1436, loss = 0.02231483\n",
      "Iteration 1437, loss = 0.02237226\n",
      "Iteration 1438, loss = 0.02369846\n",
      "Iteration 1439, loss = 0.02232460\n",
      "Iteration 1440, loss = 0.02245408\n",
      "Iteration 1441, loss = 0.02173617\n",
      "Iteration 1442, loss = 0.02219938\n",
      "Iteration 1443, loss = 0.02171897\n",
      "Iteration 1444, loss = 0.02147057\n",
      "Iteration 1445, loss = 0.02173781\n",
      "Iteration 1446, loss = 0.02161469\n",
      "Iteration 1447, loss = 0.02166305\n",
      "Iteration 1448, loss = 0.02135185\n",
      "Iteration 1449, loss = 0.02125779\n",
      "Iteration 1450, loss = 0.02115024\n",
      "Iteration 1451, loss = 0.02148066\n",
      "Iteration 1452, loss = 0.02119161\n",
      "Iteration 1453, loss = 0.02125443\n",
      "Iteration 1454, loss = 0.02090939\n",
      "Iteration 1455, loss = 0.02096262\n",
      "Iteration 1456, loss = 0.02118920\n",
      "Iteration 1457, loss = 0.02069700\n",
      "Iteration 1458, loss = 0.02076604\n",
      "Iteration 1459, loss = 0.02069179\n",
      "Iteration 1460, loss = 0.02062375\n",
      "Iteration 1461, loss = 0.02054916\n",
      "Iteration 1462, loss = 0.02045142\n",
      "Iteration 1463, loss = 0.02042211\n",
      "Iteration 1464, loss = 0.02031087\n",
      "Iteration 1465, loss = 0.02032527\n",
      "Iteration 1466, loss = 0.02040886\n",
      "Iteration 1467, loss = 0.02050263\n",
      "Iteration 1468, loss = 0.02051207\n",
      "Iteration 1469, loss = 0.02005024\n",
      "Iteration 1470, loss = 0.02057317\n",
      "Iteration 1471, loss = 0.02057447\n",
      "Iteration 1472, loss = 0.02010486\n",
      "Iteration 1473, loss = 0.02005114\n",
      "Iteration 1474, loss = 0.01999851\n",
      "Iteration 1475, loss = 0.01977158\n",
      "Iteration 1476, loss = 0.01964055\n",
      "Iteration 1477, loss = 0.02015868\n",
      "Iteration 1478, loss = 0.01977810\n",
      "Iteration 1479, loss = 0.01948731\n",
      "Iteration 1480, loss = 0.01939986\n",
      "Iteration 1481, loss = 0.01927104\n",
      "Iteration 1482, loss = 0.01938965\n",
      "Iteration 1483, loss = 0.01914965\n",
      "Iteration 1484, loss = 0.01946193\n",
      "Iteration 1485, loss = 0.01962476\n",
      "Iteration 1486, loss = 0.01930826\n",
      "Iteration 1487, loss = 0.01920443\n",
      "Iteration 1488, loss = 0.01899706\n",
      "Iteration 1489, loss = 0.01896929\n",
      "Iteration 1490, loss = 0.01888099\n",
      "Iteration 1491, loss = 0.01908685\n",
      "Iteration 1492, loss = 0.01870340\n",
      "Iteration 1493, loss = 0.01870744\n",
      "Iteration 1494, loss = 0.01870572\n",
      "Iteration 1495, loss = 0.01852416\n",
      "Iteration 1496, loss = 0.01847224\n",
      "Iteration 1497, loss = 0.01851564\n",
      "Iteration 1498, loss = 0.01839284\n",
      "Iteration 1499, loss = 0.01840007\n",
      "Iteration 1500, loss = 0.01856947\n",
      "Iteration 1501, loss = 0.01847679\n",
      "Iteration 1502, loss = 0.01842083\n",
      "Iteration 1503, loss = 0.01818219\n",
      "Iteration 1504, loss = 0.01835511\n",
      "Iteration 1505, loss = 0.01807750\n",
      "Iteration 1506, loss = 0.01818515\n",
      "Iteration 1507, loss = 0.01811072\n",
      "Iteration 1508, loss = 0.01833780\n",
      "Iteration 1509, loss = 0.01783367\n",
      "Iteration 1510, loss = 0.01788693\n",
      "Iteration 1511, loss = 0.01767904\n",
      "Iteration 1512, loss = 0.01803143\n",
      "Iteration 1513, loss = 0.01764797\n",
      "Iteration 1514, loss = 0.01789651\n",
      "Iteration 1515, loss = 0.01768515\n",
      "Iteration 1516, loss = 0.01751004\n",
      "Iteration 1517, loss = 0.01752387\n",
      "Iteration 1518, loss = 0.01747536\n",
      "Iteration 1519, loss = 0.01747932\n",
      "Iteration 1520, loss = 0.01763878\n",
      "Iteration 1521, loss = 0.01755804\n",
      "Iteration 1522, loss = 0.01724883\n",
      "Iteration 1523, loss = 0.01730604\n",
      "Iteration 1524, loss = 0.01726703\n",
      "Iteration 1525, loss = 0.01701662\n",
      "Iteration 1526, loss = 0.01717583\n",
      "Iteration 1527, loss = 0.01720652\n",
      "Iteration 1528, loss = 0.01698902\n",
      "Iteration 1529, loss = 0.01694806\n",
      "Iteration 1530, loss = 0.01684658\n",
      "Iteration 1531, loss = 0.01697455\n",
      "Iteration 1532, loss = 0.01675803\n",
      "Iteration 1533, loss = 0.01668475\n",
      "Iteration 1534, loss = 0.01661566\n",
      "Iteration 1535, loss = 0.01664973\n",
      "Iteration 1536, loss = 0.01673382\n",
      "Iteration 1537, loss = 0.01688508\n",
      "Iteration 1538, loss = 0.01666009\n",
      "Iteration 1539, loss = 0.01655317\n",
      "Iteration 1540, loss = 0.01647259\n",
      "Iteration 1541, loss = 0.01639569\n",
      "Iteration 1542, loss = 0.01629330\n",
      "Iteration 1543, loss = 0.01649860\n",
      "Iteration 1544, loss = 0.01643778\n",
      "Iteration 1545, loss = 0.01618654\n",
      "Iteration 1546, loss = 0.01626811\n",
      "Iteration 1547, loss = 0.01619014\n",
      "Iteration 1548, loss = 0.01596807\n",
      "Iteration 1549, loss = 0.01610008\n",
      "Iteration 1550, loss = 0.01604032\n",
      "Iteration 1551, loss = 0.01624616\n",
      "Iteration 1552, loss = 0.01595891\n",
      "Iteration 1553, loss = 0.01587932\n",
      "Iteration 1554, loss = 0.01602801\n",
      "Iteration 1555, loss = 0.01585980\n",
      "Iteration 1556, loss = 0.01575552\n",
      "Iteration 1557, loss = 0.01575117\n",
      "Iteration 1558, loss = 0.01559771\n",
      "Iteration 1559, loss = 0.01579535\n",
      "Iteration 1560, loss = 0.01550988\n",
      "Iteration 1561, loss = 0.01549162\n",
      "Iteration 1562, loss = 0.01570562\n",
      "Iteration 1563, loss = 0.01535833\n",
      "Iteration 1564, loss = 0.01547711\n",
      "Iteration 1565, loss = 0.01543793\n",
      "Iteration 1566, loss = 0.01543217\n",
      "Iteration 1567, loss = 0.01557099\n",
      "Iteration 1568, loss = 0.01518440\n",
      "Iteration 1569, loss = 0.01559568\n",
      "Iteration 1570, loss = 0.01519459\n",
      "Iteration 1571, loss = 0.01521201\n",
      "Iteration 1572, loss = 0.01515969\n",
      "Iteration 1573, loss = 0.01516297\n",
      "Iteration 1574, loss = 0.01535832\n",
      "Iteration 1575, loss = 0.01514023\n",
      "Iteration 1576, loss = 0.01494799\n",
      "Iteration 1577, loss = 0.01482936\n",
      "Iteration 1578, loss = 0.01492514\n",
      "Iteration 1579, loss = 0.01489601\n",
      "Iteration 1580, loss = 0.01473823\n",
      "Iteration 1581, loss = 0.01466367\n",
      "Iteration 1582, loss = 0.01472982\n",
      "Iteration 1583, loss = 0.01461687\n",
      "Iteration 1584, loss = 0.01462045\n",
      "Iteration 1585, loss = 0.01454115\n",
      "Iteration 1586, loss = 0.01461600\n",
      "Iteration 1587, loss = 0.01450134\n",
      "Iteration 1588, loss = 0.01449796\n",
      "Iteration 1589, loss = 0.01445240\n",
      "Iteration 1590, loss = 0.01439141\n",
      "Iteration 1591, loss = 0.01437161\n",
      "Iteration 1592, loss = 0.01435702\n",
      "Iteration 1593, loss = 0.01425025\n",
      "Iteration 1594, loss = 0.01427476\n",
      "Iteration 1595, loss = 0.01420947\n",
      "Iteration 1596, loss = 0.01413317\n",
      "Iteration 1597, loss = 0.01417494\n",
      "Iteration 1598, loss = 0.01403109\n",
      "Iteration 1599, loss = 0.01403390\n",
      "Iteration 1600, loss = 0.01410656\n",
      "Iteration 1601, loss = 0.01397900\n",
      "Iteration 1602, loss = 0.01394413\n",
      "Iteration 1603, loss = 0.01409771\n",
      "Iteration 1604, loss = 0.01447165\n",
      "Iteration 1605, loss = 0.01382432\n",
      "Iteration 1606, loss = 0.01388341\n",
      "Iteration 1607, loss = 0.01379593\n",
      "Iteration 1608, loss = 0.01372332\n",
      "Iteration 1609, loss = 0.01370721\n",
      "Iteration 1610, loss = 0.01367575\n",
      "Iteration 1611, loss = 0.01398260\n",
      "Iteration 1612, loss = 0.01368613\n",
      "Iteration 1613, loss = 0.01356932\n",
      "Iteration 1614, loss = 0.01360072\n",
      "Iteration 1615, loss = 0.01347425\n",
      "Iteration 1616, loss = 0.01355233\n",
      "Iteration 1617, loss = 0.01355591\n",
      "Iteration 1618, loss = 0.01342503\n",
      "Iteration 1619, loss = 0.01334776\n",
      "Iteration 1620, loss = 0.01337514\n",
      "Iteration 1621, loss = 0.01335113\n",
      "Iteration 1622, loss = 0.01342182\n",
      "Iteration 1623, loss = 0.01325554\n",
      "Iteration 1624, loss = 0.01321627\n",
      "Iteration 1625, loss = 0.01349787\n",
      "Iteration 1626, loss = 0.01326608\n",
      "Iteration 1627, loss = 0.01328212\n",
      "Iteration 1628, loss = 0.01313932\n",
      "Iteration 1629, loss = 0.01317252\n",
      "Iteration 1630, loss = 0.01328483\n",
      "Iteration 1631, loss = 0.01328891\n",
      "Iteration 1632, loss = 0.01309386\n",
      "Iteration 1633, loss = 0.01298165\n",
      "Iteration 1634, loss = 0.01298401\n",
      "Iteration 1635, loss = 0.01292883\n",
      "Iteration 1636, loss = 0.01306087\n",
      "Iteration 1637, loss = 0.01306681\n",
      "Iteration 1638, loss = 0.01285139\n",
      "Iteration 1639, loss = 0.01279398\n",
      "Iteration 1640, loss = 0.01270760\n",
      "Iteration 1641, loss = 0.01274402\n",
      "Iteration 1642, loss = 0.01271280\n",
      "Iteration 1643, loss = 0.01265807\n",
      "Iteration 1644, loss = 0.01256770\n",
      "Iteration 1645, loss = 0.01254429\n",
      "Iteration 1646, loss = 0.01254015\n",
      "Iteration 1647, loss = 0.01258674\n",
      "Iteration 1648, loss = 0.01251782\n",
      "Iteration 1649, loss = 0.01244194\n",
      "Iteration 1650, loss = 0.01249391\n",
      "Iteration 1651, loss = 0.01262193\n",
      "Iteration 1652, loss = 0.01241970\n",
      "Iteration 1653, loss = 0.01237961\n",
      "Iteration 1654, loss = 0.01230066\n",
      "Iteration 1655, loss = 0.01239570\n",
      "Iteration 1656, loss = 0.01247955\n",
      "Iteration 1657, loss = 0.01234867\n",
      "Iteration 1658, loss = 0.01246659\n",
      "Iteration 1659, loss = 0.01214443\n",
      "Iteration 1660, loss = 0.01217712\n",
      "Iteration 1661, loss = 0.01211504\n",
      "Iteration 1662, loss = 0.01219961\n",
      "Iteration 1663, loss = 0.01218144\n",
      "Iteration 1664, loss = 0.01229626\n",
      "Iteration 1665, loss = 0.01200802\n",
      "Iteration 1666, loss = 0.01195765\n",
      "Iteration 1667, loss = 0.01206461\n",
      "Iteration 1668, loss = 0.01196540\n",
      "Iteration 1669, loss = 0.01197524\n",
      "Iteration 1670, loss = 0.01184283\n",
      "Iteration 1671, loss = 0.01193443\n",
      "Iteration 1672, loss = 0.01187159\n",
      "Iteration 1673, loss = 0.01200731\n",
      "Iteration 1674, loss = 0.01178962\n",
      "Iteration 1675, loss = 0.01195333\n",
      "Iteration 1676, loss = 0.01185014\n",
      "Iteration 1677, loss = 0.01166428\n",
      "Iteration 1678, loss = 0.01164911\n",
      "Iteration 1679, loss = 0.01163682\n",
      "Iteration 1680, loss = 0.01173378\n",
      "Iteration 1681, loss = 0.01152835\n",
      "Iteration 1682, loss = 0.01166498\n",
      "Iteration 1683, loss = 0.01183551\n",
      "Iteration 1684, loss = 0.01154910\n",
      "Iteration 1685, loss = 0.01148456\n",
      "Iteration 1686, loss = 0.01141313\n",
      "Iteration 1687, loss = 0.01144024\n",
      "Iteration 1688, loss = 0.01143020\n",
      "Iteration 1689, loss = 0.01145989\n",
      "Iteration 1690, loss = 0.01136336\n",
      "Iteration 1691, loss = 0.01140373\n",
      "Iteration 1692, loss = 0.01143187\n",
      "Iteration 1693, loss = 0.01138716\n",
      "Iteration 1694, loss = 0.01144308\n",
      "Iteration 1695, loss = 0.01135834\n",
      "Iteration 1696, loss = 0.01121077\n",
      "Iteration 1697, loss = 0.01119213\n",
      "Iteration 1698, loss = 0.01118237\n",
      "Iteration 1699, loss = 0.01139672\n",
      "Iteration 1700, loss = 0.01109518\n",
      "Iteration 1701, loss = 0.01125997\n",
      "Iteration 1702, loss = 0.01106514\n",
      "Iteration 1703, loss = 0.01110604\n",
      "Iteration 1704, loss = 0.01101744\n",
      "Iteration 1705, loss = 0.01101838\n",
      "Iteration 1706, loss = 0.01093196\n",
      "Iteration 1707, loss = 0.01101909\n",
      "Iteration 1708, loss = 0.01086233\n",
      "Iteration 1709, loss = 0.01091363\n",
      "Iteration 1710, loss = 0.01091117\n",
      "Iteration 1711, loss = 0.01083135\n",
      "Iteration 1712, loss = 0.01086291\n",
      "Iteration 1713, loss = 0.01077373\n",
      "Iteration 1714, loss = 0.01098371\n",
      "Iteration 1715, loss = 0.01079367\n",
      "Iteration 1716, loss = 0.01069594\n",
      "Iteration 1717, loss = 0.01069468\n",
      "Iteration 1718, loss = 0.01073981\n",
      "Iteration 1719, loss = 0.01059148\n",
      "Iteration 1720, loss = 0.01069469\n",
      "Iteration 1721, loss = 0.01060173\n",
      "Iteration 1722, loss = 0.01053942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1723, loss = 0.01054313\n",
      "Iteration 1724, loss = 0.01055216\n",
      "Iteration 1725, loss = 0.01045167\n",
      "Iteration 1726, loss = 0.01045811\n",
      "Iteration 1727, loss = 0.01039476\n",
      "Iteration 1728, loss = 0.01036542\n",
      "Iteration 1729, loss = 0.01042690\n",
      "Iteration 1730, loss = 0.01045227\n",
      "Iteration 1731, loss = 0.01047498\n",
      "Iteration 1732, loss = 0.01030800\n",
      "Iteration 1733, loss = 0.01060959\n",
      "Iteration 1734, loss = 0.01023913\n",
      "Iteration 1735, loss = 0.01030306\n",
      "Iteration 1736, loss = 0.01021452\n",
      "Iteration 1737, loss = 0.01032773\n",
      "Iteration 1738, loss = 0.01016554\n",
      "Iteration 1739, loss = 0.01015856\n",
      "Iteration 1740, loss = 0.01018010\n",
      "Iteration 1741, loss = 0.01017387\n",
      "Iteration 1742, loss = 0.01005086\n",
      "Iteration 1743, loss = 0.01013973\n",
      "Iteration 1744, loss = 0.01004948\n",
      "Iteration 1745, loss = 0.01000937\n",
      "Iteration 1746, loss = 0.00998250\n",
      "Iteration 1747, loss = 0.00993036\n",
      "Iteration 1748, loss = 0.01000036\n",
      "Iteration 1749, loss = 0.00993452\n",
      "Iteration 1750, loss = 0.00986651\n",
      "Iteration 1751, loss = 0.00994898\n",
      "Iteration 1752, loss = 0.00983334\n",
      "Iteration 1753, loss = 0.00983534\n",
      "Iteration 1754, loss = 0.00978661\n",
      "Iteration 1755, loss = 0.00978866\n",
      "Iteration 1756, loss = 0.00978197\n",
      "Iteration 1757, loss = 0.00981851\n",
      "Iteration 1758, loss = 0.00970928\n",
      "Iteration 1759, loss = 0.00972027\n",
      "Iteration 1760, loss = 0.00966962\n",
      "Iteration 1761, loss = 0.00979619\n",
      "Iteration 1762, loss = 0.00979092\n",
      "Iteration 1763, loss = 0.00970639\n",
      "Iteration 1764, loss = 0.00961060\n",
      "Iteration 1765, loss = 0.00954863\n",
      "Iteration 1766, loss = 0.00958521\n",
      "Iteration 1767, loss = 0.00970114\n",
      "Iteration 1768, loss = 0.00947161\n",
      "Iteration 1769, loss = 0.00954532\n",
      "Iteration 1770, loss = 0.00954275\n",
      "Iteration 1771, loss = 0.00943858\n",
      "Iteration 1772, loss = 0.00941697\n",
      "Iteration 1773, loss = 0.00952360\n",
      "Iteration 1774, loss = 0.00945542\n",
      "Iteration 1775, loss = 0.00938575\n",
      "Iteration 1776, loss = 0.00933394\n",
      "Iteration 1777, loss = 0.00943748\n",
      "Iteration 1778, loss = 0.00926944\n",
      "Iteration 1779, loss = 0.00929611\n",
      "Iteration 1780, loss = 0.00943756\n",
      "Iteration 1781, loss = 0.00925235\n",
      "Iteration 1782, loss = 0.00919090\n",
      "Iteration 1783, loss = 0.00921822\n",
      "Iteration 1784, loss = 0.00929821\n",
      "Iteration 1785, loss = 0.00916441\n",
      "Iteration 1786, loss = 0.00916133\n",
      "Iteration 1787, loss = 0.00915085\n",
      "Iteration 1788, loss = 0.00912133\n",
      "Iteration 1789, loss = 0.00906253\n",
      "Iteration 1790, loss = 0.00935151\n",
      "Iteration 1791, loss = 0.00914477\n",
      "Iteration 1792, loss = 0.00911005\n",
      "Iteration 1793, loss = 0.00905982\n",
      "Iteration 1794, loss = 0.00906771\n",
      "Iteration 1795, loss = 0.00901517\n",
      "Iteration 1796, loss = 0.00897250\n",
      "Iteration 1797, loss = 0.00895981\n",
      "Iteration 1798, loss = 0.00896200\n",
      "Iteration 1799, loss = 0.00891906\n",
      "Iteration 1800, loss = 0.00891704\n",
      "Iteration 1801, loss = 0.00891565\n",
      "Iteration 1802, loss = 0.00888893\n",
      "Iteration 1803, loss = 0.00883456\n",
      "Iteration 1804, loss = 0.00886002\n",
      "Iteration 1805, loss = 0.00890020\n",
      "Iteration 1806, loss = 0.00883961\n",
      "Iteration 1807, loss = 0.00880185\n",
      "Iteration 1808, loss = 0.00876276\n",
      "Iteration 1809, loss = 0.00873892\n",
      "Iteration 1810, loss = 0.00870927\n",
      "Iteration 1811, loss = 0.00869389\n",
      "Iteration 1812, loss = 0.00869180\n",
      "Iteration 1813, loss = 0.00869689\n",
      "Iteration 1814, loss = 0.00865424\n",
      "Iteration 1815, loss = 0.00861196\n",
      "Iteration 1816, loss = 0.00858673\n",
      "Iteration 1817, loss = 0.00862683\n",
      "Iteration 1818, loss = 0.00856265\n",
      "Iteration 1819, loss = 0.00861103\n",
      "Iteration 1820, loss = 0.00852362\n",
      "Iteration 1821, loss = 0.00855214\n",
      "Iteration 1822, loss = 0.00847666\n",
      "Iteration 1823, loss = 0.00849115\n",
      "Iteration 1824, loss = 0.00845509\n",
      "Iteration 1825, loss = 0.00853087\n",
      "Iteration 1826, loss = 0.00841747\n",
      "Iteration 1827, loss = 0.00839088\n",
      "Iteration 1828, loss = 0.00839444\n",
      "Iteration 1829, loss = 0.00839016\n",
      "Iteration 1830, loss = 0.00838224\n",
      "Iteration 1831, loss = 0.00839257\n",
      "Iteration 1832, loss = 0.00832443\n",
      "Iteration 1833, loss = 0.00832269\n",
      "Iteration 1834, loss = 0.00828226\n",
      "Iteration 1835, loss = 0.00830775\n",
      "Iteration 1836, loss = 0.00827154\n",
      "Iteration 1837, loss = 0.00829558\n",
      "Iteration 1838, loss = 0.00825645\n",
      "Iteration 1839, loss = 0.00822055\n",
      "Iteration 1840, loss = 0.00825133\n",
      "Iteration 1841, loss = 0.00823704\n",
      "Iteration 1842, loss = 0.00816573\n",
      "Iteration 1843, loss = 0.00812946\n",
      "Iteration 1844, loss = 0.00825321\n",
      "Iteration 1845, loss = 0.00820096\n",
      "Iteration 1846, loss = 0.00814263\n",
      "Iteration 1847, loss = 0.00808255\n",
      "Iteration 1848, loss = 0.00809868\n",
      "Iteration 1849, loss = 0.00805292\n",
      "Iteration 1850, loss = 0.00817218\n",
      "Iteration 1851, loss = 0.00804559\n",
      "Iteration 1852, loss = 0.00802427\n",
      "Iteration 1853, loss = 0.00798346\n",
      "Iteration 1854, loss = 0.00803154\n",
      "Iteration 1855, loss = 0.00805488\n",
      "Iteration 1856, loss = 0.00796785\n",
      "Iteration 1857, loss = 0.00792282\n",
      "Iteration 1858, loss = 0.00795378\n",
      "Iteration 1859, loss = 0.00792316\n",
      "Iteration 1860, loss = 0.00791163\n",
      "Iteration 1861, loss = 0.00790165\n",
      "Iteration 1862, loss = 0.00789679\n",
      "Iteration 1863, loss = 0.00789218\n",
      "Iteration 1864, loss = 0.00786070\n",
      "Iteration 1865, loss = 0.00785990\n",
      "Iteration 1866, loss = 0.00780873\n",
      "Iteration 1867, loss = 0.00776872\n",
      "Iteration 1868, loss = 0.00777391\n",
      "Iteration 1869, loss = 0.00774866\n",
      "Iteration 1870, loss = 0.00771974\n",
      "Iteration 1871, loss = 0.00775711\n",
      "Iteration 1872, loss = 0.00771367\n",
      "Iteration 1873, loss = 0.00771887\n",
      "Iteration 1874, loss = 0.00783826\n",
      "Iteration 1875, loss = 0.00767083\n",
      "Iteration 1876, loss = 0.00767807\n",
      "Iteration 1877, loss = 0.00764771\n",
      "Iteration 1878, loss = 0.00761754\n",
      "Iteration 1879, loss = 0.00760908\n",
      "Iteration 1880, loss = 0.00759155\n",
      "Iteration 1881, loss = 0.00766475\n",
      "Iteration 1882, loss = 0.00756316\n",
      "Iteration 1883, loss = 0.00758861\n",
      "Iteration 1884, loss = 0.00752842\n",
      "Iteration 1885, loss = 0.00753436\n",
      "Iteration 1886, loss = 0.00761278\n",
      "Iteration 1887, loss = 0.00752358\n",
      "Iteration 1888, loss = 0.00759119\n",
      "Iteration 1889, loss = 0.00759088\n",
      "Iteration 1890, loss = 0.00752802\n",
      "Iteration 1891, loss = 0.00746653\n",
      "Iteration 1892, loss = 0.00744301\n",
      "Iteration 1893, loss = 0.00740137\n",
      "Iteration 1894, loss = 0.00740311\n",
      "Iteration 1895, loss = 0.00741572\n",
      "Iteration 1896, loss = 0.00739317\n",
      "Iteration 1897, loss = 0.00739088\n",
      "Iteration 1898, loss = 0.00735533\n",
      "Iteration 1899, loss = 0.00735511\n",
      "Iteration 1900, loss = 0.00733995\n",
      "Iteration 1901, loss = 0.00732133\n",
      "Iteration 1902, loss = 0.00730164\n",
      "Iteration 1903, loss = 0.00727501\n",
      "Iteration 1904, loss = 0.00726457\n",
      "Iteration 1905, loss = 0.00729163\n",
      "Iteration 1906, loss = 0.00726485\n",
      "Iteration 1907, loss = 0.00721531\n",
      "Iteration 1908, loss = 0.00721003\n",
      "Iteration 1909, loss = 0.00720351\n",
      "Iteration 1910, loss = 0.00720504\n",
      "Iteration 1911, loss = 0.00719653\n",
      "Iteration 1912, loss = 0.00716084\n",
      "Iteration 1913, loss = 0.00731485\n",
      "Iteration 1914, loss = 0.00714334\n",
      "Iteration 1915, loss = 0.00723601\n",
      "Iteration 1916, loss = 0.00717574\n",
      "Iteration 1917, loss = 0.00721552\n",
      "Iteration 1918, loss = 0.00710488\n",
      "Iteration 1919, loss = 0.00708301\n",
      "Iteration 1920, loss = 0.00714342\n",
      "Iteration 1921, loss = 0.00717850\n",
      "Iteration 1922, loss = 0.00706589\n",
      "Iteration 1923, loss = 0.00702265\n",
      "Iteration 1924, loss = 0.00706287\n",
      "Iteration 1925, loss = 0.00699864\n",
      "Iteration 1926, loss = 0.00702382\n",
      "Iteration 1927, loss = 0.00698944\n",
      "Iteration 1928, loss = 0.00700074\n",
      "Iteration 1929, loss = 0.00700260\n",
      "Iteration 1930, loss = 0.00699918\n",
      "Iteration 1931, loss = 0.00694037\n",
      "Iteration 1932, loss = 0.00694617\n",
      "Iteration 1933, loss = 0.00692584\n",
      "Iteration 1934, loss = 0.00693428\n",
      "Iteration 1935, loss = 0.00687312\n",
      "Iteration 1936, loss = 0.00689914\n",
      "Iteration 1937, loss = 0.00684949\n",
      "Iteration 1938, loss = 0.00686105\n",
      "Iteration 1939, loss = 0.00687095\n",
      "Iteration 1940, loss = 0.00681959\n",
      "Iteration 1941, loss = 0.00686313\n",
      "Iteration 1942, loss = 0.00684939\n",
      "Iteration 1943, loss = 0.00678741\n",
      "Iteration 1944, loss = 0.00678119\n",
      "Iteration 1945, loss = 0.00680689\n",
      "Iteration 1946, loss = 0.00680029\n",
      "Iteration 1947, loss = 0.00674551\n",
      "Iteration 1948, loss = 0.00673297\n",
      "Iteration 1949, loss = 0.00678208\n",
      "Iteration 1950, loss = 0.00671848\n",
      "Iteration 1951, loss = 0.00678501\n",
      "Iteration 1952, loss = 0.00671497\n",
      "Iteration 1953, loss = 0.00673971\n",
      "Iteration 1954, loss = 0.00671583\n",
      "Iteration 1955, loss = 0.00665065\n",
      "Iteration 1956, loss = 0.00667908\n",
      "Iteration 1957, loss = 0.00665720\n",
      "Iteration 1958, loss = 0.00662810\n",
      "Iteration 1959, loss = 0.00664291\n",
      "Iteration 1960, loss = 0.00660856\n",
      "Iteration 1961, loss = 0.00662564\n",
      "Iteration 1962, loss = 0.00659958\n",
      "Iteration 1963, loss = 0.00664189\n",
      "Iteration 1964, loss = 0.00653594\n",
      "Iteration 1965, loss = 0.00654806\n",
      "Iteration 1966, loss = 0.00656467\n",
      "Iteration 1967, loss = 0.00651290\n",
      "Iteration 1968, loss = 0.00650021\n",
      "Iteration 1969, loss = 0.00652231\n",
      "Iteration 1970, loss = 0.00649274\n",
      "Iteration 1971, loss = 0.00651916\n",
      "Iteration 1972, loss = 0.00648993\n",
      "Iteration 1973, loss = 0.00645035\n",
      "Iteration 1974, loss = 0.00644371\n",
      "Iteration 1975, loss = 0.00649191\n",
      "Iteration 1976, loss = 0.00642431\n",
      "Iteration 1977, loss = 0.00642820\n",
      "Iteration 1978, loss = 0.00649628\n",
      "Iteration 1979, loss = 0.00648112\n",
      "Iteration 1980, loss = 0.00648253\n",
      "Iteration 1981, loss = 0.00642849\n",
      "Iteration 1982, loss = 0.00636914\n",
      "Iteration 1983, loss = 0.00635270\n",
      "Iteration 1984, loss = 0.00634091\n",
      "Iteration 1985, loss = 0.00632957\n",
      "Iteration 1986, loss = 0.00636230\n",
      "Iteration 1987, loss = 0.00638265\n",
      "Iteration 1988, loss = 0.00632950\n",
      "Iteration 1989, loss = 0.00631129\n",
      "Iteration 1990, loss = 0.00626909\n",
      "Iteration 1991, loss = 0.00630335\n",
      "Iteration 1992, loss = 0.00628588\n",
      "Iteration 1993, loss = 0.00627819\n",
      "Iteration 1994, loss = 0.00632475\n",
      "Iteration 1995, loss = 0.00624747\n",
      "Iteration 1996, loss = 0.00638113\n",
      "Iteration 1997, loss = 0.00626231\n",
      "Iteration 1998, loss = 0.00627809\n",
      "Iteration 1999, loss = 0.00624899\n",
      "Iteration 2000, loss = 0.00618754\n",
      "Iteration 2001, loss = 0.00618706\n",
      "Iteration 2002, loss = 0.00617923\n",
      "Iteration 2003, loss = 0.00619431\n",
      "Iteration 2004, loss = 0.00621375\n",
      "Iteration 2005, loss = 0.00615301\n",
      "Iteration 2006, loss = 0.00612456\n",
      "Iteration 2007, loss = 0.00612878\n",
      "Iteration 2008, loss = 0.00612565\n",
      "Iteration 2009, loss = 0.00610044\n",
      "Iteration 2010, loss = 0.00608468\n",
      "Iteration 2011, loss = 0.00608924\n",
      "Iteration 2012, loss = 0.00607519\n",
      "Iteration 2013, loss = 0.00607912\n",
      "Iteration 2014, loss = 0.00606878\n",
      "Iteration 2015, loss = 0.00603972\n",
      "Iteration 2016, loss = 0.00603016\n",
      "Iteration 2017, loss = 0.00603911\n",
      "Iteration 2018, loss = 0.00602058\n",
      "Iteration 2019, loss = 0.00604660\n",
      "Iteration 2020, loss = 0.00603086\n",
      "Iteration 2021, loss = 0.00602185\n",
      "Iteration 2022, loss = 0.00601877\n",
      "Iteration 2023, loss = 0.00596127\n",
      "Iteration 2024, loss = 0.00595785\n",
      "Iteration 2025, loss = 0.00597239\n",
      "Iteration 2026, loss = 0.00594994\n",
      "Iteration 2027, loss = 0.00591545\n",
      "Iteration 2028, loss = 0.00591416\n",
      "Iteration 2029, loss = 0.00592967\n",
      "Iteration 2030, loss = 0.00599400\n",
      "Iteration 2031, loss = 0.00590579\n",
      "Iteration 2032, loss = 0.00587688\n",
      "Iteration 2033, loss = 0.00589601\n",
      "Iteration 2034, loss = 0.00588129\n",
      "Iteration 2035, loss = 0.00585077\n",
      "Iteration 2036, loss = 0.00583995\n",
      "Iteration 2037, loss = 0.00585996\n",
      "Iteration 2038, loss = 0.00582976\n",
      "Iteration 2039, loss = 0.00581327\n",
      "Iteration 2040, loss = 0.00579617\n",
      "Iteration 2041, loss = 0.00579641\n",
      "Iteration 2042, loss = 0.00581423\n",
      "Iteration 2043, loss = 0.00577966\n",
      "Iteration 2044, loss = 0.00579838\n",
      "Iteration 2045, loss = 0.00579063\n",
      "Iteration 2046, loss = 0.00574977\n",
      "Iteration 2047, loss = 0.00576655\n",
      "Iteration 2048, loss = 0.00575068\n",
      "Iteration 2049, loss = 0.00580549\n",
      "Iteration 2050, loss = 0.00571388\n",
      "Iteration 2051, loss = 0.00579691\n",
      "Iteration 2052, loss = 0.00572050\n",
      "Iteration 2053, loss = 0.00569059\n",
      "Iteration 2054, loss = 0.00568550\n",
      "Iteration 2055, loss = 0.00568610\n",
      "Iteration 2056, loss = 0.00568069\n",
      "Iteration 2057, loss = 0.00567821\n",
      "Iteration 2058, loss = 0.00565152\n",
      "Iteration 2059, loss = 0.00563497\n",
      "Iteration 2060, loss = 0.00564406\n",
      "Iteration 2061, loss = 0.00561051\n",
      "Iteration 2062, loss = 0.00564194\n",
      "Iteration 2063, loss = 0.00560343\n",
      "Iteration 2064, loss = 0.00567711\n",
      "Iteration 2065, loss = 0.00561694\n",
      "Iteration 2066, loss = 0.00560345\n",
      "Iteration 2067, loss = 0.00555363\n",
      "Iteration 2068, loss = 0.00557179\n",
      "Iteration 2069, loss = 0.00554730\n",
      "Iteration 2070, loss = 0.00557338\n",
      "Iteration 2071, loss = 0.00553226\n",
      "Iteration 2072, loss = 0.00553663\n",
      "Iteration 2073, loss = 0.00552107\n",
      "Iteration 2074, loss = 0.00551254\n",
      "Iteration 2075, loss = 0.00552424\n",
      "Iteration 2076, loss = 0.00552078\n",
      "Iteration 2077, loss = 0.00547863\n",
      "Iteration 2078, loss = 0.00549950\n",
      "Iteration 2079, loss = 0.00548170\n",
      "Iteration 2080, loss = 0.00548063\n",
      "Iteration 2081, loss = 0.00546988\n",
      "Iteration 2082, loss = 0.00548040\n",
      "Iteration 2083, loss = 0.00543379\n",
      "Iteration 2084, loss = 0.00542382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2085, loss = 0.00547465\n",
      "Iteration 2086, loss = 0.00542325\n",
      "Iteration 2087, loss = 0.00543769\n",
      "Iteration 2088, loss = 0.00540469\n",
      "Iteration 2089, loss = 0.00539863\n",
      "Iteration 2090, loss = 0.00537969\n",
      "Iteration 2091, loss = 0.00540859\n",
      "Iteration 2092, loss = 0.00538867\n",
      "Iteration 2093, loss = 0.00542863\n",
      "Iteration 2094, loss = 0.00534754\n",
      "Iteration 2095, loss = 0.00536882\n",
      "Iteration 2096, loss = 0.00536719\n",
      "Iteration 2097, loss = 0.00535561\n",
      "Iteration 2098, loss = 0.00532553\n",
      "Iteration 2099, loss = 0.00531813\n",
      "Iteration 2100, loss = 0.00530664\n",
      "Iteration 2101, loss = 0.00530827\n",
      "Iteration 2102, loss = 0.00533914\n",
      "Iteration 2103, loss = 0.00529998\n",
      "Iteration 2104, loss = 0.00530460\n",
      "Iteration 2105, loss = 0.00528435\n",
      "Iteration 2106, loss = 0.00525217\n",
      "Iteration 2107, loss = 0.00526567\n",
      "Iteration 2108, loss = 0.00526198\n",
      "Iteration 2109, loss = 0.00524161\n",
      "Iteration 2110, loss = 0.00523930\n",
      "Iteration 2111, loss = 0.00524270\n",
      "Iteration 2112, loss = 0.00522615\n",
      "Iteration 2113, loss = 0.00521189\n",
      "Iteration 2114, loss = 0.00525116\n",
      "Iteration 2115, loss = 0.00518771\n",
      "Iteration 2116, loss = 0.00520486\n",
      "Iteration 2117, loss = 0.00518697\n",
      "Iteration 2118, loss = 0.00516559\n",
      "Iteration 2119, loss = 0.00520534\n",
      "Iteration 2120, loss = 0.00515708\n",
      "Iteration 2121, loss = 0.00514207\n",
      "Iteration 2122, loss = 0.00513768\n",
      "Iteration 2123, loss = 0.00513606\n",
      "Iteration 2124, loss = 0.00515019\n",
      "Iteration 2125, loss = 0.00510223\n",
      "Iteration 2126, loss = 0.00512067\n",
      "Iteration 2127, loss = 0.00511628\n",
      "Iteration 2128, loss = 0.00510757\n",
      "Iteration 2129, loss = 0.00513302\n",
      "Iteration 2130, loss = 0.00506028\n",
      "Iteration 2131, loss = 0.00506485\n",
      "Iteration 2132, loss = 0.00506657\n",
      "Iteration 2133, loss = 0.00506325\n",
      "Iteration 2134, loss = 0.00506569\n",
      "Iteration 2135, loss = 0.00503112\n",
      "Iteration 2136, loss = 0.00504195\n",
      "Iteration 2137, loss = 0.00501728\n",
      "Iteration 2138, loss = 0.00506320\n",
      "Iteration 2139, loss = 0.00502395\n",
      "Iteration 2140, loss = 0.00499498\n",
      "Iteration 2141, loss = 0.00500980\n",
      "Iteration 2142, loss = 0.00498478\n",
      "Iteration 2143, loss = 0.00498219\n",
      "Iteration 2144, loss = 0.00495922\n",
      "Iteration 2145, loss = 0.00495723\n",
      "Iteration 2146, loss = 0.00495808\n",
      "Iteration 2147, loss = 0.00497682\n",
      "Iteration 2148, loss = 0.00493007\n",
      "Iteration 2149, loss = 0.00491733\n",
      "Iteration 2150, loss = 0.00492313\n",
      "Iteration 2151, loss = 0.00492349\n",
      "Iteration 2152, loss = 0.00492993\n",
      "Iteration 2153, loss = 0.00490687\n",
      "Iteration 2154, loss = 0.00490014\n",
      "Iteration 2155, loss = 0.00490908\n",
      "Iteration 2156, loss = 0.00490693\n",
      "Iteration 2157, loss = 0.00487600\n",
      "Iteration 2158, loss = 0.00486219\n",
      "Iteration 2159, loss = 0.00486143\n",
      "Iteration 2160, loss = 0.00483772\n",
      "Iteration 2161, loss = 0.00484871\n",
      "Iteration 2162, loss = 0.00484448\n",
      "Iteration 2163, loss = 0.00483466\n",
      "Iteration 2164, loss = 0.00483177\n",
      "Iteration 2165, loss = 0.00488418\n",
      "Iteration 2166, loss = 0.00481362\n",
      "Iteration 2167, loss = 0.00481299\n",
      "Iteration 2168, loss = 0.00479613\n",
      "Iteration 2169, loss = 0.00478300\n",
      "Iteration 2170, loss = 0.00477767\n",
      "Iteration 2171, loss = 0.00477972\n",
      "Iteration 2172, loss = 0.00477043\n",
      "Iteration 2173, loss = 0.00477494\n",
      "Iteration 2174, loss = 0.00475871\n",
      "Iteration 2175, loss = 0.00475250\n",
      "Iteration 2176, loss = 0.00474337\n",
      "Iteration 2177, loss = 0.00474499\n",
      "Iteration 2178, loss = 0.00476527\n",
      "Iteration 2179, loss = 0.00478582\n",
      "Iteration 2180, loss = 0.00474422\n",
      "Iteration 2181, loss = 0.00470976\n",
      "Iteration 2182, loss = 0.00471860\n",
      "Iteration 2183, loss = 0.00470678\n",
      "Iteration 2184, loss = 0.00471824\n",
      "Iteration 2185, loss = 0.00467880\n",
      "Iteration 2186, loss = 0.00470023\n",
      "Iteration 2187, loss = 0.00468928\n",
      "Iteration 2188, loss = 0.00468204\n",
      "Iteration 2189, loss = 0.00472508\n",
      "Iteration 2190, loss = 0.00464264\n",
      "Iteration 2191, loss = 0.00467695\n",
      "Iteration 2192, loss = 0.00464768\n",
      "Iteration 2193, loss = 0.00464108\n",
      "Iteration 2194, loss = 0.00467152\n",
      "Iteration 2195, loss = 0.00460833\n",
      "Iteration 2196, loss = 0.00462632\n",
      "Iteration 2197, loss = 0.00459686\n",
      "Iteration 2198, loss = 0.00459397\n",
      "Iteration 2199, loss = 0.00459522\n",
      "Iteration 2200, loss = 0.00457458\n",
      "Iteration 2201, loss = 0.00460766\n",
      "Iteration 2202, loss = 0.00456420\n",
      "Iteration 2203, loss = 0.00456189\n",
      "Iteration 2204, loss = 0.00458789\n",
      "Iteration 2205, loss = 0.00459045\n",
      "Iteration 2206, loss = 0.00453837\n",
      "Iteration 2207, loss = 0.00454933\n",
      "Iteration 2208, loss = 0.00455543\n",
      "Iteration 2209, loss = 0.00454996\n",
      "Iteration 2210, loss = 0.00454683\n",
      "Iteration 2211, loss = 0.00457601\n",
      "Iteration 2212, loss = 0.00450192\n",
      "Iteration 2213, loss = 0.00450280\n",
      "Iteration 2214, loss = 0.00450289\n",
      "Iteration 2215, loss = 0.00452285\n",
      "Iteration 2216, loss = 0.00448609\n",
      "Iteration 2217, loss = 0.00449730\n",
      "Iteration 2218, loss = 0.00447452\n",
      "Iteration 2219, loss = 0.00446224\n",
      "Iteration 2220, loss = 0.00445757\n",
      "Iteration 2221, loss = 0.00446272\n",
      "Iteration 2222, loss = 0.00445326\n",
      "Iteration 2223, loss = 0.00445024\n",
      "Iteration 2224, loss = 0.00443296\n",
      "Iteration 2225, loss = 0.00443552\n",
      "Iteration 2226, loss = 0.00443083\n",
      "Iteration 2227, loss = 0.00442874\n",
      "Iteration 2228, loss = 0.00441832\n",
      "Iteration 2229, loss = 0.00443579\n",
      "Iteration 2230, loss = 0.00441059\n",
      "Iteration 2231, loss = 0.00441202\n",
      "Iteration 2232, loss = 0.00440618\n",
      "Iteration 2233, loss = 0.00440396\n",
      "Iteration 2234, loss = 0.00440919\n",
      "Iteration 2235, loss = 0.00437945\n",
      "Iteration 2236, loss = 0.00438225\n",
      "Iteration 2237, loss = 0.00439074\n",
      "Iteration 2238, loss = 0.00439741\n",
      "Iteration 2239, loss = 0.00438195\n",
      "Iteration 2240, loss = 0.00435092\n",
      "Iteration 2241, loss = 0.00433474\n",
      "Iteration 2242, loss = 0.00433739\n",
      "Iteration 2243, loss = 0.00433605\n",
      "Iteration 2244, loss = 0.00433884\n",
      "Iteration 2245, loss = 0.00432124\n",
      "Iteration 2246, loss = 0.00431617\n",
      "Iteration 2247, loss = 0.00433572\n",
      "Iteration 2248, loss = 0.00434205\n",
      "Iteration 2249, loss = 0.00430089\n",
      "Iteration 2250, loss = 0.00428738\n",
      "Iteration 2251, loss = 0.00430838\n",
      "Iteration 2252, loss = 0.00428734\n",
      "Iteration 2253, loss = 0.00428794\n",
      "Iteration 2254, loss = 0.00430571\n",
      "Iteration 2255, loss = 0.00432034\n",
      "Iteration 2256, loss = 0.00427738\n",
      "Iteration 2257, loss = 0.00425239\n",
      "Iteration 2258, loss = 0.00428951\n",
      "Iteration 2259, loss = 0.00424230\n",
      "Iteration 2260, loss = 0.00424507\n",
      "Iteration 2261, loss = 0.00424001\n",
      "Iteration 2262, loss = 0.00423566\n",
      "Iteration 2263, loss = 0.00423967\n",
      "Iteration 2264, loss = 0.00421387\n",
      "Iteration 2265, loss = 0.00421250\n",
      "Iteration 2266, loss = 0.00422177\n",
      "Iteration 2267, loss = 0.00424143\n",
      "Iteration 2268, loss = 0.00420911\n",
      "Iteration 2269, loss = 0.00420053\n",
      "Iteration 2270, loss = 0.00418877\n",
      "Iteration 2271, loss = 0.00417851\n",
      "Iteration 2272, loss = 0.00418264\n",
      "Iteration 2273, loss = 0.00417645\n",
      "Iteration 2274, loss = 0.00417827\n",
      "Iteration 2275, loss = 0.00416557\n",
      "Iteration 2276, loss = 0.00417698\n",
      "Iteration 2277, loss = 0.00416310\n",
      "Iteration 2278, loss = 0.00416630\n",
      "Iteration 2279, loss = 0.00414393\n",
      "Iteration 2280, loss = 0.00415998\n",
      "Iteration 2281, loss = 0.00416145\n",
      "Iteration 2282, loss = 0.00416963\n",
      "Iteration 2283, loss = 0.00412041\n",
      "Iteration 2284, loss = 0.00412791\n",
      "Iteration 2285, loss = 0.00412046\n",
      "Iteration 2286, loss = 0.00410259\n",
      "Iteration 2287, loss = 0.00409923\n",
      "Iteration 2288, loss = 0.00412225\n",
      "Iteration 2289, loss = 0.00408867\n",
      "Iteration 2290, loss = 0.00409770\n",
      "Iteration 2291, loss = 0.00409632\n",
      "Iteration 2292, loss = 0.00407293\n",
      "Iteration 2293, loss = 0.00407080\n",
      "Iteration 2294, loss = 0.00407748\n",
      "Iteration 2295, loss = 0.00405618\n",
      "Iteration 2296, loss = 0.00405669\n",
      "Iteration 2297, loss = 0.00405706\n",
      "Iteration 2298, loss = 0.00404376\n",
      "Iteration 2299, loss = 0.00406832\n",
      "Iteration 2300, loss = 0.00404146\n",
      "Iteration 2301, loss = 0.00403411\n",
      "Iteration 2302, loss = 0.00403026\n",
      "Iteration 2303, loss = 0.00402759\n",
      "Iteration 2304, loss = 0.00403552\n",
      "Iteration 2305, loss = 0.00403969\n",
      "Iteration 2306, loss = 0.00400713\n",
      "Iteration 2307, loss = 0.00400290\n",
      "Iteration 2308, loss = 0.00400262\n",
      "Iteration 2309, loss = 0.00399828\n",
      "Iteration 2310, loss = 0.00398357\n",
      "Iteration 2311, loss = 0.00398378\n",
      "Iteration 2312, loss = 0.00397728\n",
      "Iteration 2313, loss = 0.00398462\n",
      "Iteration 2314, loss = 0.00400520\n",
      "Iteration 2315, loss = 0.00397330\n",
      "Iteration 2316, loss = 0.00395816\n",
      "Iteration 2317, loss = 0.00396451\n",
      "Iteration 2318, loss = 0.00395998\n",
      "Iteration 2319, loss = 0.00396077\n",
      "Iteration 2320, loss = 0.00394734\n",
      "Iteration 2321, loss = 0.00394199\n",
      "Iteration 2322, loss = 0.00394713\n",
      "Iteration 2323, loss = 0.00394885\n",
      "Iteration 2324, loss = 0.00393840\n",
      "Iteration 2325, loss = 0.00392233\n",
      "Iteration 2326, loss = 0.00393039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2327, loss = 0.00393208\n",
      "Iteration 2328, loss = 0.00390288\n",
      "Iteration 2329, loss = 0.00392869\n",
      "Iteration 2330, loss = 0.00390161\n",
      "Iteration 2331, loss = 0.00388978\n",
      "Iteration 2332, loss = 0.00391206\n",
      "Iteration 2333, loss = 0.00388858\n",
      "Iteration 2334, loss = 0.00391949\n",
      "Iteration 2335, loss = 0.00388085\n",
      "Iteration 2336, loss = 0.00389584\n",
      "Iteration 2337, loss = 0.00387997\n",
      "Iteration 2338, loss = 0.00386475\n",
      "Iteration 2339, loss = 0.00389043\n",
      "Iteration 2340, loss = 0.00385129\n",
      "Iteration 2341, loss = 0.00391122\n",
      "Iteration 2342, loss = 0.00383342\n",
      "Iteration 2343, loss = 0.00386198\n",
      "Iteration 2344, loss = 0.00386081\n",
      "Iteration 2345, loss = 0.00384650\n",
      "Iteration 2346, loss = 0.00383012\n",
      "Iteration 2347, loss = 0.00382369\n",
      "Iteration 2348, loss = 0.00385725\n",
      "Iteration 2349, loss = 0.00381940\n",
      "Iteration 2350, loss = 0.00382760\n",
      "Iteration 2351, loss = 0.00382752\n",
      "Iteration 2352, loss = 0.00382372\n",
      "Iteration 2353, loss = 0.00379446\n",
      "Iteration 2354, loss = 0.00379988\n",
      "Iteration 2355, loss = 0.00378196\n",
      "Iteration 2356, loss = 0.00381525\n",
      "Iteration 2357, loss = 0.00379424\n",
      "Iteration 2358, loss = 0.00377285\n",
      "Iteration 2359, loss = 0.00377461\n",
      "Iteration 2360, loss = 0.00378070\n",
      "Iteration 2361, loss = 0.00375891\n",
      "Iteration 2362, loss = 0.00376680\n",
      "Iteration 2363, loss = 0.00376923\n",
      "Iteration 2364, loss = 0.00375925\n",
      "Iteration 2365, loss = 0.00377340\n",
      "Iteration 2366, loss = 0.00377590\n",
      "Iteration 2367, loss = 0.00374541\n",
      "Iteration 2368, loss = 0.00374266\n",
      "Iteration 2369, loss = 0.00373030\n",
      "Iteration 2370, loss = 0.00373707\n",
      "Iteration 2371, loss = 0.00372046\n",
      "Iteration 2372, loss = 0.00372317\n",
      "Iteration 2373, loss = 0.00371068\n",
      "Iteration 2374, loss = 0.00372355\n",
      "Iteration 2375, loss = 0.00370575\n",
      "Iteration 2376, loss = 0.00370870\n",
      "Iteration 2377, loss = 0.00370152\n",
      "Iteration 2378, loss = 0.00368863\n",
      "Iteration 2379, loss = 0.00369179\n",
      "Iteration 2380, loss = 0.00368213\n",
      "Iteration 2381, loss = 0.00372056\n",
      "Iteration 2382, loss = 0.00368926\n",
      "Iteration 2383, loss = 0.00368926\n",
      "Iteration 2384, loss = 0.00366800\n",
      "Iteration 2385, loss = 0.00367457\n",
      "Iteration 2386, loss = 0.00366460\n",
      "Iteration 2387, loss = 0.00367718\n",
      "Iteration 2388, loss = 0.00365371\n",
      "Iteration 2389, loss = 0.00365391\n",
      "Iteration 2390, loss = 0.00363860\n",
      "Iteration 2391, loss = 0.00365211\n",
      "Iteration 2392, loss = 0.00363632\n",
      "Iteration 2393, loss = 0.00365379\n",
      "Iteration 2394, loss = 0.00362424\n",
      "Iteration 2395, loss = 0.00363774\n",
      "Iteration 2396, loss = 0.00361881\n",
      "Iteration 2397, loss = 0.00362976\n",
      "Iteration 2398, loss = 0.00362110\n",
      "Iteration 2399, loss = 0.00361144\n",
      "Iteration 2400, loss = 0.00361052\n",
      "Iteration 2401, loss = 0.00361202\n",
      "Iteration 2402, loss = 0.00364779\n",
      "Iteration 2403, loss = 0.00360500\n",
      "Iteration 2404, loss = 0.00359279\n",
      "Iteration 2405, loss = 0.00359222\n",
      "Iteration 2406, loss = 0.00359315\n",
      "Iteration 2407, loss = 0.00358786\n",
      "Iteration 2408, loss = 0.00357539\n",
      "Iteration 2409, loss = 0.00359751\n",
      "Iteration 2410, loss = 0.00357450\n",
      "Iteration 2411, loss = 0.00357167\n",
      "Iteration 2412, loss = 0.00357328\n",
      "Iteration 2413, loss = 0.00357101\n",
      "Iteration 2414, loss = 0.00355667\n",
      "Iteration 2415, loss = 0.00355203\n",
      "Iteration 2416, loss = 0.00355117\n",
      "Iteration 2417, loss = 0.00353462\n",
      "Iteration 2418, loss = 0.00354146\n",
      "Iteration 2419, loss = 0.00354455\n",
      "Iteration 2420, loss = 0.00354626\n",
      "Iteration 2421, loss = 0.00355332\n",
      "Iteration 2422, loss = 0.00352752\n",
      "Iteration 2423, loss = 0.00353209\n",
      "Iteration 2424, loss = 0.00351355\n",
      "Iteration 2425, loss = 0.00350579\n",
      "Iteration 2426, loss = 0.00351554\n",
      "Iteration 2427, loss = 0.00351053\n",
      "Iteration 2428, loss = 0.00350454\n",
      "Iteration 2429, loss = 0.00351041\n",
      "Iteration 2430, loss = 0.00350728\n",
      "Iteration 2431, loss = 0.00348358\n",
      "Iteration 2432, loss = 0.00348977\n",
      "Iteration 2433, loss = 0.00350384\n",
      "Iteration 2434, loss = 0.00349718\n",
      "Iteration 2435, loss = 0.00347508\n",
      "Iteration 2436, loss = 0.00347252\n",
      "Iteration 2437, loss = 0.00347822\n",
      "Iteration 2438, loss = 0.00348129\n",
      "Iteration 2439, loss = 0.00346132\n",
      "Iteration 2440, loss = 0.00346383\n",
      "Iteration 2441, loss = 0.00345939\n",
      "Iteration 2442, loss = 0.00348172\n",
      "Iteration 2443, loss = 0.00344688\n",
      "Iteration 2444, loss = 0.00343849\n",
      "Iteration 2445, loss = 0.00343301\n",
      "Iteration 2446, loss = 0.00343194\n",
      "Iteration 2447, loss = 0.00343192\n",
      "Iteration 2448, loss = 0.00342160\n",
      "Iteration 2449, loss = 0.00342819\n",
      "Iteration 2450, loss = 0.00341793\n",
      "Iteration 2451, loss = 0.00341895\n",
      "Iteration 2452, loss = 0.00344110\n",
      "Iteration 2453, loss = 0.00342578\n",
      "Iteration 2454, loss = 0.00342094\n",
      "Iteration 2455, loss = 0.00340404\n",
      "Iteration 2456, loss = 0.00341037\n",
      "Iteration 2457, loss = 0.00339684\n",
      "Iteration 2458, loss = 0.00340542\n",
      "Iteration 2459, loss = 0.00339224\n",
      "Iteration 2460, loss = 0.00342740\n",
      "Iteration 2461, loss = 0.00339758\n",
      "Iteration 2462, loss = 0.00337553\n",
      "Iteration 2463, loss = 0.00336931\n",
      "Iteration 2464, loss = 0.00338145\n",
      "Iteration 2465, loss = 0.00336601\n",
      "Iteration 2466, loss = 0.00336669\n",
      "Iteration 2467, loss = 0.00335970\n",
      "Iteration 2468, loss = 0.00335365\n",
      "Iteration 2469, loss = 0.00335050\n",
      "Iteration 2470, loss = 0.00335440\n",
      "Iteration 2471, loss = 0.00335365\n",
      "Iteration 2472, loss = 0.00336001\n",
      "Iteration 2473, loss = 0.00333996\n",
      "Iteration 2474, loss = 0.00334049\n",
      "Iteration 2475, loss = 0.00333322\n",
      "Iteration 2476, loss = 0.00334646\n",
      "Iteration 2477, loss = 0.00332472\n",
      "Iteration 2478, loss = 0.00332961\n",
      "Iteration 2479, loss = 0.00331858\n",
      "Iteration 2480, loss = 0.00332705\n",
      "Iteration 2481, loss = 0.00331086\n",
      "Iteration 2482, loss = 0.00331169\n",
      "Iteration 2483, loss = 0.00330398\n",
      "Iteration 2484, loss = 0.00330771\n",
      "Iteration 2485, loss = 0.00330903\n",
      "Iteration 2486, loss = 0.00329706\n",
      "Iteration 2487, loss = 0.00329518\n",
      "Iteration 2488, loss = 0.00328354\n",
      "Iteration 2489, loss = 0.00328958\n",
      "Iteration 2490, loss = 0.00328974\n",
      "Iteration 2491, loss = 0.00328080\n",
      "Iteration 2492, loss = 0.00327547\n",
      "Iteration 2493, loss = 0.00327930\n",
      "Iteration 2494, loss = 0.00326555\n",
      "Iteration 2495, loss = 0.00327428\n",
      "Iteration 2496, loss = 0.00326960\n",
      "Iteration 2497, loss = 0.00325452\n",
      "Iteration 2498, loss = 0.00326898\n",
      "Iteration 2499, loss = 0.00328179\n",
      "Iteration 2500, loss = 0.00324281\n",
      "Iteration 2501, loss = 0.00325762\n",
      "Iteration 2502, loss = 0.00324934\n",
      "Iteration 2503, loss = 0.00324166\n",
      "Iteration 2504, loss = 0.00323974\n",
      "Iteration 2505, loss = 0.00322935\n",
      "Iteration 2506, loss = 0.00323790\n",
      "Iteration 2507, loss = 0.00322376\n",
      "Iteration 2508, loss = 0.00322215\n",
      "Iteration 2509, loss = 0.00321939\n",
      "Iteration 2510, loss = 0.00321964\n",
      "Iteration 2511, loss = 0.00321304\n",
      "Iteration 2512, loss = 0.00320759\n",
      "Iteration 2513, loss = 0.00322427\n",
      "Iteration 2514, loss = 0.00320086\n",
      "Iteration 2515, loss = 0.00322144\n",
      "Iteration 2516, loss = 0.00319329\n",
      "Iteration 2517, loss = 0.00319298\n",
      "Iteration 2518, loss = 0.00319596\n",
      "Iteration 2519, loss = 0.00318643\n",
      "Iteration 2520, loss = 0.00319174\n",
      "Iteration 2521, loss = 0.00319162\n",
      "Iteration 2522, loss = 0.00318744\n",
      "Iteration 2523, loss = 0.00317340\n",
      "Iteration 2524, loss = 0.00318462\n",
      "Iteration 2525, loss = 0.00317304\n",
      "Iteration 2526, loss = 0.00316908\n",
      "Iteration 2527, loss = 0.00317735\n",
      "Iteration 2528, loss = 0.00315453\n",
      "Iteration 2529, loss = 0.00316037\n",
      "Iteration 2530, loss = 0.00316684\n",
      "Iteration 2531, loss = 0.00315388\n",
      "Iteration 2532, loss = 0.00316157\n",
      "Iteration 2533, loss = 0.00315409\n",
      "Iteration 2534, loss = 0.00314175\n",
      "Iteration 2535, loss = 0.00315468\n",
      "Iteration 2536, loss = 0.00313607\n",
      "Iteration 2537, loss = 0.00314641\n",
      "Iteration 2538, loss = 0.00312845\n",
      "Iteration 2539, loss = 0.00312252\n",
      "Iteration 2540, loss = 0.00312856\n",
      "Iteration 2541, loss = 0.00313098\n",
      "Iteration 2542, loss = 0.00311956\n",
      "Iteration 2543, loss = 0.00311115\n",
      "Iteration 2544, loss = 0.00311202\n",
      "Iteration 2545, loss = 0.00311138\n",
      "Iteration 2546, loss = 0.00310606\n",
      "Iteration 2547, loss = 0.00311846\n",
      "Iteration 2548, loss = 0.00311303\n",
      "Iteration 2549, loss = 0.00311453\n",
      "Iteration 2550, loss = 0.00311885\n",
      "Iteration 2551, loss = 0.00310351\n",
      "Iteration 2552, loss = 0.00309882\n",
      "Iteration 2553, loss = 0.00309167\n",
      "Iteration 2554, loss = 0.00310552\n",
      "Iteration 2555, loss = 0.00308551\n",
      "Iteration 2556, loss = 0.00307418\n",
      "Iteration 2557, loss = 0.00308113\n",
      "Iteration 2558, loss = 0.00309196\n",
      "Iteration 2559, loss = 0.00307860\n",
      "Iteration 2560, loss = 0.00307612\n",
      "Iteration 2561, loss = 0.00307751\n",
      "Iteration 2562, loss = 0.00305700\n",
      "Iteration 2563, loss = 0.00305770\n",
      "Iteration 2564, loss = 0.00306149\n",
      "Iteration 2565, loss = 0.00305300\n",
      "Iteration 2566, loss = 0.00304692\n",
      "Iteration 2567, loss = 0.00305237\n",
      "Iteration 2568, loss = 0.00304363\n",
      "Iteration 2569, loss = 0.00305281\n",
      "Iteration 2570, loss = 0.00304267\n",
      "Iteration 2571, loss = 0.00303644\n",
      "Iteration 2572, loss = 0.00302812\n",
      "Iteration 2573, loss = 0.00303360\n",
      "Iteration 2574, loss = 0.00303353\n",
      "Iteration 2575, loss = 0.00302728\n",
      "Iteration 2576, loss = 0.00303012\n",
      "Iteration 2577, loss = 0.00302678\n",
      "Iteration 2578, loss = 0.00301904\n",
      "Iteration 2579, loss = 0.00301503\n",
      "Iteration 2580, loss = 0.00302117\n",
      "Iteration 2581, loss = 0.00300552\n",
      "Iteration 2582, loss = 0.00300205\n",
      "Iteration 2583, loss = 0.00299925\n",
      "Iteration 2584, loss = 0.00301368\n",
      "Iteration 2585, loss = 0.00301777\n",
      "Iteration 2586, loss = 0.00300440\n",
      "Iteration 2587, loss = 0.00300860\n",
      "Iteration 2588, loss = 0.00298593\n",
      "Iteration 2589, loss = 0.00298927\n",
      "Iteration 2590, loss = 0.00297741\n",
      "Iteration 2591, loss = 0.00298694\n",
      "Iteration 2592, loss = 0.00298026\n",
      "Iteration 2593, loss = 0.00296794\n",
      "Iteration 2594, loss = 0.00297748\n",
      "Iteration 2595, loss = 0.00296164\n",
      "Iteration 2596, loss = 0.00296066\n",
      "Iteration 2597, loss = 0.00296427\n",
      "Iteration 2598, loss = 0.00296398\n",
      "Iteration 2599, loss = 0.00295504\n",
      "Iteration 2600, loss = 0.00296898\n",
      "Iteration 2601, loss = 0.00294817\n",
      "Iteration 2602, loss = 0.00294675\n",
      "Iteration 2603, loss = 0.00296009\n",
      "Iteration 2604, loss = 0.00295622\n",
      "Iteration 2605, loss = 0.00296759\n",
      "Iteration 2606, loss = 0.00293585\n",
      "Iteration 2607, loss = 0.00293598\n",
      "Iteration 2608, loss = 0.00293046\n",
      "Iteration 2609, loss = 0.00294074\n",
      "Iteration 2610, loss = 0.00292623\n",
      "Iteration 2611, loss = 0.00292247\n",
      "Iteration 2612, loss = 0.00292178\n",
      "Iteration 2613, loss = 0.00291954\n",
      "Iteration 2614, loss = 0.00291465\n",
      "Iteration 2615, loss = 0.00294159\n",
      "Iteration 2616, loss = 0.00291890\n",
      "Iteration 2617, loss = 0.00290850\n",
      "Iteration 2618, loss = 0.00290281\n",
      "Iteration 2619, loss = 0.00290015\n",
      "Iteration 2620, loss = 0.00290210\n",
      "Iteration 2621, loss = 0.00289850\n",
      "Iteration 2622, loss = 0.00290675\n",
      "Iteration 2623, loss = 0.00289290\n",
      "Iteration 2624, loss = 0.00291985\n",
      "Iteration 2625, loss = 0.00289875\n",
      "Iteration 2626, loss = 0.00288830\n",
      "Iteration 2627, loss = 0.00287930\n",
      "Iteration 2628, loss = 0.00287558\n",
      "Iteration 2629, loss = 0.00287855\n",
      "Iteration 2630, loss = 0.00287077\n",
      "Iteration 2631, loss = 0.00287319\n",
      "Iteration 2632, loss = 0.00286856\n",
      "Iteration 2633, loss = 0.00287248\n",
      "Iteration 2634, loss = 0.00287972\n",
      "Iteration 2635, loss = 0.00286241\n",
      "Iteration 2636, loss = 0.00286135\n",
      "Iteration 2637, loss = 0.00285118\n",
      "Iteration 2638, loss = 0.00285242\n",
      "Iteration 2639, loss = 0.00284678\n",
      "Iteration 2640, loss = 0.00288285\n",
      "Iteration 2641, loss = 0.00284298\n",
      "Iteration 2642, loss = 0.00287529\n",
      "Iteration 2643, loss = 0.00284171\n",
      "Iteration 2644, loss = 0.00283992\n",
      "Iteration 2645, loss = 0.00284273\n",
      "Iteration 2646, loss = 0.00283573\n",
      "Iteration 2647, loss = 0.00284775\n",
      "Iteration 2648, loss = 0.00282794\n",
      "Iteration 2649, loss = 0.00282446\n",
      "Iteration 2650, loss = 0.00282313\n",
      "Iteration 2651, loss = 0.00281881\n",
      "Iteration 2652, loss = 0.00282299\n",
      "Iteration 2653, loss = 0.00282534\n",
      "Iteration 2654, loss = 0.00282213\n",
      "Iteration 2655, loss = 0.00280686\n",
      "Iteration 2656, loss = 0.00280492\n",
      "Iteration 2657, loss = 0.00280499\n",
      "Iteration 2658, loss = 0.00280032\n",
      "Iteration 2659, loss = 0.00279928\n",
      "Iteration 2660, loss = 0.00279509\n",
      "Iteration 2661, loss = 0.00281467\n",
      "Iteration 2662, loss = 0.00279515\n",
      "Iteration 2663, loss = 0.00279431\n",
      "Iteration 2664, loss = 0.00278719\n",
      "Iteration 2665, loss = 0.00278760\n",
      "Iteration 2666, loss = 0.00279211\n",
      "Iteration 2667, loss = 0.00279198\n",
      "Iteration 2668, loss = 0.00278728\n",
      "Iteration 2669, loss = 0.00277533\n",
      "Iteration 2670, loss = 0.00277064\n",
      "Iteration 2671, loss = 0.00278554\n",
      "Iteration 2672, loss = 0.00277214\n",
      "Iteration 2673, loss = 0.00276761\n",
      "Iteration 2674, loss = 0.00277078\n",
      "Iteration 2675, loss = 0.00277503\n",
      "Iteration 2676, loss = 0.00276453\n",
      "Iteration 2677, loss = 0.00275583\n",
      "Iteration 2678, loss = 0.00275092\n",
      "Iteration 2679, loss = 0.00275646\n",
      "Iteration 2680, loss = 0.00274664\n",
      "Iteration 2681, loss = 0.00275168\n",
      "Iteration 2682, loss = 0.00275045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2683, loss = 0.00274911\n",
      "Iteration 2684, loss = 0.00274448\n",
      "Iteration 2685, loss = 0.00273857\n",
      "Iteration 2686, loss = 0.00273260\n",
      "Iteration 2687, loss = 0.00274609\n",
      "Iteration 2688, loss = 0.00272938\n",
      "Iteration 2689, loss = 0.00273254\n",
      "Iteration 2690, loss = 0.00273158\n",
      "Iteration 2691, loss = 0.00273205\n",
      "Iteration 2692, loss = 0.00272596\n",
      "Iteration 2693, loss = 0.00272427\n",
      "Iteration 2694, loss = 0.00271679\n",
      "Iteration 2695, loss = 0.00272130\n",
      "Iteration 2696, loss = 0.00272075\n",
      "Iteration 2697, loss = 0.00272416\n",
      "Iteration 2698, loss = 0.00270693\n",
      "Iteration 2699, loss = 0.00271400\n",
      "Iteration 2700, loss = 0.00270286\n",
      "Iteration 2701, loss = 0.00270513\n",
      "Iteration 2702, loss = 0.00270044\n",
      "Iteration 2703, loss = 0.00272367\n",
      "Iteration 2704, loss = 0.00269936\n",
      "Iteration 2705, loss = 0.00270167\n",
      "Iteration 2706, loss = 0.00269974\n",
      "Iteration 2707, loss = 0.00268389\n",
      "Iteration 2708, loss = 0.00268559\n",
      "Iteration 2709, loss = 0.00268718\n",
      "Iteration 2710, loss = 0.00267945\n",
      "Iteration 2711, loss = 0.00268204\n",
      "Iteration 2712, loss = 0.00268441\n",
      "Iteration 2713, loss = 0.00268068\n",
      "Iteration 2714, loss = 0.00267729\n",
      "Iteration 2715, loss = 0.00266387\n",
      "Iteration 2716, loss = 0.00266817\n",
      "Iteration 2717, loss = 0.00267007\n",
      "Iteration 2718, loss = 0.00266163\n",
      "Iteration 2719, loss = 0.00266174\n",
      "Iteration 2720, loss = 0.00266143\n",
      "Iteration 2721, loss = 0.00265707\n",
      "Iteration 2722, loss = 0.00265794\n",
      "Iteration 2723, loss = 0.00265557\n",
      "Iteration 2724, loss = 0.00265503\n",
      "Iteration 2725, loss = 0.00265447\n",
      "Iteration 2726, loss = 0.00264276\n",
      "Iteration 2727, loss = 0.00264466\n",
      "Iteration 2728, loss = 0.00265318\n",
      "Iteration 2729, loss = 0.00265164\n",
      "Iteration 2730, loss = 0.00264826\n",
      "Iteration 2731, loss = 0.00263514\n",
      "Iteration 2732, loss = 0.00263075\n",
      "Iteration 2733, loss = 0.00262469\n",
      "Iteration 2734, loss = 0.00262243\n",
      "Iteration 2735, loss = 0.00262045\n",
      "Iteration 2736, loss = 0.00262385\n",
      "Iteration 2737, loss = 0.00261871\n",
      "Iteration 2738, loss = 0.00262775\n",
      "Iteration 2739, loss = 0.00261770\n",
      "Iteration 2740, loss = 0.00261275\n",
      "Iteration 2741, loss = 0.00261951\n",
      "Iteration 2742, loss = 0.00261204\n",
      "Iteration 2743, loss = 0.00260999\n",
      "Iteration 2744, loss = 0.00260046\n",
      "Iteration 2745, loss = 0.00260508\n",
      "Iteration 2746, loss = 0.00259765\n",
      "Iteration 2747, loss = 0.00259899\n",
      "Iteration 2748, loss = 0.00259330\n",
      "Iteration 2749, loss = 0.00259164\n",
      "Iteration 2750, loss = 0.00259338\n",
      "Iteration 2751, loss = 0.00258731\n",
      "Iteration 2752, loss = 0.00259424\n",
      "Iteration 2753, loss = 0.00258778\n",
      "Iteration 2754, loss = 0.00258141\n",
      "Iteration 2755, loss = 0.00257627\n",
      "Iteration 2756, loss = 0.00257425\n",
      "Iteration 2757, loss = 0.00257518\n",
      "Iteration 2758, loss = 0.00257995\n",
      "Iteration 2759, loss = 0.00257052\n",
      "Iteration 2760, loss = 0.00257358\n",
      "Iteration 2761, loss = 0.00256331\n",
      "Iteration 2762, loss = 0.00256305\n",
      "Iteration 2763, loss = 0.00256770\n",
      "Iteration 2764, loss = 0.00255617\n",
      "Iteration 2765, loss = 0.00256197\n",
      "Iteration 2766, loss = 0.00255992\n",
      "Iteration 2767, loss = 0.00255399\n",
      "Iteration 2768, loss = 0.00255432\n",
      "Iteration 2769, loss = 0.00256985\n",
      "Iteration 2770, loss = 0.00254983\n",
      "Iteration 2771, loss = 0.00255566\n",
      "Iteration 2772, loss = 0.00254220\n",
      "Iteration 2773, loss = 0.00255033\n",
      "Iteration 2774, loss = 0.00254000\n",
      "Iteration 2775, loss = 0.00254766\n",
      "Iteration 2776, loss = 0.00253614\n",
      "Iteration 2777, loss = 0.00253822\n",
      "Iteration 2778, loss = 0.00254202\n",
      "Iteration 2779, loss = 0.00253501\n",
      "Iteration 2780, loss = 0.00252658\n",
      "Iteration 2781, loss = 0.00252566\n",
      "Iteration 2782, loss = 0.00252084\n",
      "Iteration 2783, loss = 0.00251623\n",
      "Iteration 2784, loss = 0.00251534\n",
      "Iteration 2785, loss = 0.00252547\n",
      "Iteration 2786, loss = 0.00251361\n",
      "Iteration 2787, loss = 0.00250809\n",
      "Iteration 2788, loss = 0.00251517\n",
      "Iteration 2789, loss = 0.00250588\n",
      "Iteration 2790, loss = 0.00250450\n",
      "Iteration 2791, loss = 0.00250122\n",
      "Iteration 2792, loss = 0.00250187\n",
      "Iteration 2793, loss = 0.00249574\n",
      "Iteration 2794, loss = 0.00249569\n",
      "Iteration 2795, loss = 0.00249662\n",
      "Iteration 2796, loss = 0.00250108\n",
      "Iteration 2797, loss = 0.00249139\n",
      "Iteration 2798, loss = 0.00249645\n",
      "Iteration 2799, loss = 0.00249323\n",
      "Iteration 2800, loss = 0.00248292\n",
      "Iteration 2801, loss = 0.00248922\n",
      "Iteration 2802, loss = 0.00248176\n",
      "Iteration 2803, loss = 0.00247780\n",
      "Iteration 2804, loss = 0.00247611\n",
      "Iteration 2805, loss = 0.00247530\n",
      "Iteration 2806, loss = 0.00247193\n",
      "Iteration 2807, loss = 0.00247039\n",
      "Iteration 2808, loss = 0.00247357\n",
      "Iteration 2809, loss = 0.00246372\n",
      "Iteration 2810, loss = 0.00246174\n",
      "Iteration 2811, loss = 0.00246023\n",
      "Iteration 2812, loss = 0.00246208\n",
      "Iteration 2813, loss = 0.00245816\n",
      "Iteration 2814, loss = 0.00245578\n",
      "Iteration 2815, loss = 0.00245427\n",
      "Iteration 2816, loss = 0.00245472\n",
      "Iteration 2817, loss = 0.00245338\n",
      "Iteration 2818, loss = 0.00245545\n",
      "Iteration 2819, loss = 0.00244497\n",
      "Iteration 2820, loss = 0.00244179\n",
      "Iteration 2821, loss = 0.00244560\n",
      "Iteration 2822, loss = 0.00244078\n",
      "Iteration 2823, loss = 0.00245139\n",
      "Iteration 2824, loss = 0.00244705\n",
      "Iteration 2825, loss = 0.00243587\n",
      "Iteration 2826, loss = 0.00243418\n",
      "Iteration 2827, loss = 0.00242821\n",
      "Iteration 2828, loss = 0.00244029\n",
      "Iteration 2829, loss = 0.00243215\n",
      "Iteration 2830, loss = 0.00242514\n",
      "Iteration 2831, loss = 0.00243793\n",
      "Iteration 2832, loss = 0.00242507\n",
      "Iteration 2833, loss = 0.00242313\n",
      "Iteration 2834, loss = 0.00242843\n",
      "Iteration 2835, loss = 0.00242446\n",
      "Iteration 2836, loss = 0.00242464\n",
      "Iteration 2837, loss = 0.00241410\n",
      "Iteration 2838, loss = 0.00241030\n",
      "Iteration 2839, loss = 0.00241597\n",
      "Iteration 2840, loss = 0.00241736\n",
      "Iteration 2841, loss = 0.00240434\n",
      "Iteration 2842, loss = 0.00240130\n",
      "Iteration 2843, loss = 0.00240476\n",
      "Iteration 2844, loss = 0.00240006\n",
      "Iteration 2845, loss = 0.00239708\n",
      "Iteration 2846, loss = 0.00240435\n",
      "Iteration 2847, loss = 0.00239526\n",
      "Iteration 2848, loss = 0.00239497\n",
      "Iteration 2849, loss = 0.00238971\n",
      "Iteration 2850, loss = 0.00239297\n",
      "Iteration 2851, loss = 0.00238877\n",
      "Iteration 2852, loss = 0.00238532\n",
      "Iteration 2853, loss = 0.00238219\n",
      "Iteration 2854, loss = 0.00238110\n",
      "Iteration 2855, loss = 0.00237813\n",
      "Iteration 2856, loss = 0.00239877\n",
      "Iteration 2857, loss = 0.00237936\n",
      "Iteration 2858, loss = 0.00237952\n",
      "Iteration 2859, loss = 0.00238035\n",
      "Iteration 2860, loss = 0.00237087\n",
      "Iteration 2861, loss = 0.00236915\n",
      "Iteration 2862, loss = 0.00236350\n",
      "Iteration 2863, loss = 0.00237358\n",
      "Iteration 2864, loss = 0.00236972\n",
      "Iteration 2865, loss = 0.00236993\n",
      "Iteration 2866, loss = 0.00236090\n",
      "Iteration 2867, loss = 0.00235501\n",
      "Iteration 2868, loss = 0.00235368\n",
      "Iteration 2869, loss = 0.00236962\n",
      "Iteration 2870, loss = 0.00235647\n",
      "Iteration 2871, loss = 0.00235758\n",
      "Iteration 2872, loss = 0.00236180\n",
      "Iteration 2873, loss = 0.00236159\n",
      "Iteration 2874, loss = 0.00234341\n",
      "Iteration 2875, loss = 0.00233873\n",
      "Iteration 2876, loss = 0.00234442\n",
      "Iteration 2877, loss = 0.00233804\n",
      "Iteration 2878, loss = 0.00234067\n",
      "Iteration 2879, loss = 0.00234186\n",
      "Iteration 2880, loss = 0.00234068\n",
      "Iteration 2881, loss = 0.00233811\n",
      "Iteration 2882, loss = 0.00232908\n",
      "Iteration 2883, loss = 0.00232540\n",
      "Iteration 2884, loss = 0.00232333\n",
      "Iteration 2885, loss = 0.00232205\n",
      "Iteration 2886, loss = 0.00232459\n",
      "Iteration 2887, loss = 0.00232060\n",
      "Iteration 2888, loss = 0.00231838\n",
      "Iteration 2889, loss = 0.00231861\n",
      "Iteration 2890, loss = 0.00231755\n",
      "Iteration 2891, loss = 0.00231321\n",
      "Iteration 2892, loss = 0.00231284\n",
      "Iteration 2893, loss = 0.00231620\n",
      "Iteration 2894, loss = 0.00230639\n",
      "Iteration 2895, loss = 0.00230603\n",
      "Iteration 2896, loss = 0.00230029\n",
      "Iteration 2897, loss = 0.00230144\n",
      "Iteration 2898, loss = 0.00230023\n",
      "Iteration 2899, loss = 0.00229799\n",
      "Iteration 2900, loss = 0.00229931\n",
      "Iteration 2901, loss = 0.00229976\n",
      "Iteration 2902, loss = 0.00229781\n",
      "Iteration 2903, loss = 0.00229621\n",
      "Iteration 2904, loss = 0.00229079\n",
      "Iteration 2905, loss = 0.00228802\n",
      "Iteration 2906, loss = 0.00228597\n",
      "Iteration 2907, loss = 0.00228407\n",
      "Iteration 2908, loss = 0.00228276\n",
      "Iteration 2909, loss = 0.00228185\n",
      "Iteration 2910, loss = 0.00228028\n",
      "Iteration 2911, loss = 0.00227890\n",
      "Iteration 2912, loss = 0.00228088\n",
      "Iteration 2913, loss = 0.00227339\n",
      "Iteration 2914, loss = 0.00227641\n",
      "Iteration 2915, loss = 0.00227722\n",
      "Iteration 2916, loss = 0.00227343\n",
      "Iteration 2917, loss = 0.00228247\n",
      "Iteration 2918, loss = 0.00227698\n",
      "Iteration 2919, loss = 0.00227420\n",
      "Iteration 2920, loss = 0.00226172\n",
      "Iteration 2921, loss = 0.00226256\n",
      "Iteration 2922, loss = 0.00225958\n",
      "Iteration 2923, loss = 0.00226035\n",
      "Iteration 2924, loss = 0.00225474\n",
      "Iteration 2925, loss = 0.00225773\n",
      "Iteration 2926, loss = 0.00225270\n",
      "Iteration 2927, loss = 0.00225304\n",
      "Iteration 2928, loss = 0.00224668\n",
      "Iteration 2929, loss = 0.00224598\n",
      "Iteration 2930, loss = 0.00224915\n",
      "Iteration 2931, loss = 0.00224893\n",
      "Iteration 2932, loss = 0.00224604\n",
      "Iteration 2933, loss = 0.00224428\n",
      "Iteration 2934, loss = 0.00224463\n",
      "Iteration 2935, loss = 0.00223469\n",
      "Iteration 2936, loss = 0.00223885\n",
      "Iteration 2937, loss = 0.00223585\n",
      "Iteration 2938, loss = 0.00223170\n",
      "Iteration 2939, loss = 0.00223320\n",
      "Iteration 2940, loss = 0.00222877\n",
      "Iteration 2941, loss = 0.00223390\n",
      "Iteration 2942, loss = 0.00223003\n",
      "Iteration 2943, loss = 0.00222811\n",
      "Iteration 2944, loss = 0.00222316\n",
      "Iteration 2945, loss = 0.00222138\n",
      "Iteration 2946, loss = 0.00222849\n",
      "Iteration 2947, loss = 0.00223150\n",
      "Iteration 2948, loss = 0.00221879\n",
      "Iteration 2949, loss = 0.00221584\n",
      "Iteration 2950, loss = 0.00221663\n",
      "Iteration 2951, loss = 0.00221203\n",
      "Iteration 2952, loss = 0.00221294\n",
      "Iteration 2953, loss = 0.00221040\n",
      "Iteration 2954, loss = 0.00221146\n",
      "Iteration 2955, loss = 0.00220946\n",
      "Iteration 2956, loss = 0.00221035\n",
      "Iteration 2957, loss = 0.00220419\n",
      "Iteration 2958, loss = 0.00220793\n",
      "Iteration 2959, loss = 0.00220157\n",
      "Iteration 2960, loss = 0.00220500\n",
      "Iteration 2961, loss = 0.00219316\n",
      "Iteration 2962, loss = 0.00219249\n",
      "Iteration 2963, loss = 0.00220093\n",
      "Iteration 2964, loss = 0.00219999\n",
      "Iteration 2965, loss = 0.00218955\n",
      "Iteration 2966, loss = 0.00218730\n",
      "Iteration 2967, loss = 0.00219087\n",
      "Iteration 2968, loss = 0.00218367\n",
      "Iteration 2969, loss = 0.00218326\n",
      "Iteration 2970, loss = 0.00218387\n",
      "Iteration 2971, loss = 0.00217994\n",
      "Iteration 2972, loss = 0.00218542\n",
      "Iteration 2973, loss = 0.00217472\n",
      "Iteration 2974, loss = 0.00217861\n",
      "Iteration 2975, loss = 0.00218829\n",
      "Iteration 2976, loss = 0.00217011\n",
      "Iteration 2977, loss = 0.00218386\n",
      "Iteration 2978, loss = 0.00217073\n",
      "Iteration 2979, loss = 0.00216314\n",
      "Iteration 2980, loss = 0.00217392\n",
      "Iteration 2981, loss = 0.00216869\n",
      "Iteration 2982, loss = 0.00216862\n",
      "Iteration 2983, loss = 0.00216931\n",
      "Iteration 2984, loss = 0.00215840\n",
      "Iteration 2985, loss = 0.00215506\n",
      "Iteration 2986, loss = 0.00216659\n",
      "Iteration 2987, loss = 0.00215745\n",
      "Iteration 2988, loss = 0.00215512\n",
      "Iteration 2989, loss = 0.00214760\n",
      "Iteration 2990, loss = 0.00214732\n",
      "Iteration 2991, loss = 0.00216061\n",
      "Iteration 2992, loss = 0.00215525\n",
      "Iteration 2993, loss = 0.00214442\n",
      "Iteration 2994, loss = 0.00214351\n",
      "Iteration 2995, loss = 0.00214038\n",
      "Iteration 2996, loss = 0.00214321\n",
      "Iteration 2997, loss = 0.00214053\n",
      "Iteration 2998, loss = 0.00214157\n",
      "Iteration 2999, loss = 0.00213513\n",
      "Iteration 3000, loss = 0.00214692\n",
      "Iteration 3001, loss = 0.00213258\n",
      "Iteration 3002, loss = 0.00213295\n",
      "Iteration 3003, loss = 0.00213791\n",
      "Iteration 3004, loss = 0.00213459\n",
      "Iteration 3005, loss = 0.00213264\n",
      "Iteration 3006, loss = 0.00212883\n",
      "Iteration 3007, loss = 0.00213019\n",
      "Iteration 3008, loss = 0.00212070\n",
      "Iteration 3009, loss = 0.00212908\n",
      "Iteration 3010, loss = 0.00211750\n",
      "Iteration 3011, loss = 0.00211595\n",
      "Iteration 3012, loss = 0.00212504\n",
      "Iteration 3013, loss = 0.00211274\n",
      "Iteration 3014, loss = 0.00211419\n",
      "Iteration 3015, loss = 0.00211710\n",
      "Iteration 3016, loss = 0.00210950\n",
      "Iteration 3017, loss = 0.00210697\n",
      "Iteration 3018, loss = 0.00211130\n",
      "Iteration 3019, loss = 0.00211860\n",
      "Iteration 3020, loss = 0.00210168\n",
      "Iteration 3021, loss = 0.00210918\n",
      "Iteration 3022, loss = 0.00211241\n",
      "Iteration 3023, loss = 0.00211463\n",
      "Iteration 3024, loss = 0.00210109\n",
      "Iteration 3025, loss = 0.00210226\n",
      "Iteration 3026, loss = 0.00209959\n",
      "Iteration 3027, loss = 0.00209548\n",
      "Iteration 3028, loss = 0.00209496\n",
      "Iteration 3029, loss = 0.00209531\n",
      "Iteration 3030, loss = 0.00209952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3031, loss = 0.00208757\n",
      "Iteration 3032, loss = 0.00208894\n",
      "Iteration 3033, loss = 0.00209289\n",
      "Iteration 3034, loss = 0.00208317\n",
      "Iteration 3035, loss = 0.00208575\n",
      "Iteration 3036, loss = 0.00208218\n",
      "Iteration 3037, loss = 0.00208316\n",
      "Iteration 3038, loss = 0.00208496\n",
      "Iteration 3039, loss = 0.00208154\n",
      "Iteration 3040, loss = 0.00207516\n",
      "Iteration 3041, loss = 0.00207378\n",
      "Iteration 3042, loss = 0.00207055\n",
      "Iteration 3043, loss = 0.00207187\n",
      "Iteration 3044, loss = 0.00206701\n",
      "Iteration 3045, loss = 0.00206689\n",
      "Iteration 3046, loss = 0.00206616\n",
      "Iteration 3047, loss = 0.00206328\n",
      "Iteration 3048, loss = 0.00206170\n",
      "Iteration 3049, loss = 0.00206554\n",
      "Iteration 3050, loss = 0.00206241\n",
      "Iteration 3051, loss = 0.00207410\n",
      "Iteration 3052, loss = 0.00206292\n",
      "Iteration 3053, loss = 0.00205540\n",
      "Iteration 3054, loss = 0.00205408\n",
      "Iteration 3055, loss = 0.00205313\n",
      "Iteration 3056, loss = 0.00205281\n",
      "Iteration 3057, loss = 0.00205025\n",
      "Iteration 3058, loss = 0.00205047\n",
      "Iteration 3059, loss = 0.00204705\n",
      "Iteration 3060, loss = 0.00204475\n",
      "Iteration 3061, loss = 0.00204838\n",
      "Iteration 3062, loss = 0.00204420\n",
      "Iteration 3063, loss = 0.00204423\n",
      "Iteration 3064, loss = 0.00204515\n",
      "Iteration 3065, loss = 0.00204299\n",
      "Iteration 3066, loss = 0.00203780\n",
      "Iteration 3067, loss = 0.00203945\n",
      "Iteration 3068, loss = 0.00203457\n",
      "Iteration 3069, loss = 0.00203618\n",
      "Iteration 3070, loss = 0.00203293\n",
      "Iteration 3071, loss = 0.00203205\n",
      "Iteration 3072, loss = 0.00203023\n",
      "Iteration 3073, loss = 0.00202890\n",
      "Iteration 3074, loss = 0.00203379\n",
      "Iteration 3075, loss = 0.00202737\n",
      "Iteration 3076, loss = 0.00203459\n",
      "Iteration 3077, loss = 0.00202515\n",
      "Iteration 3078, loss = 0.00202538\n",
      "Iteration 3079, loss = 0.00202027\n",
      "Iteration 3080, loss = 0.00202166\n",
      "Iteration 3081, loss = 0.00202077\n",
      "Iteration 3082, loss = 0.00201933\n",
      "Iteration 3083, loss = 0.00201606\n",
      "Iteration 3084, loss = 0.00202182\n",
      "Iteration 3085, loss = 0.00201095\n",
      "Iteration 3086, loss = 0.00201117\n",
      "Iteration 3087, loss = 0.00201401\n",
      "Iteration 3088, loss = 0.00200857\n",
      "Iteration 3089, loss = 0.00200972\n",
      "Iteration 3090, loss = 0.00200525\n",
      "Iteration 3091, loss = 0.00200467\n",
      "Iteration 3092, loss = 0.00201372\n",
      "Iteration 3093, loss = 0.00202167\n",
      "Iteration 3094, loss = 0.00200049\n",
      "Iteration 3095, loss = 0.00200200\n",
      "Iteration 3096, loss = 0.00200050\n",
      "Iteration 3097, loss = 0.00200093\n",
      "Iteration 3098, loss = 0.00199769\n",
      "Iteration 3099, loss = 0.00199523\n",
      "Iteration 3100, loss = 0.00199348\n",
      "Iteration 3101, loss = 0.00199443\n",
      "Iteration 3102, loss = 0.00198937\n",
      "Iteration 3103, loss = 0.00198756\n",
      "Iteration 3104, loss = 0.00198598\n",
      "Iteration 3105, loss = 0.00198693\n",
      "Iteration 3106, loss = 0.00198561\n",
      "Iteration 3107, loss = 0.00198193\n",
      "Iteration 3108, loss = 0.00198376\n",
      "Iteration 3109, loss = 0.00198745\n",
      "Iteration 3110, loss = 0.00198297\n",
      "Iteration 3111, loss = 0.00197991\n",
      "Iteration 3112, loss = 0.00197547\n",
      "Iteration 3113, loss = 0.00197755\n",
      "Iteration 3114, loss = 0.00197296\n",
      "Iteration 3115, loss = 0.00197966\n",
      "Iteration 3116, loss = 0.00197026\n",
      "Iteration 3117, loss = 0.00197612\n",
      "Iteration 3118, loss = 0.00197754\n",
      "Iteration 3119, loss = 0.00196652\n",
      "Iteration 3120, loss = 0.00196519\n",
      "Iteration 3121, loss = 0.00196565\n",
      "Iteration 3122, loss = 0.00196247\n",
      "Iteration 3123, loss = 0.00196721\n",
      "Iteration 3124, loss = 0.00196496\n",
      "Iteration 3125, loss = 0.00197209\n",
      "Iteration 3126, loss = 0.00196177\n",
      "Iteration 3127, loss = 0.00195626\n",
      "Iteration 3128, loss = 0.00195666\n",
      "Iteration 3129, loss = 0.00195462\n",
      "Iteration 3130, loss = 0.00195279\n",
      "Iteration 3131, loss = 0.00195585\n",
      "Iteration 3132, loss = 0.00195391\n",
      "Iteration 3133, loss = 0.00196557\n",
      "Iteration 3134, loss = 0.00194786\n",
      "Iteration 3135, loss = 0.00194776\n",
      "Iteration 3136, loss = 0.00195302\n",
      "Iteration 3137, loss = 0.00194549\n",
      "Iteration 3138, loss = 0.00195044\n",
      "Iteration 3139, loss = 0.00195082\n",
      "Iteration 3140, loss = 0.00195354\n",
      "Iteration 3141, loss = 0.00194373\n",
      "Iteration 3142, loss = 0.00193734\n",
      "Iteration 3143, loss = 0.00194038\n",
      "Iteration 3144, loss = 0.00193722\n",
      "Iteration 3145, loss = 0.00193923\n",
      "Iteration 3146, loss = 0.00193810\n",
      "Iteration 3147, loss = 0.00193300\n",
      "Iteration 3148, loss = 0.00193911\n",
      "Iteration 3149, loss = 0.00193167\n",
      "Iteration 3150, loss = 0.00192741\n",
      "Iteration 3151, loss = 0.00193247\n",
      "Iteration 3152, loss = 0.00192963\n",
      "Iteration 3153, loss = 0.00192856\n",
      "Iteration 3154, loss = 0.00192494\n",
      "Iteration 3155, loss = 0.00193215\n",
      "Iteration 3156, loss = 0.00191910\n",
      "Iteration 3157, loss = 0.00192196\n",
      "Iteration 3158, loss = 0.00191724\n",
      "Iteration 3159, loss = 0.00192169\n",
      "Iteration 3160, loss = 0.00191409\n",
      "Iteration 3161, loss = 0.00191489\n",
      "Iteration 3162, loss = 0.00192053\n",
      "Iteration 3163, loss = 0.00192573\n",
      "Iteration 3164, loss = 0.00191545\n",
      "Iteration 3165, loss = 0.00192190\n",
      "Iteration 3166, loss = 0.00191208\n",
      "Iteration 3167, loss = 0.00191516\n",
      "Iteration 3168, loss = 0.00190541\n",
      "Iteration 3169, loss = 0.00190282\n",
      "Iteration 3170, loss = 0.00190366\n",
      "Iteration 3171, loss = 0.00190227\n",
      "Iteration 3172, loss = 0.00190097\n",
      "Iteration 3173, loss = 0.00190321\n",
      "Iteration 3174, loss = 0.00189713\n",
      "Iteration 3175, loss = 0.00189825\n",
      "Iteration 3176, loss = 0.00189798\n",
      "Iteration 3177, loss = 0.00189611\n",
      "Iteration 3178, loss = 0.00189558\n",
      "Iteration 3179, loss = 0.00189139\n",
      "Iteration 3180, loss = 0.00188905\n",
      "Iteration 3181, loss = 0.00189029\n",
      "Iteration 3182, loss = 0.00189052\n",
      "Iteration 3183, loss = 0.00188814\n",
      "Iteration 3184, loss = 0.00188762\n",
      "Iteration 3185, loss = 0.00188865\n",
      "Iteration 3186, loss = 0.00188409\n",
      "Iteration 3187, loss = 0.00188315\n",
      "Iteration 3188, loss = 0.00188513\n",
      "Iteration 3189, loss = 0.00188096\n",
      "Iteration 3190, loss = 0.00187978\n",
      "Iteration 3191, loss = 0.00187979\n",
      "Iteration 3192, loss = 0.00187566\n",
      "Iteration 3193, loss = 0.00187539\n",
      "Iteration 3194, loss = 0.00187648\n",
      "Iteration 3195, loss = 0.00187811\n",
      "Iteration 3196, loss = 0.00187429\n",
      "Iteration 3197, loss = 0.00187263\n",
      "Iteration 3198, loss = 0.00187151\n",
      "Iteration 3199, loss = 0.00186983\n",
      "Iteration 3200, loss = 0.00187644\n",
      "Iteration 3201, loss = 0.00187379\n",
      "Iteration 3202, loss = 0.00186989\n",
      "Iteration 3203, loss = 0.00186698\n",
      "Iteration 3204, loss = 0.00186553\n",
      "Iteration 3205, loss = 0.00186316\n",
      "Iteration 3206, loss = 0.00186067\n",
      "Iteration 3207, loss = 0.00186233\n",
      "Iteration 3208, loss = 0.00185921\n",
      "Iteration 3209, loss = 0.00185809\n",
      "Iteration 3210, loss = 0.00185571\n",
      "Iteration 3211, loss = 0.00185383\n",
      "Iteration 3212, loss = 0.00185573\n",
      "Iteration 3213, loss = 0.00185004\n",
      "Iteration 3214, loss = 0.00184957\n",
      "Iteration 3215, loss = 0.00184940\n",
      "Iteration 3216, loss = 0.00184844\n",
      "Iteration 3217, loss = 0.00184630\n",
      "Iteration 3218, loss = 0.00184594\n",
      "Iteration 3219, loss = 0.00184540\n",
      "Iteration 3220, loss = 0.00184768\n",
      "Iteration 3221, loss = 0.00184448\n",
      "Iteration 3222, loss = 0.00184566\n",
      "Iteration 3223, loss = 0.00184480\n",
      "Iteration 3224, loss = 0.00184400\n",
      "Iteration 3225, loss = 0.00184162\n",
      "Iteration 3226, loss = 0.00184665\n",
      "Iteration 3227, loss = 0.00183528\n",
      "Iteration 3228, loss = 0.00183536\n",
      "Iteration 3229, loss = 0.00183222\n",
      "Iteration 3230, loss = 0.00184173\n",
      "Iteration 3231, loss = 0.00183144\n",
      "Iteration 3232, loss = 0.00183168\n",
      "Iteration 3233, loss = 0.00183045\n",
      "Iteration 3234, loss = 0.00182750\n",
      "Iteration 3235, loss = 0.00182916\n",
      "Iteration 3236, loss = 0.00182910\n",
      "Iteration 3237, loss = 0.00182542\n",
      "Iteration 3238, loss = 0.00182349\n",
      "Iteration 3239, loss = 0.00182122\n",
      "Iteration 3240, loss = 0.00183227\n",
      "Iteration 3241, loss = 0.00182256\n",
      "Iteration 3242, loss = 0.00182296\n",
      "Iteration 3243, loss = 0.00181943\n",
      "Iteration 3244, loss = 0.00182232\n",
      "Iteration 3245, loss = 0.00181755\n",
      "Iteration 3246, loss = 0.00181728\n",
      "Iteration 3247, loss = 0.00181641\n",
      "Iteration 3248, loss = 0.00181425\n",
      "Iteration 3249, loss = 0.00181168\n",
      "Iteration 3250, loss = 0.00181428\n",
      "Iteration 3251, loss = 0.00181585\n",
      "Iteration 3252, loss = 0.00181158\n",
      "Iteration 3253, loss = 0.00180936\n",
      "Iteration 3254, loss = 0.00181192\n",
      "Iteration 3255, loss = 0.00180358\n",
      "Iteration 3256, loss = 0.00180635\n",
      "Iteration 3257, loss = 0.00180295\n",
      "Iteration 3258, loss = 0.00180079\n",
      "Iteration 3259, loss = 0.00180226\n",
      "Iteration 3260, loss = 0.00180415\n",
      "Iteration 3261, loss = 0.00179959\n",
      "Iteration 3262, loss = 0.00179811\n",
      "Iteration 3263, loss = 0.00179782\n",
      "Iteration 3264, loss = 0.00179877\n",
      "Iteration 3265, loss = 0.00179467\n",
      "Iteration 3266, loss = 0.00179563\n",
      "Iteration 3267, loss = 0.00179265\n",
      "Iteration 3268, loss = 0.00179573\n",
      "Iteration 3269, loss = 0.00178969\n",
      "Iteration 3270, loss = 0.00178847\n",
      "Iteration 3271, loss = 0.00178978\n",
      "Iteration 3272, loss = 0.00178663\n",
      "Iteration 3273, loss = 0.00179368\n",
      "Iteration 3274, loss = 0.00178317\n",
      "Iteration 3275, loss = 0.00178566\n",
      "Iteration 3276, loss = 0.00178974\n",
      "Iteration 3277, loss = 0.00179384\n",
      "Iteration 3278, loss = 0.00178433\n",
      "Iteration 3279, loss = 0.00178251\n",
      "Iteration 3280, loss = 0.00177629\n",
      "Iteration 3281, loss = 0.00177843\n",
      "Iteration 3282, loss = 0.00177654\n",
      "Iteration 3283, loss = 0.00177533\n",
      "Iteration 3284, loss = 0.00177354\n",
      "Iteration 3285, loss = 0.00177335\n",
      "Iteration 3286, loss = 0.00176848\n",
      "Iteration 3287, loss = 0.00177507\n",
      "Iteration 3288, loss = 0.00177300\n",
      "Iteration 3289, loss = 0.00177355\n",
      "Iteration 3290, loss = 0.00176565\n",
      "Iteration 3291, loss = 0.00176429\n",
      "Iteration 3292, loss = 0.00176401\n",
      "Iteration 3293, loss = 0.00176651\n",
      "Iteration 3294, loss = 0.00176059\n",
      "Iteration 3295, loss = 0.00176226\n",
      "Iteration 3296, loss = 0.00176348\n",
      "Iteration 3297, loss = 0.00176233\n",
      "Iteration 3298, loss = 0.00175976\n",
      "Iteration 3299, loss = 0.00176070\n",
      "Iteration 3300, loss = 0.00176240\n",
      "Iteration 3301, loss = 0.00175843\n",
      "Iteration 3302, loss = 0.00175937\n",
      "Iteration 3303, loss = 0.00175555\n",
      "Iteration 3304, loss = 0.00175142\n",
      "Iteration 3305, loss = 0.00175339\n",
      "Iteration 3306, loss = 0.00175603\n",
      "Iteration 3307, loss = 0.00174739\n",
      "Iteration 3308, loss = 0.00174543\n",
      "Iteration 3309, loss = 0.00174687\n",
      "Iteration 3310, loss = 0.00174712\n",
      "Iteration 3311, loss = 0.00174515\n",
      "Iteration 3312, loss = 0.00174057\n",
      "Iteration 3313, loss = 0.00174350\n",
      "Iteration 3314, loss = 0.00174005\n",
      "Iteration 3315, loss = 0.00173955\n",
      "Iteration 3316, loss = 0.00174726\n",
      "Iteration 3317, loss = 0.00174013\n",
      "Iteration 3318, loss = 0.00173948\n",
      "Iteration 3319, loss = 0.00173636\n",
      "Iteration 3320, loss = 0.00173403\n",
      "Iteration 3321, loss = 0.00174468\n",
      "Iteration 3322, loss = 0.00173254\n",
      "Iteration 3323, loss = 0.00173188\n",
      "Iteration 3324, loss = 0.00172916\n",
      "Iteration 3325, loss = 0.00173398\n",
      "Iteration 3326, loss = 0.00173132\n",
      "Iteration 3327, loss = 0.00172682\n",
      "Iteration 3328, loss = 0.00172830\n",
      "Iteration 3329, loss = 0.00172668\n",
      "Iteration 3330, loss = 0.00172509\n",
      "Iteration 3331, loss = 0.00172495\n",
      "Iteration 3332, loss = 0.00172475\n",
      "Iteration 3333, loss = 0.00171926\n",
      "Iteration 3334, loss = 0.00171998\n",
      "Iteration 3335, loss = 0.00171727\n",
      "Iteration 3336, loss = 0.00171681\n",
      "Iteration 3337, loss = 0.00172648\n",
      "Iteration 3338, loss = 0.00171597\n",
      "Iteration 3339, loss = 0.00171325\n",
      "Iteration 3340, loss = 0.00171947\n",
      "Iteration 3341, loss = 0.00171654\n",
      "Iteration 3342, loss = 0.00171213\n",
      "Iteration 3343, loss = 0.00171188\n",
      "Iteration 3344, loss = 0.00170898\n",
      "Iteration 3345, loss = 0.00170916\n",
      "Iteration 3346, loss = 0.00171091\n",
      "Iteration 3347, loss = 0.00170732\n",
      "Iteration 3348, loss = 0.00170837\n",
      "Iteration 3349, loss = 0.00171226\n",
      "Iteration 3350, loss = 0.00170373\n",
      "Iteration 3351, loss = 0.00170322\n",
      "Iteration 3352, loss = 0.00170103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3353, loss = 0.00170494\n",
      "Iteration 3354, loss = 0.00170077\n",
      "Iteration 3355, loss = 0.00169805\n",
      "Iteration 3356, loss = 0.00169663\n",
      "Iteration 3357, loss = 0.00170627\n",
      "Iteration 3358, loss = 0.00169391\n",
      "Iteration 3359, loss = 0.00169501\n",
      "Iteration 3360, loss = 0.00170757\n",
      "Iteration 3361, loss = 0.00169671\n",
      "Iteration 3362, loss = 0.00169080\n",
      "Iteration 3363, loss = 0.00169081\n",
      "Iteration 3364, loss = 0.00169015\n",
      "Iteration 3365, loss = 0.00168885\n",
      "Iteration 3366, loss = 0.00169448\n",
      "Iteration 3367, loss = 0.00168794\n",
      "Iteration 3368, loss = 0.00168605\n",
      "Iteration 3369, loss = 0.00168706\n",
      "Iteration 3370, loss = 0.00168273\n",
      "Iteration 3371, loss = 0.00168787\n",
      "Iteration 3372, loss = 0.00168384\n",
      "Iteration 3373, loss = 0.00168852\n",
      "Iteration 3374, loss = 0.00168644\n",
      "Iteration 3375, loss = 0.00168387\n",
      "Iteration 3376, loss = 0.00168718\n",
      "Iteration 3377, loss = 0.00167746\n",
      "Iteration 3378, loss = 0.00167929\n",
      "Iteration 3379, loss = 0.00167660\n",
      "Iteration 3380, loss = 0.00167621\n",
      "Iteration 3381, loss = 0.00167964\n",
      "Iteration 3382, loss = 0.00167513\n",
      "Iteration 3383, loss = 0.00167067\n",
      "Iteration 3384, loss = 0.00166981\n",
      "Iteration 3385, loss = 0.00167696\n",
      "Iteration 3386, loss = 0.00167630\n",
      "Iteration 3387, loss = 0.00167035\n",
      "Iteration 3388, loss = 0.00167131\n",
      "Iteration 3389, loss = 0.00166525\n",
      "Iteration 3390, loss = 0.00166497\n",
      "Iteration 3391, loss = 0.00166453\n",
      "Iteration 3392, loss = 0.00166338\n",
      "Iteration 3393, loss = 0.00166120\n",
      "Iteration 3394, loss = 0.00165892\n",
      "Iteration 3395, loss = 0.00166452\n",
      "Iteration 3396, loss = 0.00165720\n",
      "Iteration 3397, loss = 0.00165690\n",
      "Iteration 3398, loss = 0.00165715\n",
      "Iteration 3399, loss = 0.00165629\n",
      "Iteration 3400, loss = 0.00165726\n",
      "Iteration 3401, loss = 0.00165430\n",
      "Iteration 3402, loss = 0.00165357\n",
      "Iteration 3403, loss = 0.00165159\n",
      "Iteration 3404, loss = 0.00165337\n",
      "Iteration 3405, loss = 0.00164866\n",
      "Iteration 3406, loss = 0.00165215\n",
      "Iteration 3407, loss = 0.00164963\n",
      "Iteration 3408, loss = 0.00165154\n",
      "Iteration 3409, loss = 0.00164881\n",
      "Iteration 3410, loss = 0.00164672\n",
      "Iteration 3411, loss = 0.00164712\n",
      "Iteration 3412, loss = 0.00164565\n",
      "Iteration 3413, loss = 0.00164313\n",
      "Iteration 3414, loss = 0.00164303\n",
      "Iteration 3415, loss = 0.00164349\n",
      "Iteration 3416, loss = 0.00164579\n",
      "Iteration 3417, loss = 0.00164009\n",
      "Iteration 3418, loss = 0.00165252\n",
      "Iteration 3419, loss = 0.00164268\n",
      "Iteration 3420, loss = 0.00163984\n",
      "Iteration 3421, loss = 0.00163602\n",
      "Iteration 3422, loss = 0.00163355\n",
      "Iteration 3423, loss = 0.00163326\n",
      "Iteration 3424, loss = 0.00163532\n",
      "Iteration 3425, loss = 0.00163100\n",
      "Iteration 3426, loss = 0.00163082\n",
      "Iteration 3427, loss = 0.00163076\n",
      "Iteration 3428, loss = 0.00163212\n",
      "Iteration 3429, loss = 0.00162889\n",
      "Iteration 3430, loss = 0.00162740\n",
      "Iteration 3431, loss = 0.00162685\n",
      "Iteration 3432, loss = 0.00162569\n",
      "Iteration 3433, loss = 0.00162642\n",
      "Iteration 3434, loss = 0.00162734\n",
      "Iteration 3435, loss = 0.00162382\n",
      "Iteration 3436, loss = 0.00162267\n",
      "Iteration 3437, loss = 0.00162197\n",
      "Iteration 3438, loss = 0.00162034\n",
      "Iteration 3439, loss = 0.00161946\n",
      "Iteration 3440, loss = 0.00162003\n",
      "Iteration 3441, loss = 0.00161554\n",
      "Iteration 3442, loss = 0.00162217\n",
      "Iteration 3443, loss = 0.00161900\n",
      "Iteration 3444, loss = 0.00162256\n",
      "Iteration 3445, loss = 0.00161528\n",
      "Iteration 3446, loss = 0.00161331\n",
      "Iteration 3447, loss = 0.00161966\n",
      "Iteration 3448, loss = 0.00161164\n",
      "Iteration 3449, loss = 0.00161249\n",
      "Iteration 3450, loss = 0.00160951\n",
      "Iteration 3451, loss = 0.00160948\n",
      "Iteration 3452, loss = 0.00160740\n",
      "Iteration 3453, loss = 0.00160960\n",
      "Iteration 3454, loss = 0.00160529\n",
      "Iteration 3455, loss = 0.00161121\n",
      "Iteration 3456, loss = 0.00160814\n",
      "Iteration 3457, loss = 0.00160234\n",
      "Iteration 3458, loss = 0.00160127\n",
      "Iteration 3459, loss = 0.00160239\n",
      "Iteration 3460, loss = 0.00159911\n",
      "Iteration 3461, loss = 0.00159931\n",
      "Iteration 3462, loss = 0.00160280\n",
      "Iteration 3463, loss = 0.00159729\n",
      "Iteration 3464, loss = 0.00159668\n",
      "Iteration 3465, loss = 0.00159868\n",
      "Iteration 3466, loss = 0.00159414\n",
      "Iteration 3467, loss = 0.00159344\n",
      "Iteration 3468, loss = 0.00159174\n",
      "Iteration 3469, loss = 0.00159756\n",
      "Iteration 3470, loss = 0.00159234\n",
      "Iteration 3471, loss = 0.00159156\n",
      "Iteration 3472, loss = 0.00159179\n",
      "Iteration 3473, loss = 0.00159180\n",
      "Iteration 3474, loss = 0.00159074\n",
      "Iteration 3475, loss = 0.00158861\n",
      "Iteration 3476, loss = 0.00158895\n",
      "Iteration 3477, loss = 0.00158432\n",
      "Iteration 3478, loss = 0.00158645\n",
      "Iteration 3479, loss = 0.00158535\n",
      "Iteration 3480, loss = 0.00158286\n",
      "Iteration 3481, loss = 0.00158161\n",
      "Iteration 3482, loss = 0.00158022\n",
      "Iteration 3483, loss = 0.00158251\n",
      "Iteration 3484, loss = 0.00157973\n",
      "Iteration 3485, loss = 0.00157911\n",
      "Iteration 3486, loss = 0.00157697\n",
      "Iteration 3487, loss = 0.00157824\n",
      "Iteration 3488, loss = 0.00157572\n",
      "Iteration 3489, loss = 0.00159060\n",
      "Iteration 3490, loss = 0.00157812\n",
      "Iteration 3491, loss = 0.00157714\n",
      "Iteration 3492, loss = 0.00157438\n",
      "Iteration 3493, loss = 0.00157017\n",
      "Iteration 3494, loss = 0.00156978\n",
      "Iteration 3495, loss = 0.00157296\n",
      "Iteration 3496, loss = 0.00156877\n",
      "Iteration 3497, loss = 0.00156942\n",
      "Iteration 3498, loss = 0.00157142\n",
      "Iteration 3499, loss = 0.00156680\n",
      "Iteration 3500, loss = 0.00156453\n",
      "Iteration 3501, loss = 0.00156623\n",
      "Iteration 3502, loss = 0.00157037\n",
      "Iteration 3503, loss = 0.00156325\n",
      "Iteration 3504, loss = 0.00156494\n",
      "Iteration 3505, loss = 0.00156007\n",
      "Iteration 3506, loss = 0.00155909\n",
      "Iteration 3507, loss = 0.00155913\n",
      "Iteration 3508, loss = 0.00155906\n",
      "Iteration 3509, loss = 0.00156018\n",
      "Iteration 3510, loss = 0.00155726\n",
      "Iteration 3511, loss = 0.00155731\n",
      "Iteration 3512, loss = 0.00155927\n",
      "Iteration 3513, loss = 0.00155614\n",
      "Iteration 3514, loss = 0.00155568\n",
      "Iteration 3515, loss = 0.00155113\n",
      "Iteration 3516, loss = 0.00155690\n",
      "Iteration 3517, loss = 0.00155493\n",
      "Iteration 3518, loss = 0.00154985\n",
      "Iteration 3519, loss = 0.00155265\n",
      "Iteration 3520, loss = 0.00154737\n",
      "Iteration 3521, loss = 0.00155634\n",
      "Iteration 3522, loss = 0.00154978\n",
      "Iteration 3523, loss = 0.00155039\n",
      "Iteration 3524, loss = 0.00154556\n",
      "Iteration 3525, loss = 0.00154503\n",
      "Iteration 3526, loss = 0.00154535\n",
      "Iteration 3527, loss = 0.00154447\n",
      "Iteration 3528, loss = 0.00154493\n",
      "Iteration 3529, loss = 0.00154234\n",
      "Iteration 3530, loss = 0.00154501\n",
      "Iteration 3531, loss = 0.00153988\n",
      "Iteration 3532, loss = 0.00153628\n",
      "Iteration 3533, loss = 0.00153668\n",
      "Iteration 3534, loss = 0.00153542\n",
      "Iteration 3535, loss = 0.00153654\n",
      "Iteration 3536, loss = 0.00154081\n",
      "Iteration 3537, loss = 0.00153570\n",
      "Iteration 3538, loss = 0.00153495\n",
      "Iteration 3539, loss = 0.00153247\n",
      "Iteration 3540, loss = 0.00154140\n",
      "Iteration 3541, loss = 0.00153910\n",
      "Iteration 3542, loss = 0.00153269\n",
      "Iteration 3543, loss = 0.00153582\n",
      "Iteration 3544, loss = 0.00152791\n",
      "Iteration 3545, loss = 0.00152751\n",
      "Iteration 3546, loss = 0.00152679\n",
      "Iteration 3547, loss = 0.00152666\n",
      "Iteration 3548, loss = 0.00153073\n",
      "Iteration 3549, loss = 0.00152384\n",
      "Iteration 3550, loss = 0.00152365\n",
      "Iteration 3551, loss = 0.00152331\n",
      "Iteration 3552, loss = 0.00152155\n",
      "Iteration 3553, loss = 0.00152322\n",
      "Iteration 3554, loss = 0.00151893\n",
      "Iteration 3555, loss = 0.00151958\n",
      "Iteration 3556, loss = 0.00152088\n",
      "Iteration 3557, loss = 0.00152031\n",
      "Iteration 3558, loss = 0.00151543\n",
      "Iteration 3559, loss = 0.00151830\n",
      "Iteration 3560, loss = 0.00151617\n",
      "Iteration 3561, loss = 0.00151900\n",
      "Iteration 3562, loss = 0.00151366\n",
      "Iteration 3563, loss = 0.00151277\n",
      "Iteration 3564, loss = 0.00151345\n",
      "Iteration 3565, loss = 0.00151101\n",
      "Iteration 3566, loss = 0.00152498\n",
      "Iteration 3567, loss = 0.00151683\n",
      "Iteration 3568, loss = 0.00151157\n",
      "Iteration 3569, loss = 0.00151092\n",
      "Iteration 3570, loss = 0.00151396\n",
      "Iteration 3571, loss = 0.00150680\n",
      "Iteration 3572, loss = 0.00150962\n",
      "Iteration 3573, loss = 0.00150533\n",
      "Iteration 3574, loss = 0.00150717\n",
      "Iteration 3575, loss = 0.00150992\n",
      "Iteration 3576, loss = 0.00151005\n",
      "Iteration 3577, loss = 0.00150254\n",
      "Iteration 3578, loss = 0.00150168\n",
      "Iteration 3579, loss = 0.00149832\n",
      "Iteration 3580, loss = 0.00150003\n",
      "Iteration 3581, loss = 0.00149756\n",
      "Iteration 3582, loss = 0.00150044\n",
      "Iteration 3583, loss = 0.00149932\n",
      "Iteration 3584, loss = 0.00149602\n",
      "Iteration 3585, loss = 0.00149569\n",
      "Iteration 3586, loss = 0.00149513\n",
      "Iteration 3587, loss = 0.00149385\n",
      "Iteration 3588, loss = 0.00149273\n",
      "Iteration 3589, loss = 0.00149248\n",
      "Iteration 3590, loss = 0.00149195\n",
      "Iteration 3591, loss = 0.00148932\n",
      "Iteration 3592, loss = 0.00148942\n",
      "Iteration 3593, loss = 0.00148892\n",
      "Iteration 3594, loss = 0.00149270\n",
      "Iteration 3595, loss = 0.00149176\n",
      "Iteration 3596, loss = 0.00148982\n",
      "Iteration 3597, loss = 0.00148468\n",
      "Iteration 3598, loss = 0.00148761\n",
      "Iteration 3599, loss = 0.00149024\n",
      "Iteration 3600, loss = 0.00148615\n",
      "Iteration 3601, loss = 0.00148292\n",
      "Iteration 3602, loss = 0.00148977\n",
      "Iteration 3603, loss = 0.00148063\n",
      "Iteration 3604, loss = 0.00148190\n",
      "Iteration 3605, loss = 0.00147843\n",
      "Iteration 3606, loss = 0.00148319\n",
      "Iteration 3607, loss = 0.00148185\n",
      "Iteration 3608, loss = 0.00147801\n",
      "Iteration 3609, loss = 0.00147691\n",
      "Iteration 3610, loss = 0.00147681\n",
      "Iteration 3611, loss = 0.00147406\n",
      "Iteration 3612, loss = 0.00147509\n",
      "Iteration 3613, loss = 0.00147403\n",
      "Iteration 3614, loss = 0.00147437\n",
      "Iteration 3615, loss = 0.00147361\n",
      "Iteration 3616, loss = 0.00147642\n",
      "Iteration 3617, loss = 0.00147156\n",
      "Iteration 3618, loss = 0.00146979\n",
      "Iteration 3619, loss = 0.00146903\n",
      "Iteration 3620, loss = 0.00147054\n",
      "Iteration 3621, loss = 0.00146890\n",
      "Iteration 3622, loss = 0.00146822\n",
      "Iteration 3623, loss = 0.00146519\n",
      "Iteration 3624, loss = 0.00146494\n",
      "Iteration 3625, loss = 0.00146298\n",
      "Iteration 3626, loss = 0.00146272\n",
      "Iteration 3627, loss = 0.00146485\n",
      "Iteration 3628, loss = 0.00146623\n",
      "Iteration 3629, loss = 0.00146571\n",
      "Iteration 3630, loss = 0.00146199\n",
      "Iteration 3631, loss = 0.00145861\n",
      "Iteration 3632, loss = 0.00146276\n",
      "Iteration 3633, loss = 0.00145933\n",
      "Iteration 3634, loss = 0.00146012\n",
      "Iteration 3635, loss = 0.00145602\n",
      "Iteration 3636, loss = 0.00146014\n",
      "Iteration 3637, loss = 0.00145503\n",
      "Iteration 3638, loss = 0.00145514\n",
      "Iteration 3639, loss = 0.00145766\n",
      "Iteration 3640, loss = 0.00145364\n",
      "Iteration 3641, loss = 0.00145095\n",
      "Iteration 3642, loss = 0.00145296\n",
      "Iteration 3643, loss = 0.00144948\n",
      "Iteration 3644, loss = 0.00145086\n",
      "Iteration 3645, loss = 0.00145363\n",
      "Iteration 3646, loss = 0.00145107\n",
      "Iteration 3647, loss = 0.00145250\n",
      "Iteration 3648, loss = 0.00144681\n",
      "Iteration 3649, loss = 0.00144774\n",
      "Iteration 3650, loss = 0.00144647\n",
      "Iteration 3651, loss = 0.00144587\n",
      "Iteration 3652, loss = 0.00145322\n",
      "Iteration 3653, loss = 0.00144585\n",
      "Iteration 3654, loss = 0.00144699\n",
      "Iteration 3655, loss = 0.00144368\n",
      "Iteration 3656, loss = 0.00144040\n",
      "Iteration 3657, loss = 0.00144249\n",
      "Iteration 3658, loss = 0.00144082\n",
      "Iteration 3659, loss = 0.00144328\n",
      "Iteration 3660, loss = 0.00143819\n",
      "Iteration 3661, loss = 0.00144113\n",
      "Iteration 3662, loss = 0.00144002\n",
      "Iteration 3663, loss = 0.00143739\n",
      "Iteration 3664, loss = 0.00143622\n",
      "Iteration 3665, loss = 0.00143688\n",
      "Iteration 3666, loss = 0.00143525\n",
      "Iteration 3667, loss = 0.00143431\n",
      "Iteration 3668, loss = 0.00143283\n",
      "Iteration 3669, loss = 0.00143168\n",
      "Iteration 3670, loss = 0.00143021\n",
      "Iteration 3671, loss = 0.00143224\n",
      "Iteration 3672, loss = 0.00143182\n",
      "Iteration 3673, loss = 0.00142764\n",
      "Iteration 3674, loss = 0.00142754\n",
      "Iteration 3675, loss = 0.00142791\n",
      "Iteration 3676, loss = 0.00142698\n",
      "Iteration 3677, loss = 0.00142764\n",
      "Iteration 3678, loss = 0.00142405\n",
      "Iteration 3679, loss = 0.00142395\n",
      "Iteration 3680, loss = 0.00142806\n",
      "Iteration 3681, loss = 0.00142239\n",
      "Iteration 3682, loss = 0.00142290\n",
      "Iteration 3683, loss = 0.00142077\n",
      "Iteration 3684, loss = 0.00142179\n",
      "Iteration 3685, loss = 0.00142040\n",
      "Iteration 3686, loss = 0.00142100\n",
      "Iteration 3687, loss = 0.00141792\n",
      "Iteration 3688, loss = 0.00141776\n",
      "Iteration 3689, loss = 0.00141774\n",
      "Iteration 3690, loss = 0.00141831\n",
      "Iteration 3691, loss = 0.00142176\n",
      "Iteration 3692, loss = 0.00141671\n",
      "Iteration 3693, loss = 0.00141645\n",
      "Iteration 3694, loss = 0.00141694\n",
      "Iteration 3695, loss = 0.00141856\n",
      "Iteration 3696, loss = 0.00141299\n",
      "Iteration 3697, loss = 0.00141274\n",
      "Iteration 3698, loss = 0.00141251\n",
      "Iteration 3699, loss = 0.00141640\n",
      "Iteration 3700, loss = 0.00141021\n",
      "Iteration 3701, loss = 0.00141079\n",
      "Iteration 3702, loss = 0.00140909\n",
      "Iteration 3703, loss = 0.00140689\n",
      "Iteration 3704, loss = 0.00140897\n",
      "Iteration 3705, loss = 0.00140526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3706, loss = 0.00140505\n",
      "Iteration 3707, loss = 0.00140914\n",
      "Iteration 3708, loss = 0.00140321\n",
      "Iteration 3709, loss = 0.00140632\n",
      "Iteration 3710, loss = 0.00140167\n",
      "Iteration 3711, loss = 0.00140091\n",
      "Iteration 3712, loss = 0.00140179\n",
      "Iteration 3713, loss = 0.00140642\n",
      "Iteration 3714, loss = 0.00140172\n",
      "Iteration 3715, loss = 0.00140447\n",
      "Iteration 3716, loss = 0.00140125\n",
      "Iteration 3717, loss = 0.00140081\n",
      "Iteration 3718, loss = 0.00139819\n",
      "Iteration 3719, loss = 0.00139449\n",
      "Iteration 3720, loss = 0.00139642\n",
      "Iteration 3721, loss = 0.00139760\n",
      "Iteration 3722, loss = 0.00139657\n",
      "Iteration 3723, loss = 0.00139395\n",
      "Iteration 3724, loss = 0.00139262\n",
      "Iteration 3725, loss = 0.00139319\n",
      "Iteration 3726, loss = 0.00139208\n",
      "Iteration 3727, loss = 0.00139028\n",
      "Iteration 3728, loss = 0.00139116\n",
      "Iteration 3729, loss = 0.00139082\n",
      "Iteration 3730, loss = 0.00139350\n",
      "Iteration 3731, loss = 0.00138844\n",
      "Iteration 3732, loss = 0.00139024\n",
      "Iteration 3733, loss = 0.00138554\n",
      "Iteration 3734, loss = 0.00138714\n",
      "Iteration 3735, loss = 0.00138888\n",
      "Iteration 3736, loss = 0.00139264\n",
      "Iteration 3737, loss = 0.00138753\n",
      "Iteration 3738, loss = 0.00138637\n",
      "Iteration 3739, loss = 0.00138589\n",
      "Iteration 3740, loss = 0.00138258\n",
      "Iteration 3741, loss = 0.00138126\n",
      "Iteration 3742, loss = 0.00138036\n",
      "Iteration 3743, loss = 0.00137990\n",
      "Iteration 3744, loss = 0.00138019\n",
      "Iteration 3745, loss = 0.00137885\n",
      "Iteration 3746, loss = 0.00137954\n",
      "Iteration 3747, loss = 0.00137866\n",
      "Iteration 3748, loss = 0.00137618\n",
      "Iteration 3749, loss = 0.00137583\n",
      "Iteration 3750, loss = 0.00137555\n",
      "Iteration 3751, loss = 0.00137509\n",
      "Iteration 3752, loss = 0.00137484\n",
      "Iteration 3753, loss = 0.00137288\n",
      "Iteration 3754, loss = 0.00137601\n",
      "Iteration 3755, loss = 0.00137414\n",
      "Iteration 3756, loss = 0.00137056\n",
      "Iteration 3757, loss = 0.00136943\n",
      "Iteration 3758, loss = 0.00137324\n",
      "Iteration 3759, loss = 0.00137251\n",
      "Iteration 3760, loss = 0.00136884\n",
      "Iteration 3761, loss = 0.00136859\n",
      "Iteration 3762, loss = 0.00136751\n",
      "Iteration 3763, loss = 0.00137115\n",
      "Iteration 3764, loss = 0.00136686\n",
      "Iteration 3765, loss = 0.00136618\n",
      "Iteration 3766, loss = 0.00136558\n",
      "Iteration 3767, loss = 0.00136843\n",
      "Iteration 3768, loss = 0.00136578\n",
      "Iteration 3769, loss = 0.00136145\n",
      "Iteration 3770, loss = 0.00136126\n",
      "Iteration 3771, loss = 0.00136123\n",
      "Iteration 3772, loss = 0.00135960\n",
      "Iteration 3773, loss = 0.00136029\n",
      "Iteration 3774, loss = 0.00136017\n",
      "Iteration 3775, loss = 0.00135754\n",
      "Iteration 3776, loss = 0.00135747\n",
      "Iteration 3777, loss = 0.00135795\n",
      "Iteration 3778, loss = 0.00135628\n",
      "Iteration 3779, loss = 0.00135574\n",
      "Iteration 3780, loss = 0.00135617\n",
      "Iteration 3781, loss = 0.00135643\n",
      "Iteration 3782, loss = 0.00135573\n",
      "Iteration 3783, loss = 0.00135227\n",
      "Iteration 3784, loss = 0.00135199\n",
      "Iteration 3785, loss = 0.00135258\n",
      "Iteration 3786, loss = 0.00135478\n",
      "Iteration 3787, loss = 0.00135162\n",
      "Iteration 3788, loss = 0.00135059\n",
      "Iteration 3789, loss = 0.00135100\n",
      "Iteration 3790, loss = 0.00135087\n",
      "Iteration 3791, loss = 0.00135124\n",
      "Iteration 3792, loss = 0.00134788\n",
      "Iteration 3793, loss = 0.00134740\n",
      "Iteration 3794, loss = 0.00134750\n",
      "Iteration 3795, loss = 0.00134550\n",
      "Iteration 3796, loss = 0.00134636\n",
      "Iteration 3797, loss = 0.00134494\n",
      "Iteration 3798, loss = 0.00134771\n",
      "Iteration 3799, loss = 0.00134273\n",
      "Iteration 3800, loss = 0.00134298\n",
      "Iteration 3801, loss = 0.00134164\n",
      "Iteration 3802, loss = 0.00134131\n",
      "Iteration 3803, loss = 0.00133873\n",
      "Iteration 3804, loss = 0.00134378\n",
      "Iteration 3805, loss = 0.00133937\n",
      "Iteration 3806, loss = 0.00133887\n",
      "Iteration 3807, loss = 0.00133674\n",
      "Iteration 3808, loss = 0.00133688\n",
      "Iteration 3809, loss = 0.00133824\n",
      "Iteration 3810, loss = 0.00133542\n",
      "Iteration 3811, loss = 0.00133804\n",
      "Iteration 3812, loss = 0.00133331\n",
      "Iteration 3813, loss = 0.00133538\n",
      "Iteration 3814, loss = 0.00133400\n",
      "Iteration 3815, loss = 0.00133361\n",
      "Iteration 3816, loss = 0.00133444\n",
      "Iteration 3817, loss = 0.00133097\n",
      "Iteration 3818, loss = 0.00133169\n",
      "Iteration 3819, loss = 0.00133027\n",
      "Iteration 3820, loss = 0.00133188\n",
      "Iteration 3821, loss = 0.00132961\n",
      "Iteration 3822, loss = 0.00133295\n",
      "Iteration 3823, loss = 0.00133103\n",
      "Iteration 3824, loss = 0.00132953\n",
      "Iteration 3825, loss = 0.00132651\n",
      "Iteration 3826, loss = 0.00132695\n",
      "Iteration 3827, loss = 0.00132394\n",
      "Iteration 3828, loss = 0.00132396\n",
      "Iteration 3829, loss = 0.00132378\n",
      "Iteration 3830, loss = 0.00132312\n",
      "Iteration 3831, loss = 0.00132236\n",
      "Iteration 3832, loss = 0.00132208\n",
      "Iteration 3833, loss = 0.00132219\n",
      "Iteration 3834, loss = 0.00132074\n",
      "Iteration 3835, loss = 0.00131942\n",
      "Iteration 3836, loss = 0.00131912\n",
      "Iteration 3837, loss = 0.00131890\n",
      "Iteration 3838, loss = 0.00131918\n",
      "Iteration 3839, loss = 0.00131930\n",
      "Iteration 3840, loss = 0.00131572\n",
      "Iteration 3841, loss = 0.00131917\n",
      "Iteration 3842, loss = 0.00131465\n",
      "Iteration 3843, loss = 0.00131927\n",
      "Iteration 3844, loss = 0.00131354\n",
      "Iteration 3845, loss = 0.00131460\n",
      "Iteration 3846, loss = 0.00131473\n",
      "Iteration 3847, loss = 0.00131403\n",
      "Iteration 3848, loss = 0.00131341\n",
      "Iteration 3849, loss = 0.00131342\n",
      "Iteration 3850, loss = 0.00131150\n",
      "Iteration 3851, loss = 0.00131054\n",
      "Iteration 3852, loss = 0.00130930\n",
      "Iteration 3853, loss = 0.00130878\n",
      "Iteration 3854, loss = 0.00130788\n",
      "Iteration 3855, loss = 0.00131564\n",
      "Iteration 3856, loss = 0.00130839\n",
      "Iteration 3857, loss = 0.00130750\n",
      "Iteration 3858, loss = 0.00130549\n",
      "Iteration 3859, loss = 0.00130601\n",
      "Iteration 3860, loss = 0.00130530\n",
      "Iteration 3861, loss = 0.00130603\n",
      "Iteration 3862, loss = 0.00130487\n",
      "Iteration 3863, loss = 0.00130263\n",
      "Iteration 3864, loss = 0.00130130\n",
      "Iteration 3865, loss = 0.00130173\n",
      "Iteration 3866, loss = 0.00130078\n",
      "Iteration 3867, loss = 0.00129905\n",
      "Iteration 3868, loss = 0.00129973\n",
      "Iteration 3869, loss = 0.00129787\n",
      "Iteration 3870, loss = 0.00129781\n",
      "Iteration 3871, loss = 0.00129916\n",
      "Iteration 3872, loss = 0.00129672\n",
      "Iteration 3873, loss = 0.00130117\n",
      "Iteration 3874, loss = 0.00129492\n",
      "Iteration 3875, loss = 0.00130060\n",
      "Iteration 3876, loss = 0.00129540\n",
      "Iteration 3877, loss = 0.00129409\n",
      "Iteration 3878, loss = 0.00129229\n",
      "Iteration 3879, loss = 0.00129250\n",
      "Iteration 3880, loss = 0.00129342\n",
      "Iteration 3881, loss = 0.00129806\n",
      "Iteration 3882, loss = 0.00129054\n",
      "Iteration 3883, loss = 0.00129209\n",
      "Iteration 3884, loss = 0.00128996\n",
      "Iteration 3885, loss = 0.00129005\n",
      "Iteration 3886, loss = 0.00129076\n",
      "Iteration 3887, loss = 0.00128854\n",
      "Iteration 3888, loss = 0.00128636\n",
      "Iteration 3889, loss = 0.00128639\n",
      "Iteration 3890, loss = 0.00128506\n",
      "Iteration 3891, loss = 0.00128608\n",
      "Iteration 3892, loss = 0.00128518\n",
      "Iteration 3893, loss = 0.00128331\n",
      "Iteration 3894, loss = 0.00128356\n",
      "Iteration 3895, loss = 0.00128760\n",
      "Iteration 3896, loss = 0.00128545\n",
      "Iteration 3897, loss = 0.00128149\n",
      "Iteration 3898, loss = 0.00128081\n",
      "Iteration 3899, loss = 0.00128098\n",
      "Iteration 3900, loss = 0.00128000\n",
      "Iteration 3901, loss = 0.00128069\n",
      "Iteration 3902, loss = 0.00127808\n",
      "Iteration 3903, loss = 0.00127802\n",
      "Iteration 3904, loss = 0.00127854\n",
      "Iteration 3905, loss = 0.00128188\n",
      "Iteration 3906, loss = 0.00127637\n",
      "Iteration 3907, loss = 0.00127770\n",
      "Iteration 3908, loss = 0.00127612\n",
      "Iteration 3909, loss = 0.00127449\n",
      "Iteration 3910, loss = 0.00127409\n",
      "Iteration 3911, loss = 0.00127699\n",
      "Iteration 3912, loss = 0.00127855\n",
      "Iteration 3913, loss = 0.00127287\n",
      "Iteration 3914, loss = 0.00127082\n",
      "Iteration 3915, loss = 0.00127152\n",
      "Iteration 3916, loss = 0.00127197\n",
      "Iteration 3917, loss = 0.00127014\n",
      "Iteration 3918, loss = 0.00126839\n",
      "Iteration 3919, loss = 0.00127014\n",
      "Iteration 3920, loss = 0.00126780\n",
      "Iteration 3921, loss = 0.00127060\n",
      "Iteration 3922, loss = 0.00126633\n",
      "Iteration 3923, loss = 0.00126741\n",
      "Iteration 3924, loss = 0.00126755\n",
      "Iteration 3925, loss = 0.00126539\n",
      "Iteration 3926, loss = 0.00126469\n",
      "Iteration 3927, loss = 0.00126487\n",
      "Iteration 3928, loss = 0.00126553\n",
      "Iteration 3929, loss = 0.00126324\n",
      "Iteration 3930, loss = 0.00126295\n",
      "Iteration 3931, loss = 0.00126066\n",
      "Iteration 3932, loss = 0.00126332\n",
      "Iteration 3933, loss = 0.00127191\n",
      "Iteration 3934, loss = 0.00126175\n",
      "Iteration 3935, loss = 0.00126005\n",
      "Iteration 3936, loss = 0.00125959\n",
      "Iteration 3937, loss = 0.00125905\n",
      "Iteration 3938, loss = 0.00125754\n",
      "Iteration 3939, loss = 0.00126057\n",
      "Iteration 3940, loss = 0.00125685\n",
      "Iteration 3941, loss = 0.00125744\n",
      "Iteration 3942, loss = 0.00125550\n",
      "Iteration 3943, loss = 0.00125515\n",
      "Iteration 3944, loss = 0.00125651\n",
      "Iteration 3945, loss = 0.00125377\n",
      "Iteration 3946, loss = 0.00125253\n",
      "Iteration 3947, loss = 0.00125116\n",
      "Iteration 3948, loss = 0.00125185\n",
      "Iteration 3949, loss = 0.00125137\n",
      "Iteration 3950, loss = 0.00125212\n",
      "Iteration 3951, loss = 0.00125034\n",
      "Iteration 3952, loss = 0.00125342\n",
      "Iteration 3953, loss = 0.00124919\n",
      "Iteration 3954, loss = 0.00124878\n",
      "Iteration 3955, loss = 0.00124939\n",
      "Iteration 3956, loss = 0.00124638\n",
      "Iteration 3957, loss = 0.00124572\n",
      "Iteration 3958, loss = 0.00124622\n",
      "Iteration 3959, loss = 0.00124495\n",
      "Iteration 3960, loss = 0.00124636\n",
      "Iteration 3961, loss = 0.00124394\n",
      "Iteration 3962, loss = 0.00125137\n",
      "Iteration 3963, loss = 0.00124936\n",
      "Iteration 3964, loss = 0.00124354\n",
      "Iteration 3965, loss = 0.00124492\n",
      "Iteration 3966, loss = 0.00124151\n",
      "Iteration 3967, loss = 0.00124162\n",
      "Iteration 3968, loss = 0.00124683\n",
      "Iteration 3969, loss = 0.00124095\n",
      "Iteration 3970, loss = 0.00124287\n",
      "Iteration 3971, loss = 0.00123807\n",
      "Iteration 3972, loss = 0.00123973\n",
      "Iteration 3973, loss = 0.00123945\n",
      "Iteration 3974, loss = 0.00123959\n",
      "Iteration 3975, loss = 0.00123791\n",
      "Iteration 3976, loss = 0.00123837\n",
      "Iteration 3977, loss = 0.00123718\n",
      "Iteration 3978, loss = 0.00123434\n",
      "Iteration 3979, loss = 0.00123556\n",
      "Iteration 3980, loss = 0.00123476\n",
      "Iteration 3981, loss = 0.00123849\n",
      "Iteration 3982, loss = 0.00123492\n",
      "Iteration 3983, loss = 0.00123841\n",
      "Iteration 3984, loss = 0.00123199\n",
      "Iteration 3985, loss = 0.00123036\n",
      "Iteration 3986, loss = 0.00123423\n",
      "Iteration 3987, loss = 0.00123301\n",
      "Iteration 3988, loss = 0.00123976\n",
      "Iteration 3989, loss = 0.00123063\n",
      "Iteration 3990, loss = 0.00122939\n",
      "Iteration 3991, loss = 0.00122825\n",
      "Iteration 3992, loss = 0.00123079\n",
      "Iteration 3993, loss = 0.00122688\n",
      "Iteration 3994, loss = 0.00122680\n",
      "Iteration 3995, loss = 0.00122696\n",
      "Iteration 3996, loss = 0.00122678\n",
      "Iteration 3997, loss = 0.00122520\n",
      "Iteration 3998, loss = 0.00122591\n",
      "Iteration 3999, loss = 0.00122285\n",
      "Iteration 4000, loss = 0.00122362\n",
      "Iteration 4001, loss = 0.00122504\n",
      "Iteration 4002, loss = 0.00122311\n",
      "Iteration 4003, loss = 0.00121971\n",
      "Iteration 4004, loss = 0.00122194\n",
      "Iteration 4005, loss = 0.00122170\n",
      "Iteration 4006, loss = 0.00122708\n",
      "Iteration 4007, loss = 0.00122212\n",
      "Iteration 4008, loss = 0.00121701\n",
      "Iteration 4009, loss = 0.00121957\n",
      "Iteration 4010, loss = 0.00121942\n",
      "Iteration 4011, loss = 0.00121770\n",
      "Iteration 4012, loss = 0.00121527\n",
      "Iteration 4013, loss = 0.00121925\n",
      "Iteration 4014, loss = 0.00121709\n",
      "Iteration 4015, loss = 0.00121606\n",
      "Iteration 4016, loss = 0.00121355\n",
      "Iteration 4017, loss = 0.00121653\n",
      "Iteration 4018, loss = 0.00121785\n",
      "Iteration 4019, loss = 0.00121343\n",
      "Iteration 4020, loss = 0.00121225\n",
      "Iteration 4021, loss = 0.00121158\n",
      "Iteration 4022, loss = 0.00121148\n",
      "Iteration 4023, loss = 0.00121325\n",
      "Iteration 4024, loss = 0.00120971\n",
      "Iteration 4025, loss = 0.00120940\n",
      "Iteration 4026, loss = 0.00120941\n",
      "Iteration 4027, loss = 0.00120979\n",
      "Iteration 4028, loss = 0.00120726\n",
      "Iteration 4029, loss = 0.00120823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4030, loss = 0.00120840\n",
      "Iteration 4031, loss = 0.00120581\n",
      "Iteration 4032, loss = 0.00120772\n",
      "Iteration 4033, loss = 0.00120740\n",
      "Iteration 4034, loss = 0.00120579\n",
      "Iteration 4035, loss = 0.00120579\n",
      "Iteration 4036, loss = 0.00120575\n",
      "Iteration 4037, loss = 0.00121002\n",
      "Iteration 4038, loss = 0.00120411\n",
      "Iteration 4039, loss = 0.00120372\n",
      "Iteration 4040, loss = 0.00120136\n",
      "Iteration 4041, loss = 0.00120228\n",
      "Iteration 4042, loss = 0.00120149\n",
      "Iteration 4043, loss = 0.00120452\n",
      "Iteration 4044, loss = 0.00120094\n",
      "Iteration 4045, loss = 0.00120046\n",
      "Iteration 4046, loss = 0.00120073\n",
      "Iteration 4047, loss = 0.00119888\n",
      "Iteration 4048, loss = 0.00119826\n",
      "Iteration 4049, loss = 0.00119753\n",
      "Iteration 4050, loss = 0.00119581\n",
      "Iteration 4051, loss = 0.00119548\n",
      "Iteration 4052, loss = 0.00119708\n",
      "Iteration 4053, loss = 0.00119482\n",
      "Iteration 4054, loss = 0.00119367\n",
      "Iteration 4055, loss = 0.00119284\n",
      "Iteration 4056, loss = 0.00119438\n",
      "Iteration 4057, loss = 0.00119219\n",
      "Iteration 4058, loss = 0.00119263\n",
      "Iteration 4059, loss = 0.00119348\n",
      "Iteration 4060, loss = 0.00119130\n",
      "Iteration 4061, loss = 0.00119066\n",
      "Iteration 4062, loss = 0.00119443\n",
      "Iteration 4063, loss = 0.00118944\n",
      "Iteration 4064, loss = 0.00119758\n",
      "Iteration 4065, loss = 0.00119105\n",
      "Iteration 4066, loss = 0.00119144\n",
      "Iteration 4067, loss = 0.00118700\n",
      "Iteration 4068, loss = 0.00118757\n",
      "Iteration 4069, loss = 0.00118811\n",
      "Iteration 4070, loss = 0.00118754\n",
      "Iteration 4071, loss = 0.00118930\n",
      "Iteration 4072, loss = 0.00118594\n",
      "Iteration 4073, loss = 0.00118527\n",
      "Iteration 4074, loss = 0.00118436\n",
      "Iteration 4075, loss = 0.00118403\n",
      "Iteration 4076, loss = 0.00118482\n",
      "Iteration 4077, loss = 0.00118349\n",
      "Iteration 4078, loss = 0.00118206\n",
      "Iteration 4079, loss = 0.00118599\n",
      "Iteration 4080, loss = 0.00118253\n",
      "Iteration 4081, loss = 0.00117972\n",
      "Iteration 4082, loss = 0.00118260\n",
      "Iteration 4083, loss = 0.00117943\n",
      "Iteration 4084, loss = 0.00118204\n",
      "Iteration 4085, loss = 0.00118014\n",
      "Iteration 4086, loss = 0.00117764\n",
      "Iteration 4087, loss = 0.00118118\n",
      "Iteration 4088, loss = 0.00118111\n",
      "Iteration 4089, loss = 0.00117517\n",
      "Iteration 4090, loss = 0.00117553\n",
      "Iteration 4091, loss = 0.00117875\n",
      "Iteration 4092, loss = 0.00118314\n",
      "Iteration 4093, loss = 0.00117651\n",
      "Iteration 4094, loss = 0.00117405\n",
      "Iteration 4095, loss = 0.00117316\n",
      "Iteration 4096, loss = 0.00117425\n",
      "Iteration 4097, loss = 0.00117275\n",
      "Iteration 4098, loss = 0.00117340\n",
      "Iteration 4099, loss = 0.00117128\n",
      "Iteration 4100, loss = 0.00117086\n",
      "Iteration 4101, loss = 0.00117129\n",
      "Iteration 4102, loss = 0.00117100\n",
      "Iteration 4103, loss = 0.00116980\n",
      "Iteration 4104, loss = 0.00117009\n",
      "Iteration 4105, loss = 0.00117476\n",
      "Iteration 4106, loss = 0.00116744\n",
      "Iteration 4107, loss = 0.00116986\n",
      "Iteration 4108, loss = 0.00116749\n",
      "Iteration 4109, loss = 0.00116571\n",
      "Iteration 4110, loss = 0.00116590\n",
      "Iteration 4111, loss = 0.00116983\n",
      "Iteration 4112, loss = 0.00116639\n",
      "Iteration 4113, loss = 0.00116675\n",
      "Iteration 4114, loss = 0.00116385\n",
      "Iteration 4115, loss = 0.00116488\n",
      "Iteration 4116, loss = 0.00116221\n",
      "Iteration 4117, loss = 0.00116341\n",
      "Iteration 4118, loss = 0.00116112\n",
      "Iteration 4119, loss = 0.00116272\n",
      "Iteration 4120, loss = 0.00116017\n",
      "Iteration 4121, loss = 0.00115988\n",
      "Iteration 4122, loss = 0.00116090\n",
      "Iteration 4123, loss = 0.00115955\n",
      "Iteration 4124, loss = 0.00115800\n",
      "Iteration 4125, loss = 0.00116113\n",
      "Iteration 4126, loss = 0.00115892\n",
      "Iteration 4127, loss = 0.00115685\n",
      "Iteration 4128, loss = 0.00115849\n",
      "Iteration 4129, loss = 0.00116777\n",
      "Iteration 4130, loss = 0.00115506\n",
      "Iteration 4131, loss = 0.00115458\n",
      "Iteration 4132, loss = 0.00115544\n",
      "Iteration 4133, loss = 0.00115365\n",
      "Iteration 4134, loss = 0.00115417\n",
      "Iteration 4135, loss = 0.00115281\n",
      "Iteration 4136, loss = 0.00115782\n",
      "Iteration 4137, loss = 0.00115207\n",
      "Iteration 4138, loss = 0.00115459\n",
      "Iteration 4139, loss = 0.00115057\n",
      "Iteration 4140, loss = 0.00115173\n",
      "Iteration 4141, loss = 0.00115076\n",
      "Iteration 4142, loss = 0.00115152\n",
      "Iteration 4143, loss = 0.00115202\n",
      "Iteration 4144, loss = 0.00115094\n",
      "Iteration 4145, loss = 0.00114979\n",
      "Iteration 4146, loss = 0.00114875\n",
      "Iteration 4147, loss = 0.00114794\n",
      "Iteration 4148, loss = 0.00114861\n",
      "Iteration 4149, loss = 0.00114814\n",
      "Iteration 4150, loss = 0.00114981\n",
      "Iteration 4151, loss = 0.00114540\n",
      "Iteration 4152, loss = 0.00114530\n",
      "Iteration 4153, loss = 0.00114538\n",
      "Iteration 4154, loss = 0.00114424\n",
      "Iteration 4155, loss = 0.00114344\n",
      "Iteration 4156, loss = 0.00114283\n",
      "Iteration 4157, loss = 0.00114439\n",
      "Iteration 4158, loss = 0.00114245\n",
      "Iteration 4159, loss = 0.00114129\n",
      "Iteration 4160, loss = 0.00114268\n",
      "Iteration 4161, loss = 0.00114029\n",
      "Iteration 4162, loss = 0.00114072\n",
      "Iteration 4163, loss = 0.00113912\n",
      "Iteration 4164, loss = 0.00113844\n",
      "Iteration 4165, loss = 0.00113819\n",
      "Iteration 4166, loss = 0.00114186\n",
      "Iteration 4167, loss = 0.00113779\n",
      "Iteration 4168, loss = 0.00113722\n",
      "Iteration 4169, loss = 0.00113681\n",
      "Iteration 4170, loss = 0.00113713\n",
      "Iteration 4171, loss = 0.00113522\n",
      "Iteration 4172, loss = 0.00113599\n",
      "Iteration 4173, loss = 0.00113464\n",
      "Iteration 4174, loss = 0.00113451\n",
      "Iteration 4175, loss = 0.00113670\n",
      "Iteration 4176, loss = 0.00113304\n",
      "Iteration 4177, loss = 0.00113393\n",
      "Iteration 4178, loss = 0.00113319\n",
      "Iteration 4179, loss = 0.00113269\n",
      "Iteration 4180, loss = 0.00113245\n",
      "Iteration 4181, loss = 0.00113151\n",
      "Iteration 4182, loss = 0.00112983\n",
      "Iteration 4183, loss = 0.00113013\n",
      "Iteration 4184, loss = 0.00112991\n",
      "Iteration 4185, loss = 0.00113125\n",
      "Iteration 4186, loss = 0.00113364\n",
      "Iteration 4187, loss = 0.00112885\n",
      "Iteration 4188, loss = 0.00112748\n",
      "Iteration 4189, loss = 0.00112787\n",
      "Iteration 4190, loss = 0.00112838\n",
      "Iteration 4191, loss = 0.00112602\n",
      "Iteration 4192, loss = 0.00112941\n",
      "Iteration 4193, loss = 0.00112575\n",
      "Iteration 4194, loss = 0.00112469\n",
      "Iteration 4195, loss = 0.00112633\n",
      "Iteration 4196, loss = 0.00112514\n",
      "Iteration 4197, loss = 0.00112362\n",
      "Iteration 4198, loss = 0.00112616\n",
      "Iteration 4199, loss = 0.00112251\n",
      "Iteration 4200, loss = 0.00112258\n",
      "Iteration 4201, loss = 0.00112943\n",
      "Iteration 4202, loss = 0.00112301\n",
      "Iteration 4203, loss = 0.00112183\n",
      "Iteration 4204, loss = 0.00112086\n",
      "Iteration 4205, loss = 0.00112072\n",
      "Iteration 4206, loss = 0.00111928\n",
      "Iteration 4207, loss = 0.00112443\n",
      "Iteration 4208, loss = 0.00111899\n",
      "Iteration 4209, loss = 0.00111984\n",
      "Iteration 4210, loss = 0.00112070\n",
      "Iteration 4211, loss = 0.00111713\n",
      "Iteration 4212, loss = 0.00111687\n",
      "Iteration 4213, loss = 0.00111589\n",
      "Iteration 4214, loss = 0.00111799\n",
      "Iteration 4215, loss = 0.00111801\n",
      "Iteration 4216, loss = 0.00111887\n",
      "Iteration 4217, loss = 0.00111712\n",
      "Iteration 4218, loss = 0.00111396\n",
      "Iteration 4219, loss = 0.00111439\n",
      "Iteration 4220, loss = 0.00111544\n",
      "Iteration 4221, loss = 0.00111384\n",
      "Iteration 4222, loss = 0.00111107\n",
      "Iteration 4223, loss = 0.00111281\n",
      "Iteration 4224, loss = 0.00111130\n",
      "Iteration 4225, loss = 0.00111059\n",
      "Iteration 4226, loss = 0.00111119\n",
      "Iteration 4227, loss = 0.00111258\n",
      "Iteration 4228, loss = 0.00110983\n",
      "Iteration 4229, loss = 0.00110982\n",
      "Iteration 4230, loss = 0.00111041\n",
      "Iteration 4231, loss = 0.00110725\n",
      "Iteration 4232, loss = 0.00110667\n",
      "Iteration 4233, loss = 0.00110707\n",
      "Iteration 4234, loss = 0.00110730\n",
      "Iteration 4235, loss = 0.00110631\n",
      "Iteration 4236, loss = 0.00110615\n",
      "Iteration 4237, loss = 0.00111179\n",
      "Iteration 4238, loss = 0.00110689\n",
      "Iteration 4239, loss = 0.00110544\n",
      "Iteration 4240, loss = 0.00110793\n",
      "Iteration 4241, loss = 0.00110335\n",
      "Iteration 4242, loss = 0.00110464\n",
      "Iteration 4243, loss = 0.00110239\n",
      "Iteration 4244, loss = 0.00110191\n",
      "Iteration 4245, loss = 0.00110440\n",
      "Iteration 4246, loss = 0.00110131\n",
      "Iteration 4247, loss = 0.00110181\n",
      "Iteration 4248, loss = 0.00110018\n",
      "Iteration 4249, loss = 0.00110078\n",
      "Iteration 4250, loss = 0.00109961\n",
      "Iteration 4251, loss = 0.00109965\n",
      "Iteration 4252, loss = 0.00109890\n",
      "Iteration 4253, loss = 0.00109718\n",
      "Iteration 4254, loss = 0.00109748\n",
      "Iteration 4255, loss = 0.00109826\n",
      "Iteration 4256, loss = 0.00109880\n",
      "Iteration 4257, loss = 0.00109706\n",
      "Iteration 4258, loss = 0.00109608\n",
      "Iteration 4259, loss = 0.00109665\n",
      "Iteration 4260, loss = 0.00109692\n",
      "Iteration 4261, loss = 0.00109365\n",
      "Iteration 4262, loss = 0.00109448\n",
      "Iteration 4263, loss = 0.00109728\n",
      "Iteration 4264, loss = 0.00109394\n",
      "Iteration 4265, loss = 0.00109273\n",
      "Iteration 4266, loss = 0.00109343\n",
      "Iteration 4267, loss = 0.00109298\n",
      "Iteration 4268, loss = 0.00109282\n",
      "Iteration 4269, loss = 0.00109149\n",
      "Iteration 4270, loss = 0.00109396\n",
      "Iteration 4271, loss = 0.00109177\n",
      "Iteration 4272, loss = 0.00109068\n",
      "Iteration 4273, loss = 0.00108878\n",
      "Iteration 4274, loss = 0.00108863\n",
      "Iteration 4275, loss = 0.00108967\n",
      "Iteration 4276, loss = 0.00108769\n",
      "Iteration 4277, loss = 0.00108729\n",
      "Iteration 4278, loss = 0.00108649\n",
      "Iteration 4279, loss = 0.00108969\n",
      "Iteration 4280, loss = 0.00108893\n",
      "Iteration 4281, loss = 0.00108775\n",
      "Iteration 4282, loss = 0.00108763\n",
      "Iteration 4283, loss = 0.00108491\n",
      "Iteration 4284, loss = 0.00108654\n",
      "Iteration 4285, loss = 0.00108440\n",
      "Iteration 4286, loss = 0.00108777\n",
      "Iteration 4287, loss = 0.00108312\n",
      "Iteration 4288, loss = 0.00108301\n",
      "Iteration 4289, loss = 0.00108383\n",
      "Iteration 4290, loss = 0.00108423\n",
      "Iteration 4291, loss = 0.00108116\n",
      "Iteration 4292, loss = 0.00108171\n",
      "Iteration 4293, loss = 0.00108013\n",
      "Iteration 4294, loss = 0.00108070\n",
      "Iteration 4295, loss = 0.00107981\n",
      "Iteration 4296, loss = 0.00107820\n",
      "Iteration 4297, loss = 0.00108510\n",
      "Iteration 4298, loss = 0.00107982\n",
      "Iteration 4299, loss = 0.00108624\n",
      "Iteration 4300, loss = 0.00108003\n",
      "Iteration 4301, loss = 0.00107706\n",
      "Iteration 4302, loss = 0.00108132\n",
      "Iteration 4303, loss = 0.00107985\n",
      "Iteration 4304, loss = 0.00107688\n",
      "Iteration 4305, loss = 0.00107551\n",
      "Iteration 4306, loss = 0.00107504\n",
      "Iteration 4307, loss = 0.00107433\n",
      "Iteration 4308, loss = 0.00107348\n",
      "Iteration 4309, loss = 0.00107482\n",
      "Iteration 4310, loss = 0.00107345\n",
      "Iteration 4311, loss = 0.00107372\n",
      "Iteration 4312, loss = 0.00107420\n",
      "Iteration 4313, loss = 0.00107185\n",
      "Iteration 4314, loss = 0.00107086\n",
      "Iteration 4315, loss = 0.00107333\n",
      "Iteration 4316, loss = 0.00107163\n",
      "Iteration 4317, loss = 0.00107392\n",
      "Iteration 4318, loss = 0.00107354\n",
      "Iteration 4319, loss = 0.00107205\n",
      "Iteration 4320, loss = 0.00107319\n",
      "Iteration 4321, loss = 0.00107388\n",
      "Iteration 4322, loss = 0.00106867\n",
      "Iteration 4323, loss = 0.00106738\n",
      "Iteration 4324, loss = 0.00106799\n",
      "Iteration 4325, loss = 0.00106886\n",
      "Iteration 4326, loss = 0.00106745\n",
      "Iteration 4327, loss = 0.00106602\n",
      "Iteration 4328, loss = 0.00106519\n",
      "Iteration 4329, loss = 0.00106559\n",
      "Iteration 4330, loss = 0.00106722\n",
      "Iteration 4331, loss = 0.00106433\n",
      "Iteration 4332, loss = 0.00106578\n",
      "Iteration 4333, loss = 0.00106385\n",
      "Iteration 4334, loss = 0.00106312\n",
      "Iteration 4335, loss = 0.00106185\n",
      "Iteration 4336, loss = 0.00106322\n",
      "Iteration 4337, loss = 0.00106397\n",
      "Iteration 4338, loss = 0.00106203\n",
      "Iteration 4339, loss = 0.00106237\n",
      "Iteration 4340, loss = 0.00106056\n",
      "Iteration 4341, loss = 0.00106098\n",
      "Iteration 4342, loss = 0.00105905\n",
      "Iteration 4343, loss = 0.00105912\n",
      "Iteration 4344, loss = 0.00105843\n",
      "Iteration 4345, loss = 0.00105834\n",
      "Iteration 4346, loss = 0.00105796\n",
      "Iteration 4347, loss = 0.00105739\n",
      "Iteration 4348, loss = 0.00105720\n",
      "Iteration 4349, loss = 0.00105739\n",
      "Iteration 4350, loss = 0.00105689\n",
      "Iteration 4351, loss = 0.00105563\n",
      "Iteration 4352, loss = 0.00105658\n",
      "Iteration 4353, loss = 0.00105643\n",
      "Iteration 4354, loss = 0.00105539\n",
      "Iteration 4355, loss = 0.00105396\n",
      "Iteration 4356, loss = 0.00105407\n",
      "Iteration 4357, loss = 0.00105474\n",
      "Iteration 4358, loss = 0.00105376\n",
      "Iteration 4359, loss = 0.00105205\n",
      "Iteration 4360, loss = 0.00105341\n",
      "Iteration 4361, loss = 0.00105197\n",
      "Iteration 4362, loss = 0.00105315\n",
      "Iteration 4363, loss = 0.00105107\n",
      "Iteration 4364, loss = 0.00105309\n",
      "Iteration 4365, loss = 0.00105015\n",
      "Iteration 4366, loss = 0.00104970\n",
      "Iteration 4367, loss = 0.00105011\n",
      "Iteration 4368, loss = 0.00104939\n",
      "Iteration 4369, loss = 0.00104914\n",
      "Iteration 4370, loss = 0.00104843\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20, 15, 13, 10, 8), max_iter=10000,\n",
       "              solver=&#x27;sgd&#x27;, tol=1e-06, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(20, 15, 13, 10, 8), max_iter=10000,\n",
       "              solver=&#x27;sgd&#x27;, tol=1e-06, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 15, 13, 10, 8), max_iter=10000,\n",
       "              solver='sgd', tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (20,15,13,10,8),\n",
    "    activation =\"relu\",\n",
    "    solver = 'sgd',\n",
    "    learning_rate = 'constant',\n",
    "    tol = 0.000001,\n",
    "    max_iter = 10000,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2de393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:14.439466Z",
     "start_time": "2022-10-13T04:13:13.952422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Confusion Matrix')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\n",
    "fig.figure_.suptitle(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e03bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T04:13:19.258757Z",
     "start_time": "2022-10-13T04:13:19.132947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/clf.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "joblib.dump(clf, 'model/clf.pkl', compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "mediapipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
